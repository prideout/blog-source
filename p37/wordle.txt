"It makes programming fun again!" is a cliché
    among geeks; all too often it's used to extol the virtues of some
    new-fangled programming language or platform. But I honestly think there's
    no better aphorism to describe iPhone graphics programming. Whether you're
    a professional or a hobbyist, I hope this book can play a small role in
    helping you to re-discover the joy of programming.This book is  an OpenGL
    manual, but it does teach many basic OpenGL concepts as a means to an end:
    namely, 3D graphics programming on the iPhone and iPod Touch. Much of the
    book is written in a tutorial style, and I encourage you to download the
    sample code and play with it. Readers don't need a graphics background,
    nor do they need any experience with the iPhone SDK. A sound understanding
    of C++ is required; fluency in Objective C is useful but not necessary. A
    smidgen of Python is used in , but don't let it
    scare you off.I tried to avoid making this book math-heavy,
    but, as with any 3D graphics book, you at least need a fearless attitude
    towards basic linear algebra. I'll hold your hand and jog your memory
    along the way.If you're already familiar with 3D graphics,
    but haven't done much with the iPhone, you can still learn a thing or two
    from this book. There are certain sections that you can probably skip
    over. Much of chapter 2 is an overview of general 3D graphics concepts; I
    won't be offended if you just skim through it. Conversely, if you have
    iPhone experience but are new to 3D graphics, you can gloss over some of
    the Objective C and Xcode overviews given in Chapter 1.In any case, I hope you enjoy reading the book
    as much as I enjoyed writing it!The following typographical conventions are
    used in this book:Indicates new terms, URLs, email
          addresses, filenames, and file extensions.Used for program listings, as well as
          within paragraphs to refer to program elements such as variable or
          function names, databases, data types, environment variables,
          statements, and keywords.Shows commands or other text that should
          be typed literally by the user.Shows text that should be replaced with
          user-supplied values or by values determined by context.This icon signifies a tip, suggestion, or
      general note.This icon indicates a warning or
      caution.This book is here to help you get your job
    done. In general, you may use the code in this book in your programs and
    documentation. You do not need to contact us for permission unless you’re
    reproducing a significant portion of the code. For example, writing a
    program that uses several chunks of code from this book does not require
    permission. Selling or distributing a CD-ROM of examples from O’Reilly
    books does require permission. Answering a question by citing this book
    and quoting example code does not require permission. Incorporating a
    significant amount of example code from this book into your product’s
    documentation does require permission.We appreciate, but do not require, attribution.
    An attribution usually includes the title, author, publisher, and ISBN.
    For example: “ by Some Author. Copyright
    2008 O’Reilly Media, Inc., 978-0-596-xxxx-x.”If you feel your use of code examples falls
    outside fair use or the permission given above, feel free to contact us at
    .When you see a Safari® Books Online icon on
      the cover of your favorite technology book, that means the book is
      available online through the O’Reilly Network Safari Bookshelf.Safari offers a solution that’s better than
    e-books. It’s a virtual library that lets you easily search thousands of
    top tech books, cut and paste code samples, download chapters, and find
    quick answers when you need the most accurate, current information. Try it
    for free at .Please address comments and questions
    concerning this book to the publisher:We have a web page for this book, where we list
    errata, examples, and any additional information. You can access this page
    at:To comment or ask technical questions about
    this book, send email to:For more information about our books,
    conferences, Resource Centers, and the O’Reilly Network, see our web site
    at:This book exists only because of the logical
    AND of the existence of a number of different people, including Harsha
    Kuntur, whose rabid iPhone enthusiasm is the seed for this book; Stephen
    Holmes, who unintentionally made me into an Apple fanboy; David Banks, who
    got me into graphics in the first place.I'd also like to thank my editor and personal
    champion at O'Reilly, Brian Jepson. Much thanks to both John T. Kennedy
    and Jon C. Kennedy for their valuable suggestions (can't the Irish be more
    creative with names?). I was joyous when Serban Porumbescu agreed to
    review my book — I needed his experience. I'm also supremely grateful to
    Alex MacPhee and David Schmitt of Medical Simulation Corporation, who have
    been accomodating and patient as I tried to juggle my time with this
    book.Finally I'd like to thank Mona, who had
    1-800-DIVORCE on speed dial while I was having an affair with this book,
    but managed to resist the temptation. In fact, without her limitless
    support and encouragement, there's absolutely no way I could've done
    this.Rumors of my assimilation are greatly
    exaggerated.In this chapter, you'll plunge in and develop
  your first application from scratch. The goal is to write a "Hello Arrow"
  program that draws an arrow and rotates it in response to an orientation
  change.You'll be using the OpenGL ES API to render the
  arrow, but OpenGL is only one of many graphics technologies supported on the
  iPhone. At first, it can be confusing which of these technologies is most
  appropriate for your requirements. It's also not always obvious which
  technologies are iPhone-specific and which cross over into general Mac OS X
  development.Apple neatly organizes all of the iPhone's public
  APIs into four layers: Cocoa Touch, Media Services, Core Services, and Core
  OS. Mac OS X is a bit more sprawling, but it too can be roughly organized
  into four layers, as shown in .At the very bottom layer, Mac OS X and the iPhone
  share their kernel architecture and core operating system; these shared
  components are collectively known as .Despite the similarities between the two
  platforms, they diverge quite a bit in their handling of OpenGL.  includes some
  OpenGL-related classes, shown in bold. The 
  class in Mac OS X does not exist on the iPhone, and the iPhone's
   and  classes are
  absent on Mac OS X. The OpenGL API itself is also quite different in the two
  platforms, as Mac OS X supports full-blown OpenGL while the iPhone relies on
  the more svelte OpenGL ES.The iPhone graphics technologies include:Vector-based graphics library that
          supports alpha blending, layers, and anti-aliasing. This is also
          available on Mac OS X. Applications that leverage Quartz technology
          must reference a  (Apple's term for a
          bundle of resources and libraries) known as .Vanilla C interface to Quartz. This is
          also available on Mac OS X.Native windowing framework for iPhone.
          Among other things, UIKit wraps Quartz primitives into Objective-C
          classes. This has a Mac OS X counterpart called
          , which is a component of
          .Conceptual layer in the iPhone
          programming stack that contains UIKit along with a few other
          frameworks.Objective-C framework that facilitates
          complex animations.Low-level hardware-accelerated C API for
          rendering 2D or 3D graphics.Tiny glue API between OpenGL ES and
          UIKit. Some EAGL classes (such as ) are
          defined in Quartz Core framework, while others (such as
          ) are defined in the OpenGL ES
          framework.This book chiefly deals with OpenGL ES, the only
  technology in the above list that isn't Apple-specific. The OpenGL ES
  specification is controlled by a consortium of companies called the
  . Different implementations of OpenGL ES
  all support the same core API, making it easy to write portable code.
  Vendors can pick and choose from a formally-defined set of extensions to the
  API, and the iPhone supports a rich set of these extensions. We'll cover
  many of these extensions throughout the book.Yes, you do need a Mac to develop applications
    for the iPhone App Store! Developers with a PC background should quell
    their fear; my own experience was that the PC-to-Apple transition was
    quite painless, aside from some initial frustration with a different
    keyboard.Xcode serves as Apple's preferred development
    environment for Mac OS X. If you are new to Xcode, it might initially
    strike you as resembling an email client more than an IDE. This layout is
    actually quite intuitive; after learning the keyboard shortcuts, I found
    Xcode to be a productive environment. It's also fun to work with. For
    example, after typing in a closing delimiter such as
    , the corresponding 
    momentarily glows and seems to push itself out from the screen. This
    effect is pleasant and subtle; the only thing missing is a kitten-purr
    sound effect. Maybe Apple will add that to the next version of
    Xcode.Now we come to the elephant in the room. At
      some point, you've probably heard that Objective-C is a requirement for
      iPhone development. You can actually use pure C or C++ for much of your
      application logic, if it does not make extensive use of UIKit. This is
      especially true for OpenGL development as it is a C API. Most of this
      book uses C++; Objective-C is only used for the bridge code between the
      iPhone operating system and OpenGL ES.The origin of Apple's usage of Objective-C
      lies with NeXT, which was another Steve Jobs company whose technology
      was ahead of its time in many ways — perhaps too far ahead. NeXT failed
      to survive on its own, and Apple purchased it in 1997. To this day, you
      can still find the  prefix in many of Apple's APIs,
      including those for the iPhone.Some would say that Objective-C is not as
      complex or feature-rich as C++, which isn't necessarily a bad thing. In
      many cases, Objective-C is the right tool for the right job. It's a
      fairly simple superset of C, making it quite easy to learn.However, for 3D graphics, I find that certain
      C++ features are indispensable. Operator overloading makes it possible
      to perform vector math in a syntactically natural way. Templates allow
      the re-use of vector and matrix types using a variety of underlying
      numerical representations. Most importantly, C++ is widely used on many
      platforms, and in many ways, it's the 
      of game developers.In 1982, a Stanford University professor named
    Jim Clark started one of the world's first computer graphics companies:
    Silicon Graphics Computer Systems, later known as SGI. SGI engineers
    needed a standard way of specifying common 3D transformations and
    operations, so they designed a proprietary API called IrisGL. In the early
    90's, SGI reworked IrisGL and released it to the public as an industry
    standard, and OpenGL was born.Over the years, graphics technology advanced
    even more rapidly than Moore's Law could have predicted.OpenGL went through many revisions while largely preserving
    backwards compatibility. Many developers believed that the API became
    bloated. When the mobile phone revolution took off, the need for a
    trimmed-down version of OpenGL became more apparent than ever. The Khronos
    Group announced OpenGL for Embedded Systems (OpenGL ES) at the annual
    SIGGRAPH conference in 2003.John Hart, , 2002OpenGL ES rapidly gained popularity, and today
    is used on many platforms besides the iPhone, including Android, Symbian,
    and Playstation 3.All Apple devices support at least version 1.1
    of the OpenGL ES API, which added several powerful features to the core
    specification, including vertex buffer objects and mandatory multitexture
    support, both of which we'll learn about in this book.In March of 2007, the Khronos Group released
    the OpenGL ES 2.0 specification, which entailed a major break in backwards
    compatibility by ripping out many of the fixed-function features and
    replacing them with a . This new
    model for controlling graphics simplified the API and shifted greater
    control into the hands of developers. Many developers (including myself)
    find the ES 2.0 programming model to be more elegant than the ES 1.1
    model. But in the end, the two APIs simply represent two different
    approaches to the same problem. With ES 2.0, an application developer
    needs to do much more work just to write a simple Hello World application.
    The 1.x flavor of OpenGL ES will probably continue to be used for some
    time, due to its low implementation burden.Apple's newer handheld devices, such as the
    iPhone 3GS, have graphics hardware that supports both ES 1.1 and 2.0;
    these devices are said to have a  because the graphics processor executes instructions
    rather then performing fixed mathematical operations. Older devices like
    the first-generation iPod touch, iPhone, and iPhone 3G are said to have a
     because they
    support only ES 1.1.Before writing your first line of code, be sure
    to have a good handle on your graphics requirements. It's tempting to use
    the latest and greatest API, but keep in mind that there are many 1.1
    devices out there, so this could open up a much broader market for your
    application. It can also be less work to write an ES 1.1 application, if
    your graphical requirements are fairly modest.Of course, many advanced effects are only
    possible in ES 2.0 — and, as I mentioned before, I believe it to be more
    elegant programming model.To summarize, you can choose from among four
    possibilities for your application:Use OpenGL ES 1.1 only.Use OpenGL ES 2.0 only.Determine capabilities at run-time; use ES
        2.0 if it's supported, otherwise fall back to ES 1.1.Release two separate applications: one for
        ES 1.1, one for ES 2.0. (This could get messy.)Choose wisely! We'll be using choice #3 for
    many of the samples in this book, including the Hello Arrow sample
    presented in this chapter.Assuming you already have a Mac, the first step
    is to head over to Apple's iPhone developer site and download the SDK.
    With only the free SDK in hand, you have the tools at your disposal to
    develop complex applications, and even test them on the iPhone
    simulator.The iPhone simulator cannot emulate certain
    features such as the accelerometer, nor does it perfectly reflect the
    iPhone's implementation of OpenGL ES. For example, a physical iPhone
    cannot render anti-aliased lines using OpenGL's smooth lines feature, but
    the simulator can. Conversely, there may be extensions that a physical
    iPhone supports that the simulator does not. (Incidentally, we'll discuss
    how to work around the anti-aliasing limitation later in the book.)Having said all that, you do not need to own an
    iPhone to use this book. I've ensured that every code sample either runs
    against the simulator, or at least fails gracefully in the rare case where
    it leverages a feature not supported on the simulator.If you do own an iPhone and are willing to
    cough up a reasonable fee ($100 at the time of this writing), you can join
    Apple's iPhone Developer Program to enable deployment to a physical
    iPhone. When I did this it was not a painful process, and Apple granted me
    approval almost immediately. If the approval process takes longer in your
    case, I suggest forging ahead with the simulator while you wait. I
    actually use the simulator for most of my day-to-day development anyway,
    since it provides a much faster debug-build-run turnaround than deploying
    to my device.The remainder of this chapter is written in a
    tutorial style. Be aware that some of the steps may vary slightly from
    what's written, depending on the versions of the tools that you're using.
    These minor deviations most likely pertain to specific actions within the
    Xcode UI; for example, a menu might be renamed or shifted in future
    versions. However the actual sample code is relatively
    future-proof.The iPhone SDK can be downloaded from
      here:It's packaged as a  file, Apple's
      standard disk image format. After you download it, it should
      automatically open in a Finder window — if it doesn't, you can find its
      disk icon on the desktop and open it from there. The contents of the
      disk image usually consist of an "about" PDF, a
       subfolder, and an installation package,
      whose icon resembles a cardboard box. Open the installer and follow the
      steps. When confirming the subset of components to install, simply
      accept the defaults. When you're done, you can "eject" the disk image to
      remove it from the desktop.As an Apple developer, Xcode will become
        your home base. I recommend dragging it to your dock at the bottom of
        the screen. You'll find Xcode in
        .If you're coming from a PC background,
        Mac's windowing system may seem difficult to organize at first. I
        highly recommend the  and
         desktop managers that are built into Mac
        OS X. Exposé lets you switch between windows using an intuitive
        "spread-out" view. Spaces can be used to organize your windows into
        multiple virtual desktops. I've used several virtual desktop managers
        on Windows, and in my opinion Spaces beats them all hands down.When running Xcode for the first time, it
      presents you with a  dialog. Click
      the  button. (If you've
      got the Welcome dialog turned off, go to .) Now you'll be presented with the dialog seen in
      , consisting of a
      collection of templates. The template we're interested in is
      , under the  heading. It's nothing fancy, but it is a fully-functional
      OpenGL application, and serves as a decent starting point.In the next dialog, choose a goofy name for
      your application, then you'll finally see Xcode's main window. Build and
      run the application by selecting Build and Run from the Build menu, or
      by pressing ⌘-Return. When the build finishes, you should see the iPhone
      simulator pop up with a moving square in the middle, as in . When you're done
      gawking at this amazing application, press ⌘-Q to quit.This is not required for development, but if
      you wish the deploy your application to a physical device, you should
      sign up for Apple's iPhone Developer Program. This enables you to
      provision your iPhone for developer builds, in addition to the usual
      software you get from the App Store. Provisioning is a somewhat
      laborious process, but thankfully it only needs to be done once per
      device. Apple has now made it reasonably straightforward by providing a
      "Provisioning Assistant" applet that walks you through the process.
      You'll find this applet after logging into the iPhone Dev Center ()
      and entering the iPhone Developer Program Portal.When your iPhone has been provisioned
      properly, you should be able to see it in Xcode's window (Control-⌘-O). Open up the  tree node in the left-hand pane and make sure your
      device is listed.Now you can go back to Xcode's main window,
      open the  combo box in the upper-left
      corner, and choose the latest SDK that has the
       prefix. Next time you build and run (
      ⌘-Return), the moving square should appear on your iPhone.In the previous section you learned your way
    around the development environment with Apple's boilerplate OpenGL
    application, but to get a good understanding of the fundamentals, you need
    to start from scratch. This section of the book builds a simple
    application from the ground up using OpenGL ES 1.1. The 1.x track of
    OpenGL ES is sometimes called  to
    distinguish it from the OpenGL ES 2.0 track, which relies on shaders.
    We'll learn how to modify the sample to use shaders later in the
    chapter.Let's come up with a variation of the classic
    Hello World in a way that fits well with the theme of this book. As you'll
    learn later, most of what gets rendered in OpenGL can be reduced down to
    triangles. We can use two overlapping triangles to draw a simple arrow
    shape, as seen in .
    Any resemblance to the  logo is purely
    coincidental.To add an interesting twist, the program will
    make the arrow stay upright when the user changes the orientation of his
    iPhone.If you love Objective-C, then by all means,
      use it everywhere you can. This book supports cross-platform code reuse,
      so we leverage Objective-C only when necessary.  depicts a couple ways
      of organizing your application code such that the guts of the program
      are written in C++ (or vanilla C), while the iPhone-specific glue is
      written in Objective C. The variation on the right separates the
      application engine (also known as )
      from the rendering engine. Some of the more complex samples in this book
      take this approach.The key to either approach depicted in  is
      designing a robust interface to the rendering engine, and ensuring that
      any platform can use it. The sample code in this book uses the name
       for this interface, but you can call
      it what you want.The 
      interface also allows you to build multiple rendering engines into your
      application, as seen in . This facilitates the "Use ES 2.0 if
      supported, otherwise fall back" scenario mentioned on page . We'll take this approach for Hello Arrow.You'll learn more about the pieces in  as we walk through the code to Hello
      Arrow. To summarize, you'll be writing three classes:These classes are where most of the
            work takes place; all calls to OpenGL ES are made from here.
             uses ES 1.1 while
             uses ES 2.0.Small Objective-C class that derives
            from  and adopts the
             protocol. ("Adopting a
            protocol" in Objective-C is somewhat analogous to "implementing an
            interface" in languages such as Java or C#.) This does not use
            OpenGL or EAGL; it simply initializes the
             object and releases memory when the
            application closes.Derives from the standard
             class and uses EAGL to instance a valid
            rendering surface for OpenGL.Launch Xcode and start with the simplest
      project template by going to  and
      selecting  from the list of
      iPhone OS Application templates. Name it HelloArrow.Xcode comes bundled with an application
      called , which is Apple's
      interactive designer for building interfaces with UIKit (and AppKit on
      Mac OS X). This book does not attempt to cover UIKit because most 3D
      applications do not make extensive use of it. For best performance,
      Apple advises against mixing UIKit with OpenGL.For simple 3D applications that aren't too demanding, it
        probably won't hurt you to add some UIKit controls to your OpenGL
        view. We cover this briefly in .The following steps remove some Interface
        Builder odds and ends from the project; this is optional, but it's
        something I like to do to start from a clean slate.Interface Builder uses an XML file
            called a  for defining an object graph
            of UI elements. Since you're creating a pure OpenGL application,
            you don't need this file in your project. In the  pane on the left, find the folder that says
             (or something similar such as
            ). Delete the file ending in
            . When prompted, move it to trash.The xib file normally compiles to a
            binary file called a nib, which is loaded at run-time to build up
            the UI. To instruct the OS not to load the nib file, you'll need
            to remove an application property. In the Resources folder, find
            the  file. Click it then
            remove the property called  (towards the bottom of the Information Property
            List). You can remove a property by clicking to select it, then
            pressing the Delete key.The template application normally
            extracts the name of the application delegate from the nib file
            during startup; since you're not using a nib file, you need to
            pass in an explicit string. Under , open  and change
            the last argument of the call to
             from 
            to the name of your application delegate class (for example,
            ). The @ prefix means
            this is a proper Objective-C string rather than a C-style pointer
            to .The template includes a property in the
            application delegate that allows Interface Builder to hook in.
            This is no longer needed. To remove the property declaration, open
             (in the
             folder) and remove the
             line. To remove the implementation,
            open  and remove the
             line.In the world of Apple programming, you can
      think of a  as being similar to a library,
      but technically it's a  of resources. A
      bundle is a special type of folder that acts like a single file, and
      it's quite common in Mac OS X. For example, applications are usually
      deployed as bundles — open the action menu on nearly any icon in your
       folder, and you'll see an option for
      , which allows you to get past
      the façade.You need to add some framework references to
      your Xcode project. Pull down the action menu for the
       folder. This can be done by selecting
      the folder and clicking the Action icon, or by right-clicking or
      control-clicking the folder. Next choose . Select 
      and click the Add button. You may see a dialog after this; if so, simply
      accept its defaults. Now, repeat this procedure with
      .Why do we need Quartz if we're writing an
        OpenGL ES application? The answer is that Quartz owns the layer object
        that gets presented to the screen, even though we're rendering with
        OpenGL. The layer object is an instance of
        , which is a subclass of
        ; these classes are defined in the Quartz
        Core framework.The abstract  class
      controls a rectangular area of the screen, handles user events, and
      sometimes serves as a container for child views. Almost all standard
      controls such as buttons, sliders, and text fields are descendants of
      . We tend to avoid using these controls in this
      book; for most of the sample code, the UI requirements are so modest
      that OpenGL itself can be used to render simple buttons and various
      widgets.For our Hello Arrow sample, we do need to
      define a single  subclass, since all rendering
      on the iPhone must take place within a view. Select the
       folder in Xcode, click the Action menu in
      the toolbar, and select . Under the
       category, select the Objective-C
      class template, and choose in the
       menu. In the next dialog, name it
       and leave the box checked to ensure that
      the corresponding header gets generated. The 
      extension indicates that this file can support C++ in addition to
      Objective-C. Open . You should see
      something like this:For C/C++ veterans, this syntax can be a
      little jarring — just wait till you see the syntax for methods! Fear
      not, it's easy to become accustomed to. is almost the same
      thing as , but automatically ensures that the
      header file does not get expanded twice within the same source file.
      This is similar to the  feature found in
      many C/C++ compilers.Keywords specific to Objective-C stand out
      due to the  prefix. The
       keyword marks the beginning of a class
      declaration; the  keyword marks the end of a
      class declaration. A single source file may contains several class
      declarations, and therefore can have several
       blocks.As you probably already guessed, the above
      code snippet simply declares an empty class called
       that derives from .
      What's less obvious is the fact that data fields will go inside the
      curly braces, while method declarations will go between the ending curly
      brace and the , like this:By default, data fields have protected
      accessibility, but you can make them private using the
       keyword. Let's march onward and fill in
      these pieces shown in bold in . We're also adding some new #imports for
      OpenGL-related stuff.The  field is a
      pointer to the EAGL object that manages our OpenGL context. EAGL is a
      small Apple-specific API that links the iPhone operating system with
      OpenGL.Every time you modify API state through an
        OpenGL function call, you do so within a .
        For a given thread running on your system, only one context can be
        current at any time. With the iPhone, you'll rarely need more than one
        context for your application. Due to the limited resources on mobile
        devices, I don't recommend using multiple contexts.If you have a C/C++ background, the
       method declaration in  may look odd. It's less
      jarring if you're familiar with UML syntax, but UML uses
       and  to denote private and
      public methods, respectively; with Objective-C, 
      and  denote instance methods and class methods.
      (Class methods in Objective-C are somewhat similar to C++ static
      methods, but in Objective-C, the class itself is a proper
      object.)Take a look at the top of the
       file that Xcode generated. Everything
      between  and 
      is the definition of the  class. Xcode created
      three methods for you: ,
       (which may be commented out), and
      . Note that these methods do not have
      declarations in the header file that Xcode generated. In this respect,
      an Objective-C method is similar to a plain old function in C; it needs
      a forward declaration only if gets called before it's defined. I usually
      declare all methods in the header file anyway, to be consistent with C++
      class declarations.Take a closer look at the first method in the
      file:This is an Objective-C initializer method,
      which is somewhat analogous to a C++ constructor. The return type and
      argument types are enclosed in parentheses, similar to C-style casting
      syntax. The conditional in the  statement
      accomplishes several things at once: it calls the base implementation of
      , assigns the objects pointer to
      , and checks the result for success.In Objective-C parlance, you don't call
      methods on objects; you send messages to objects. The square bracket
      syntax denotes a message. Rather than a comma-separated list of values,
      arguments are denoted with a whitespace-separated list of name-value
      pairs. The idea is that messages can vaguely resemble English sentences.
      For example, consider this statement, which adds an element to a
      :If you read the argument list aloud, you get
      an English sentence! Well, sort of.That's enough of an Objective-C lesson for
      now. Let's get back to the Hello Arrow application. In
      , provide the implementation to the
       method by adding the following snippet
      after the  line:This simply overrides the default
      implementation of  to return an
      OpenGL-friendly layer type. The method is
      similar to the  operator found in other
      languages; it returns an object that represents the type itself, rather
      than an instance of the type.The  prefix means
        that this is an override of a class method rather than an instance
        method. This type of override is a feature of Objective-C rarely found
        in other languages.Now, go back to
       and replace the contents of the
       block with some EAGL initialization code, as seen
      in .Here's what's going on:Retrieve the 
          property from the base class () and
          downcast it from  into a
          . This is safe because of the override
          to the  method.Set the 
          property on the layer to indicate that you do not need Quartz to
          handle transparency. This is a performance benefit that Apple
          recommends in all OpenGL programs. Don't worry, you can easily use
          OpenGL to handle alpha blending.Create an 
          object and tell it which version of OpenGL you need, which is ES
          1.1.Tell the 
          to make itself current, which means any subsequent OpenGL calls in
          this thread will be tied to it.If context creation fails, or if
           fails, then poop out and return
          .The alloc-init pattern that we used for
        instancing  in  is very common in Objective-C. With
        Objective-C, constructing an object is always split into two phases:
        allocation and initialization. However some of the classes you'll
        encounter supply a class method to make this easier. For example, to
        convert a utf-8 string into a  using the
        alloc-init pattern, you could do this:But, I prefer doing this:Not only is this more terse, it also adds
        auto-release semantics to the object, so there's no need to call
         on it when you're done.Next, continue filling in the initialization
      code with some OpenGL setup. Replace the  comment with . starts off by generating two OpenGL
      identifiers, one for a  and one for a
      . Briefly, a renderbuffer is a
      two-dimensional surface filled with some type of data (in this case,
      color), and a framebuffer is a bundle of renderbuffers. You'll learn
      more about framebuffer objects (FBOs) in later chapters.The use of FBOs is an advanced feature that
        is not part of the core OpenGL ES 1.1 API, but it is specified in an
        OpenGL extension that all iPhones support. In OpenGL ES 2.0, FBOs are
        included in the core API. It may seem odd to use this advanced feature
        in the simple Hello Arrow program, but all OpenGL iPhone applications
        need to leverage FBOs to draw anything to the screen.The renderbuffer and framebuffer are both of
      type , which is the type that OpenGL uses to
      represent various objects that it manages. You could just as easily use
       in lieu of ,
      but I recommend using the GL-prefixed types for objects that get passed
      to the API. If nothing else, the GL-prefixed types make it easier for
      humans to identify which pieces of your code interact with
      OpenGL.After generating identifiers for the
      framebuffer and renderbuffer,  then these
      objects to the pipeline. When an object is bound, it can be modified or
      consumed by subsequent OpenGL operations. After binding the
      renderbuffer, storage is allocated by sending the
       message to the
       object.For an offscreen surface, you would use the
        OpenGL command  to perform
        allocation, but in this case you're associating the renderbuffer with
        an EAGL layer. You'll learn more about offscreen surfaces later in the
        book.Next, the
       command is used to
      attach the renderbuffer object to the framebuffer object.After this, the 
      command is issued. You can think of this as setting up a coordinate
      system. In  you'll learn more precisely what's
      going on here.The final call in  is to the 
      method. Go ahead and create the 
      implementation:This uses OpenGL's "clear" mechanism to fill
      the buffer with a solid color. First the color is set to gray using four
      values (red, green, blue, alpha). Then, the clear operation is issued.
      Finally, the  object is told to present
      the renderbuffer to the screen. Rather than drawing directly to the
      screen, most OpenGL programs render to a buffer that is then presented
      to the screen in an atomic operation, just like we're doing here.You can remove the
       stub that Xcode provided for you. The
       method is typically used for a "paint
      refresh" in more traditional UIKit-based applications; in 3D
      applications, you'll want finer control over when rendering
      occurs.At this point, you almost have a
      fully-functioning OpenGL ES program, but there's one more loose end to
      tie up. You need to clean up when the  object
      is destroyed. Replace the definition of 
      with:You can now build and run the program, but
      you won't even see the gray background color just yet. This brings us to
      the next step: hooking up the application delegate.The application delegate template
      () that Xcode provided
      contains nothing more than an instance of .
      Let's add a pointer to an instance of the 
      class along with a couple method declarations (new/changed lines are
      shown in bold):If you performed the instructions in , you won't see the 
      line, which is fine. Interface Builder leverages Objective-C's property
      mechanism to establish connections between objects, but we're not using
      Interface Builder or properties in this book. In brief, the
       keyword declares a property; the
       keyword defines accessor methods.Note that the Xcode template already had a
       member, but I renamed it to
      . This is in keeping with the coding
      conventions that we use throughout the book.I recommend using Xcode's  feature
        to rename this variable because it will also rename the corresponding
        property (if it exists). Simply right-click the
         variable and choose
        . If you did not make the changes shown
        in , you must use Refactor so that the
         file knows the window is now represented by
        .Now open the corresponding
       file. Xcode already
      provided skeleton implementations for
       and
       as part of the "Window-Based Application"
      template that we selected to create our project.Since you need this file to handle both Objective-C and C++, you
        must rename the extension to . Right-click the
        file to bring up the action menu, then select
        .Flesh out the file as seen in . uses the alloc-init pattern to construct the
      window and view objects, passing in the bounding rectangle for the
      entire screen.If you haven't removed the Interface Builder bits as described in
      , you'll need to make a couple changes to
      the above code listing:Add a new line after
          :As mentioned previously, the 
          keyword defines a set of property accessors, and Interface Builder
          uses properties to hook things up.Remove the line that constructs .
          Interface Builder has a special way of constructing the window
          behind the scenes. (Leave in the calls to
           and
          .)Compile and build and you should now see a
      solid gray screen. Hurray!To set a custom launch icon for your
      application, create a 57x57 PNG file and add it to your Xcode project in
      the Resources folder. If you refer to a PNG file that is not in the same
      location as your project folder, Xcode will copy it for you; be sure to
      check the box labeled "Copy items into destination group's folder (if
      needed)" before you click Add. Then, open the
       file (also in the Resources
      folder), find the  property, and enter in
      the name of your PNG file.The iPhone will automatically give your icon
      rounded corners and a shiny overlay. If you want to turn this feature
      off, find the  file in your
      Xcode project, select the last row, click the + button, and choose "Icon
      already includes gloss and bevel effects" from the menu. Don't do this
      unless you're really sure of yourself; Apple wants users to have a
      consistent look in SpringBoard (the built-in program used to launch
      apps).In addition to the 57x57 launch icon, Apple
      recommends that you also provide a 29x29 miniature icon for the
      Spotlight search and Settings screen. The procedure is similar except
      that the filename must be , and
      there's no need to modify the  file.For the splash screen, the procedure is
      similar to the small icon, except that the filename must be
       and there's no need to modify the
       file. The iPhone fills the entire screen with
      your image, so the ideal size is 320x480, unless you want to see an ugly
      stretchy effect. Apple's guidelines say that this image isn't a splash
      screen at all, but a "launch image" whose purpose is to create a swift
      and seamless startup experience. Rather than showing a creative logo,
      they want your launch image to mimic the starting screen of your running
      application. Of course, many applications ignore this rule!Even though your application fills the
      renderbuffer with gray, the iPhone's status bar still appears at the top
      of the screen. One way of dealing with this would be adding the
      following line to
      :The problem with this approach is that the
      status bar does not hide until after the splash screen animation. For
      Hello Arrow, let's remove the pesky status bar from the very beginning.
      Find the  file in your Xcode
      project, and add a new property by selecting the last row, clicking the
      + button, choosing "Status bar is initially hidden" from the menu, and
      checking the box.Of course, for some applications, you'll want
      to keep the status bar visible — after all, the user might wish to keep
      on eye on battery life and connectivity status! If your application has
      a black background, you can add a "Status bar style" property and select
      the black style. For non-black backgrounds, the semi-transparent style
      often works well.At this point, you have a walking skeleton
      for Hello Arrow, but you still don't have the rendering layer depicted
      in  on
      page . Add a file to your Xcode
      project to define the C++ interface. Right-click the Classes folder and
      choose , select , and choose . Call it
      . The 
      extension signals that this is a pure C++ file; no Objective-C syntax
      allowed. Replace the contents of this file with .Xcode doesn't care whether you use  or
           for headers; we use this convention purely
          for the benefit of human readers. defines an interface in C++ using some
        component-oriented conventions that we'll follow throughout the
        book:All interface methods are pure
            virtual.Interfaces are of type
             because interface methods are always
            public. (Recall that in C++, struct members default to public,
            while class members default to private.)Names of interfaces are prefixed with a
            capital .Interfaces consist of methods only; no
            fields are permitted.Construction of implementation classes
            is achieved via factory methods. In this case, the factory method
            is .All interfaces should have a virtual
            destructor to enable proper cleanup.It seems redundant to include an enumeration
      for device orientation when one already exists in an iPhone header
      (namely, ), but this makes the
       interface portable to other
      environments.Since the view class consumes the rendering
      engine interface, you need to add an pointer to the  class declaration,
      along with some fields and methods to help with rotation and animation.
      The complete class declaration is shown in . New fields and methods are shown in bold.
      Note that we removed the two OpenGL ES 1.1 #imports; these OpenGL calls
      are moving to the  class. The EAGL
      header is not part of the OpenGL standard, but it's required to create
      the OpenGL ES context. is the full listing for the class
      implementation. Calls to the rendering engine are highlighted in bold.
      Note that  no longer contains any OpenGL calls;
      we're delegating all OpenGL work to the rendering engine.This completes the Objective-C portion of the
      project, but it won't build yet as you still need to implement the
      rendering engine. There's no need to dissect all the code in , but a brief summary
      follows:The 
            method calls the factory method to instantiate the C++ renderer.
            It also sets up two event handlers. One is for the "display link",
            which fires every time the screen refreshes. The other event
            handler responds to orientation changes.The  event
            handler casts the iPhone-specific
             to our portable
             type, then passes it on to
            the rendering engine.The  method,
            called in response to a display link event, computes the elapsed
            time since it was last called, and passes that value into the
            renderer's  method. This allows
            the renderer to update any animations or physics that it might be
            controlling.The  method
            also issues the  command and presents the
            renderbuffer to the screen.At the time of writing, Apple recommends
           for triggering OpenGL rendering. An
          alternative strategy is leveraging the 
          class.  became available with iPhone
          OS 3.1, so if you need to support older versions of the iPhone OS,
          take a look at  in the
          documentation.In this section you'll create an
      implementation class for the 
      interface. Right-click the Classes folder, choose , click the  category, and
      select the  template. Call it
       and deselect the "Also create
      RenderingEngine1.h" option, since you'll declare the class directly
      within the cpp file. Enter in the class declaration and factory method
      shown in .For now, 
      and  are implemented with stubs; you'll add
      support for the rotation feature after we get up and running. shows more of the code from
       with the OpenGL initialization
      code. first defines a POD type (plain old data)
      that represents the structure of each vertex that makes up the
      triangles. As you'll learn in the chapters to come, a vertex in OpenGL
      can be associated with a variety of attributes. Hello Arrow requires
      only two attributes: a 2D position and an RGBA color.In more complex OpenGL applications, the
      vertex data is usually read from an external file or generated on the
      fly. In this case, the geometry is so simple that the vertex data is
      defined within the code itself. Two triangles are specified using six
      vertices. The first triangle is yellow, the second gray (see  on page ).Next,  divides up some framebuffer initialization
      work between the constructor and the 
      method. Between instancing the rendering engine and calling
      , the caller () is
      responsible for allocating the renderbuffer's storage. Allocation of the
      renderbuffer isn't done with the rendering engine because it requires
      Objective-C.Last but not least,
       sets up the viewport transform and
      . The projection matrix defines
      the three-dimensional volume that contains the visible portion of the
      scene. This will be explained in detail in the next chapter.To recap, here's the startup sequence:Generate an identifier for the
          renderbuffer and bind it to the pipeline.Allocate the renderbuffer's storage by
          associating it with an EAGL layer. This has to be done in the
          Objective-C layer.Create a framebuffer object and attach
          the renderbuffer to it.Set up the vertex transformation state
          with  and
          . contains the implementation of the
       method.We'll examine much of this in the next
      chapter, but briefly here's what's going on:Clear the renderbuffer to gray.Enable two vertex attributes (position
          and color).Tell OpenGL how to fetch the data for the
          position and color attributes. See .Execute the draw command with
          , specifying
           for the topology, 0 for the starting
          vertex, and  for the number of
          vertices. This function call is exactly when OpenGL fetches the data
          from the pointers specified in the preceding
           calls; this is also when the triangles
          are actually rendered on the target surface.Disable the two vertex attributes; they
          only need to be enabled during the preceding draw command. It's bad
          form to leave attributes enabled because subsequent draw commands
          might want to use a completely different set of vertex attributes.
          In this case, we could get by without disabling them because the
          program is so simple, but it's a good habit to follow.Congratulations, you created a complete
      OpenGL program from scratch! The result can be seen in .At the beginning of the chapter, I promised
      you would learn how to rotate the arrow in response to an orientation
      change. Since you already created the listener in the
       class on page ,
      all that remains is handling it in the rendering engine.First add a new floating-point field to the
       class called
      . This represents an angle in degrees,
      not radians. Note the changes to  and
       (they are no longer stubs and will be
      defined shortly).Now let's implement the
       method as follows:Note that orientations such as
      , ,
      , and  are not
      included in the switch statement, so the angle defaults to zero in those
      cases.Now you can rotate the arrow using a call to
       in the  method, as
      seen in .
      New code lines are shown in bold. This also adds some calls to
       and  to
      prevent rotations from accumulating. You'll learn more about these
      commands (including ) in the next
      chapter.You now have a Hello Arrow program that
      rotates in response to an orientation change, but it's lacking a bit of
      grace — most iPhone applications smoothly rotate the image, rather than
      suddenly jolting it by 90 degrees.It turns out that Apple provides
      infrastructure for smooth rotation via the
       class, but this is not the
      recommended approach for OpenGL ES applications. There are several
      reasons for this:For best performance, Apple recommends
          avoiding interaction between Core Animation and OpenGL ES.Ideally, the renderbuffer stays the same
          size and aspect ratio for the lifetime of the application. This
          helps performance and simplifies code.In graphically intense applications,
          developers need to have complete control over animations and
          rendering.To achieve the animation effect,  adds a new
      floating-point field to the  class
      called . This represents the
      destination value of the current animation; if no animation is
      occurring, then  and
       are equal. also introduces a floating-point constant
      called  to represent angular
      velocity, and the private method ,
      which we'll explain later.Now you can modify
       so that it changes the desired angle rather
      than the current angle:Before implementing
      , think about how the application
      decides whether to rotate the arrow clockwise or counter-clockwise.
      Simply checking if the desired angle is greater than the current angle
      is incorrect; if the user changes his device from a 270º orientation to
      a 0º orientation, the angle should  up to
      360º.This is where the
       method comes in. It returns -1,
      0, or +1, depending on which direction the arrow needs to spin. Assume
      that  and
       are both normalized to values between
      0 (inclusive) and 360 (exclusive).Now you're ready to write the
       method, which takes a time step in
      seconds:This is fairly straightforward, but that last
      conditional might look curious. Since this method incrementally adjusts
      the angle with floating-point numbers, it could easily overshoot the
      destination, especially for large time steps. Those last two lines
      correct this by simply snapping the angle back to the desired position.
      You're not trying to emulate a shaky compass here, even though doing so
      might be a compelling iPhone application!You now have a fully functional Hello Arrow
      application. As with the other examples, the complete code can be found
      on the book's website (see the Preface for more information on code
      samples).In this section you'll create a new rendering
    engine that uses ES 2.0. This will show you the immense difference between
    ES 1.1 and 2.0. Personally I like the fact that Khronos decided against
    making ES 2.0 backwards compatible with ES 1.1; the API is leaner and more
    elegant as a result.Thanks to the layered architecture of Hello
    Arrow, it's easy to add ES 2.0 support while retaining 1.1 functionality
    for older devices. You'll be making these four changes:Add some new source files to the project
        for the vertex shader and fragment shader.Update frameworks references if
        needed.Change the logic in
         so that it attempts
         with ES 2.0.Create the new
         class by modifying a copy of
        .These changes are described in detail in the
    following sections. Note that step four is somewhat artificial; in the
    real world, you'll probably want to write your ES 2.0 backend from the
    ground up.By far the most significant new feature in ES
      2.0 is the shading language.  are
      relatively small pieces of code that run on the graphics processor, and
      they are divided into two categories: vertex shaders and fragment
      shaders. Vertex shaders are used to transform the vertices that you
      submit with , while fragment shaders
      compute the colors for every pixel in every triangle. Due to the highly
      parallel nature of graphics processors, thousands of shader instances
      execute simultaneously.Shaders are written in a C-like language
      called GLSL (OpenGL Shading Language), but unlike C, you do not compile
      GLSL programs within Xcode. Shaders are compiled at run time, on the
      iPhone itself. Your application submits shader source to the OpenGL API
      in the form of a C-style string, which OpenGL then compiles to machine
      code.Some implementations of OpenGL ES do allow
        you to compile your shaders offline; on these platforms your
        application submits binaries rather than strings at run time. At
        present, the iPhone only supports shader compilation at run time. Its
        ARM processor compiles the shaders and sends the resulting machine
        code over to the graphics processor for execution. That little ARM
        does some heavy lifting!The first step to upgrading HelloArrow is
      creating a new project folder for the shaders. Right-click the
      HelloArrow root node in the 
      pane, and choose . Call the new group
      "Shaders".Next, right-click the Shaders folder and
      choose . Select the  template in the  category.
      Name it  and add
       after  in
      the location field. You can deselect the checkbox under , because there's no need to deploy the file to the
      device. In the next dialog, allow it to create a new directory. Now
      repeat the procedure with a new file called
      .Before showing you the code for these two
      files, let me explain a little trick. Rather than reading in the shaders
      with file I/O, you can simply embed them within your C/C++ code through
      the use of . Multi-line strings are usually
      cumbersome in C/C++, but they can be tamed with a sneaky little
      macro:Later in this section, we'll place this macro
      definition at the top of the rendering engine source code, right before
      ing the shaders. The entire shader (line
      breaks included) gets wrapped into a single string — without the use of
      quotation marks on every line!While the  macro is convenient for
        simple shaders, I don't recommend it for production code. For one
        thing, the line numbers reported by Apple's shader compiler may be
        incorrect. The gcc pre-processor can also get confused if your shader
        string defines functions. A common practice is to read a shader from a
        file into a monolithic string, which can easily be done from the
        Objective-C side using the 
        method. and  show the contents of the vertex shader and
      fragment shader. For brevity's sake, we'll leave out the
       accoutrements in future shader listings,
      but we're including them here for clarity.First the vertex shader declares a set of
      attributes, varyings, and uniforms. For now you can think of these as
      connection points between the vertex shader and the outside world. The
      vertex shader itself simply passes through a color and performs a
      standard transformation on the position. You'll learn more about
      transformations in the next chapter. The fragment shader () is even more
      trivial.Again, the varying parameter is a connection
      point. The fragment shader itself does nothing but pass on the color
      that it's given.Next, make sure all the frameworks in Hello
      Arrow are referencing a 3.1 (or greater) version of the SDK. To find out
      which version a particular framework is using, right-click it in Xcode's
       pane and select  to look at the full path.There's a trick to quickly change all your
        SDK references by manually modifying the project file. First you need
        to quit Xcode. Next, open Finder and right-click
         and select . Inside, you'll find a file called
        , which you can then open with
        TextEdit. Find the two places that define SDKROOT and change them
        appropriately.You might recall passing in a version
      constant when constructing the OpenGL context; this is another place
      that obviously needs to be changed. In the 
      folder, open  and change this
      snippet:to this:The above code snippet creates a fallback
      path to allow the application to work on older devices while leveraging
      ES 2.0 on newer devices. For convenience, the ES 1.1 path is used even
      on newer devices if the  constant is enabled.
      Add this to the top of :There's no need to make any changes to the
       interface, but you do need to add a
      declaration for the new  factory
      method in :You're done with the requisite changes to the
      glue code; now it's time for the meat. Use Finder to create a copy of
       (right-click or Control-click
       and choose Reveal in Finder),
      and name the new file . Add it
      to your Xcode project by right-clicking the 
      group and choosing . Next, revamp
      the top part of the file as shown in . New or modified lines are shown in
      bold.As you might expect, the implementation of
       needs to be replaced. Flip back to  on page  to compare it with .As you can see, the 1.1 and 2.0 versions of
       are quite different, but at a high level
      they basically follow the same sequence of actions.Framebuffer objects have been promoted from a
      mere OpenGL extension to the core API. Luckily OpenGL has a very strict
      and consistent naming convention, so this change is fairly mechanical.
      Simply remove the OES suffix everywhere it appears. For function calls,
      the suffix is "OES"; for constants the suffix is "_OES". The constructor
      is very easy to update:The only remaining public method that needs
      to be updated is , shown in .This calls the private method
      , which in turn makes two calls on the
      private method . In OpenGL terminology, a
       is a module composed of several shaders
      that get linked together. The implementation of these two methods is
      shown in .You might be surprised to see some console
      I/O in .
      This dumps out the shader compiler output if an error occurs. Trust me,
      you'll always want to gracefully handle these kind of errors, no matter
      how simple you think your shaders are. The console output doesn't show
      up on the iPhone screen, but it can be seen in XCode's GDB window, which
      is shown via the  menu. See  for an example of how
      a shader compilation error shows up in the console window.Note that the log in  shows the version of
      OpenGL ES being used. To include this information, go back to the
       class and add the lines in bold:The preferred method of outputting diagnostic
      information in Objective-C is , which
      automatically prefixes your string with a timestamp, and follows it with
      a carriage return. (Recall that Objective-C string objects are
      distinguished from C-style strings using the @ symbol.)Return to
      . Two methods remain:
       and .
      Since ES 2.0 does not provide  or
      , you need to implement them manually. (In
      the next chapter we'll create a simple math library to simplify code
      like this.) The calls to  provide
      values for the  variables that were declared
      in the shader source.Again, don't be intimidated by the matrix
      math; we'll explain it all in the next chapter.Next, go through the file and change any
      remaining occurrences of  to
      , including the factory method (and
      be sure to change the name of that method to
      ). This completes all the
      modifications required to run against ES 2.0. Phew! It's obvious by now
      that ES 2.0 is "closer to the metal" than ES 1.1.This chapter has been a head-first dive into
    the world of OpenGL ES development for the iPhone. We established some
    patterns and practices that we'll use throughout the book, and we
    constructed our first application from the ground up — using
     versions of OpenGL ES!In the next chapter, we'll go over some
    graphics fundamentals and explain the various concepts used in Hello
    Arrow. If you're already a computer graphics ninja, you'll be able to skim
    through quickly.There's a pizza place near where I live that
    sells only slices. In the back you can see a guy tossing a triangle in the
    air.Computer graphics requires more mathematics than
  many other fields in computer science. But if you're a pragmatic OpenGL
  programmer, all you really need is a basic grasp of linear algebra and an
  understanding of a few metaphors.In this chapter, I explain these metaphors and
  review the relevant linear algebra concepts. Along the way I'll tick off
  various OpenGL functions that relate to these concepts. Several of such
  functions were used in the Hello Arrow sample, so code that may have seemed
  mysterious should now become clear.Near the end of the chapter, we'll leverage these
  math concepts to push the Hello Arrow sample into the third dimension,
  transforming it into "Hello Cone".You can think of any graphics API, including
    OpenGL ES, as an assembly line that takes an assortment of raw materials
    such as textures and vertices for input, and eventually produces a
    neatly-packaged grid of colors.The inputs to the assembly line are the natural
    starting points for learning OpenGL, and in this chapter we'll focus on
    vertices. The gradual metamorphosis of vertices into pixels is depicted at
    a very high level in . First, a series
    of transformations is performed on the vertices; next the vertices are
    assembled into primitives; finally, primitives are rasterized into
    pixels.At a high level,  applies to both OpenGL ES 1.1 and 2.0, but
      it's important to note that in 2.0, the leftmost elf contains a vertex
      shader, and the rightmost elf hands his output over to a fragment
      shader.In this chapter we'll mostly focus on the
    transformations that occur early-on in the assembly line, but first we'll
    give a brief overview of the primitive assembly step, since it's fairly
    easy to digest.The three-dimensional shape of an object is
    known as its . In OpenGL, the geometry of
    an object constitutes a set of  that are
    either triangles, points, or lines. These primitives are defined using an
    array of vertices, and the vertices are connected according to the
    primitive's . OpenGL ES supports seven
    topologies, as depicted in .Recall the one line of code in Hello Arrow from
     that tells OpenGL to render the triangles to
    the backbuffer:The first argument to this function specifies
    the :
     tells OpenGL to interpret the vertex
    buffer such that the first three vertices compose the first triangle, the
    second three vertices compose the second triangle, and so on.In many situations you need to specify a
    sequence of adjoining triangles, in which case several vertices would be
    duplicated in the vertex array. That's when
     comes in. It allows a much smaller
    set of vertices to expand to the same number of triangles, as seen in
    . In the table,  is the
    number of vertices, and  is the number of
    primitives. For example, to draw three triangles using
    , you'd need nine vertices (3p). To draw
    them using , you'd need only five (p +
    2).Another way of specifying triangles is
    , which is useful for drawing a polygon,
    a circle, or the top of a three-dimensional dome. The first vertex
    specifies the apex while the remaining vertices form the rim. For many of
    these shapes, it's possible to use ,
    but doing so would result in degenerate triangles (triangles with zero
    area).For example, suppose you wanted to draw a
    square shape using two triangles, as shown in . (Incidentally, full-blown OpenGL has a
     primitive that would come in handy for this,
    but quads are not supported in OpenGL ES.) The following code snippet
    draws the same square three times, using a different primitive topology
    each time.Triangles aren't the only primitive type
    supported in OpenGL ES. Individual points can be rendered using
    . The size of each point can be specified
    individually, and large points are rendered as squares. Optionally, a
    small bitmap can be applied to each point; these are called
    s, and we'll learn more about them in
    .OpenGL supports line primitives using three
    different topologies: separate lines, strips, and loops. With strips and
    loops, the endpoint of each line serves as the starting point for the
    following line. With loops, the starting point of the first line also
    serves as the endpoint of the last line. Suppose you wanted to draw the
    border of the square shown in ; here's how
    you could do so using the three different line topologies:Let's go back to the assembly line and take a
    closer look at the inputs. Every vertex that you hand over to OpenGL has
    one or more attributes, the most crucial being its position. Vertex
    attributes in OpenGL ES 1.1 can have any of the forms listed in .With OpenGL ES 2.0, only the last row in the
    above table applies; it needs you to define your own custom attributes
    however you see fit. For example, recall that both rendering engines in
    Hello Arrow enabled two attributes:The ES 1.1 backend enabled attributes using
    constants provided by OpenGL, while the ES 2.0 backend used constants that
    were extracted from the shader program (
    and ). Both backends specified the
    dimensionality and types of the vertex attributes that they
    enabled:The data type of each vertex attribute can be
    one of the forms in . With ES 2.0, all of
    these types may be used; with ES 1.1, only a subset is permitted,
    depending on which attribute you are specifying (see the far right column
    in ).The position attribute in OpenGL ES 1.1 is a
    bit of a special case because it's the only required attribute. It can be
    specified as a two-dimensional, three-dimensional, or four-dimensional
    coordinate. Internally, the OpenGL implementation always converts it into
    a four-dimensional (4D) floating-point number. dimensional? This
    might conjure images of Dr. Who, but it actually has nothing to do with
    time, nor anything else in physics; it's an artificial construction that
    allows all transformations to be represented with matrix multiplication.
    These 4D coordinates are known as . When converting a 3D coordinate into a homogeneous
    coordinate, the fourth component (also known as )
    usually defaults to one. A  of zero is rarely found,
    but can be taken to mean "point at infinity". (One of the few places in
    OpenGL that uses  is light source positioning, as
    we'll see in Chapter 4.) Specifying a vertex with a negative
     is almost never useful.Homogeneous coordinates came into existence
      in 1827 when August Ferdinand Möbius published . This is the same Möbius of Möbius
      strip fame. Just for fun, we’ll discuss how to render a Möbius strip
      later in the book. Incidentally, Möbius also invented the concept of
       coordinates, which is leveraged by the
      graphics chip in your iPhone when computing the interior colors of
      triangles. This term stems from the word "barycentre", the archaic word
      for center of mass. If you place three weights at the corners of a
      triangle, you can compute the balance point using barycentric
      coordinates. Their derivation is beyond the scope of this book, but an
      interesting aside nonetheless!So, shortly after entering the assembly line,
    all vertex positions become four-dimensional; don't they need to become
    two-dimensional at some point? The answer is yes, at least until Apple
    releases an iPhone with a holographic screen. We'll learn more about the
    life of a vertex and how it gets reduced to two dimensions in the next
    section, but for now let me mention that one of the last transformations
    is the removal of , which is achieved like
    this:This divide-by-w computation is known as the
    . Note that we didn't discard
    ; it'll come in handy later, as you'll see in the
    chapter on depth and realism. and  depict the process of how a vertex goes from
    being four-dimensional to being two-dimensional. This portion of the
    assembly line is commonly known as , or T&L. We'll discuss lighting in ; for now let's focus on the transforms.After each transform, the vertex is said to be
    in a new "space". The original input vertices are in , and are called . In object space, the origin typically lies at the
    center of the object. This is also sometimes known as .When object coordinates are transformed by the
    model-view matrix, they enter . In eye
    space, the origin is the camera position.Next, the vertex position is transformed by the
    projection matrix to enter . It's called
    clip space because it's where OpenGL typically discards vertices that lie
    outside the viewing frustum. This is one of the places where the elusive W
    component comes into play; if the X or Y components are greater than +W or
    less than -W, then the vertex is clipped.With ES 1.1, the steps in  are fixed; every vertex must go through this
    process. With ES 2.0, it's up to you to do whatever transforms you'd like
    before clip space. Typically you'll actually want to perform these same
    transforms anyway.After clipping comes the perspective transform
    mentioned earlier in the chapter. This normalizes the coordinates to [-1,
    +1], so they're known as  at this point. 
    depicts the tranforms that take place after clip space. Unlike the steps
    in , these transforms are integral to both
    ES 1.1 and ES 2.0.The last transform before rasterization is the
    viewport transform, which depends on some values supplied from the
    application. You might recognize this line from
     in Hello Arrow:The arguments to 
    are , ,
    , and . On the iPhone
    you'll probably want width and height to be 320 and 480, but to ensure
    compatibility with future Apple devices (and other platforms) try to avoid
    hard-coding these values by obtaining the width and height at run time,
    just like we did in Hello Arrow.The  function
    controls how X and Y transform to 
    (somewhat inaptly named for mobile devices; you'll rarely have a
    non-fullscreen window!). The transform that takes Z into window space is
    controlled with a different function:In practice, this function is rarely used; its
    defaults are quite reasonable:  and
     default to zero and one, respectively.So, you now have a basic idea of what happens
    to vertex position, but we haven't yet discussed color. When lighting is
    disabled (as it is by default), colors are passed straight through
    untouched. When lighting is enabled, these transforms become germane
    again. We'll discuss lighting in detail in .The assembly line metaphor illustrates how
    OpenGL works behind the scenes, but a photography metaphor is more useful
    when thinking about a 3D application's work flow. When my wife makes an
    especially elaborate Indian dinner, she often asks me to take a photo of
    the feast for her personal blog. I usually perform the following actions
    to achieve this:Arrange the various dishes on the
        table.Arrange one or more light sources.Position the camera.Aim the camera towards the food.Adjust the zoom lens.Snap the picture.It turns out that each of these actions have
    analogues in OpenGL, although they typically occur in a different order.
    Setting aside the issue of lighting (which we'll address in a future
    chapter), an OpenGL program performs the following actions:Adjust the camera's field-of-view angle;
        this is the .Position the camera and aim it in the
        appropriate direction; this is the .For each object:Scale, rotate, and translate the
            object; this is the .Render the object.The product of the model and view matrices is
    known as the . When rendering an
    object, OpenGL ES 1.1 transforms every vertex first by the model-view
    matrix, then by the projection matrix. With OpenGL ES 2.0, you can perform
    any transforms you wish, but it's often useful to follow the same
    model-view/projection convention, at least in simple scenerios.Later we'll go over each of the three
    transforms (projection, view, model) in detail, but first we need to get
    some preliminaries out of the way. OpenGL has a unified way of dealing
    with all transforms, regardless of how they're used. With ES 1.1, the
    current transformation state can be configured by loading matrices
    explicitly, like this:With ES 2.0, there is no inherent concept of
    model-view and projection; in fact,  and
     do not exist in 2.0. Rather, matrices are
    loaded into  which are then
    consumed by shaders. Uniforms are a type of shader connection that we'll
    learn about later, but you can think of them as constants that shaders
    can't modify. They're loaded like this:By now you might be wondering why so many
      OpenGL functions end in  or .
      Many functions (including ) can take
      floating-point arguments, integer arguments, or other types. OpenGL is a
      C API, and C does not support function overloading; the names of each
      function variant must be distinct. The suffix of the function call
      denotes the type as seen in .
      Additionally, if the suffix is followed by , then
      it's a pointer type.ES 1.1 provides additional ways of manipulating
    matrices that do not exist in 2.0. For example, the following 1.1 snippet
    loads in an identity matrix and multiplies it by two other
    matrices:The default model-view and projection matrices
    are identity matrices. The identity transform is effectively a no-op, as
    seen in .For details on how to multiply a vector with
      a matrix, or a matrix with another matrix, check out the code in .It's important to note that this book uses row
    vector notation rather than column vector notation. In , both the left side
     and right side
     are 4D row vectors. That equation
    could, however, be expressed in column vector notation like so:Sometimes it helps to think of a 4D row vector
    as being a 1x4 matrix, and a 4D column vector as being a 4x1 matrix.
    (x denotes the dimensions of a
    matrix where  is the number of rows and
     is the number of columns.) shows a trick for figuring out
    if it's legal to multiply two quantities in a certain order: the inner
    numbers should match. The outer numbers tell you the dimensions of the
    result. Applying this rule, we can see that it's legal to multiply the two
    matrices shown in : the 4D row vector
    (effectively a 1x4 matrix) on the left of the  and the 4x4
    matrix on the right are multiplied to produce a 1x4 matrix (which also
    happens to be a 4D row vector).From a coding perspective, I find that row
    vectors are more natural than column vectors because they look like tiny
    C-style arrays. It's valid to think of them as column vectors if you'd
    like, but if you do so, be aware that the ordering of your transforms will
    flip around. Ordering is crucial because matrix multiplication is not
    commutative.Consider this snippet of ES 1.1 code:With row vectors, you can think of each
    successive transform as being -multiplied with the
    current transform, so the above snippet is equivalent to:With column vectors, each successive transform
    is -multiplied, so the code snippet is actually
    equivalent to:Regardless of whether you prefer row or column
    vectors, you should always think of the last transformation in your code
    as being the first one to be applied to the vertex. To make this apparent
    with column vectors, use parantheses to show the order of
    operations:This illustrates another reason why I like row
    vectors; they make OpenGL's reverse-ordering characteristic a little more
    obvious.Enough of this mathematical diversion; let's
    get back to the photography metaphor and see how it translates into
    OpenGL. OpenGL ES 1.1 provides a set of helper functions that can generate
    a matrix and multiply the current transformation by the result, all in one
    step. We'll go over each of these helper functions in the coming sections.
    Since ES 2.0 does not provide helper functions, we'll also show what they
    do behind the scenes so that you can implement them yourself.Recall that there are three matrices involved
    in OpenGL's setup:Adjust the camera's field-of-view angle;
        this is the .Position the camera and aim it in the
        appropriate direction; this is the .Scale, rotate, and translate each object;
        this is the .We'll go over each these three transforms in
    reverse so that we can present the simplest transformations first.The three most common operations when
      positioning an object in a scene are scale, translation, and
      rotation.The most trivial helper function is
        :The matrix for scale and its derivation are
        shown in . depicts a
        scale transform where s =
        s = 0.5.
          is the case where the x, y, and z scale factors are not all equal to
          the same value. Such a transformation is perfectly valid, but it can
          hurt performance in some cases. OpenGL has to do more work to
          perform the correct lighting computations when non-uniform scale is
          applied.Another simple helper transform is
        , which shifts an object by a fixed
        amount:Intuitively, translation is achieved with
        addition, but recall that homogeneous coordinates allow us to express
        all transformations using multiplication, as seen in . depicts
        a translation transform where t = 0.25 and
        t = 0.5.You might recall this transform from the
        fixed-function variant (ES 1.1) of the Hello Arrow sample:This applies a counter-clockwise rotation
        about the Z-axis. The first argument is an angle in degrees; the
        latter three arguments define the axis of rotation. The ES 2.0
        renderer in Hello Arrow was a bit tedious because it computed the
        matrix manually: depicts a
        rotation transform where the angle is 45°.Rotation about the Z-axis is relatively
        simple, but rotation about an arbitrary axis requires a more complex
        matrix. For ES 1.1, generates the matrix
        for you, so there's no need to get too concerned with its contents.
        For ES 2.0, check out  to see how to
        implement this.By itself, 
        only rotates about the origin, so what if you wish to rotate about an
        arbitrary point ? To accomplish
        this, use a three-step process:Translate by .Perform the rotation.Translate by .For example, to change Hello Arrow to
        rotate about (0, 1) rather than the center, you could do this:Remember, the last transform in your code
        is actually the first one that gets applied!The simplest way to create a view matrix is
      with the popular  function. It's not built into
      OpenGL ES, but it's easy enough to implement it from scratch.
       takes three parameters: a camera position, a
      target location, and an "up" vector to define the camera's orientation
      (see ).Using the three input vectors,
       produces a transformation matrix that would
      otherwise be cumbersome to derive using the fundamental transforms
      (scale, translation, rotation).  is one possible implementation of
      .Note that  uses custom types like
      , , and
      . This isn't pseudocode; it's actual code from
      the C++ vector library in . We'll discuss the
      library later in the chapter.Until this point, we've been dealing with
      transformations that are typically used to modify the model-view rather
      than the projection. ES 1.1 operations such as
       and  always
      affect the current matrix, which can be changed at any time using
      . Initially the matrix mode is
      .What's the distinction between projection and
      model-view? Novice OpenGL programmers sometimes think of the projection
      as being the "camera matrix", but this is an oversimplification, if not
      completely wrong; the position and orientation of the camera should
      actually be specified in the model-view. I prefer to think of the
      projection as being the camera's "zoom lens" because it affects the
      field of view.Camera position and orientation should
        always go in the model-view, not the projection. OpenGL ES 1.1 depends
        on this to perform correct lighting calculations.Two types of projections commonly appear in
      computer graphics: perspective and orthographic. Perspective projections
      cause distant objects to appear smaller, just as they do in real life.
      You can see the difference in .An orthographic projection is usually only
      appropriate for 2D graphics, so that's what we used in Hello
      Arrow:The arguments for 
      specify the distance of the six bounding planes from the origin: left,
      right, bottom, top, near, and far. Note that our example arguments
      create an aspect ratio of 2:3; this is appropriate since the iPhone's
      screen is 320x480. The ES 2.0 renderer in Hello Arrow reveals how the
      orthographic projection is computed:When an orthographic projection is centered
      around the origin, it's really just a special case of the scale matrix
      that we already presented on page :Since Hello Cone (the example you'll see
      later in this chapter) will have true 3D rendering, we'll give it a
      perspective matrix using the  command, like
      this:The arguments to
       are the same as
      . Since  does not
      exist in ES 2.0, Hello Cone's 2.0 renderer will compute the matrix
      manually, like this:When a perspective projection is applied, the
      field of view is in the shape of a frustum. The viewing frustum is just
      a chopped-off pyramid with the eye at the apex of the pyramid (see ).A viewing frustum can also be computed based
      on the angle of the pyramid's apex (known as ); some developers find these to be more intuitive than
      specifying all six planes. The function in  takes four
      arguments: the field-of-view angle, the aspect ratio of the pyramid's
      base, and the near and far planes.For perspective projection, avoid setting
        your near or far plane to zero or a negative number. Mathematically
        this just doesn't work out.Recall that the ES 1.1 renderer in Hello
    Arrow used  and
     to save and restore the transformation
    state:It's fairly standard practice to wrap the
     method in a Push/Pop block like this, because it
    prevents transformations from accumulating from one frame to the
    next.In the above example, the matrix stack is never
    more than two entries deep, but the iPhone allows up to sixteen stack
    entries. This facilitates complex sequences of transforms, such as those
    required to render the articulated arm in ,
    or any other hierarchical model. When writing code with frequent pushes
    and pops, it helps to add extra indentation, as in .Each matrix mode has its own stack, as depicted
    in ; typically
     gets the heaviest use. Don't worry about
    the  stacks, we'll cover them in another
    chapter. Earlier we mentioned that OpenGL transforms every vertex position
    by the "current" model-view and projection matrices, by which we meant the
    topmost element in their respective stacks. To switch from one stack to
    another, use .Matrix stacks do not exist in ES 2.0; if you
    need them, you'll need to create them in your application code, or in your
    own math library. Again, this may seem cruel, but always keep in mind that
    ES 2.0 is a "closer to the metal" API, and that it actually gives you much
    more power and control through the use of shaders.As we've seen so far, OpenGL performs quite a
    bit of math behind the scenes. But ultimately OpenGL is just a low-level
    graphics API and not an animation API. Luckily the math required for
    animation is quite simple.To sum it up in five words: . An application's animation system
    will often take a set of  from an artist,
    user, or algorithm. At runtime, it computes values between those
    keyframes. The type of data associated with keyframes can be anything, but
    typical examples are color, position, and rotation.The process of computing an intermediate
      frame from two keyframes is called . If you
      divide elapsed time by desired duration, you get a  between zero and one. There are three easing equations
      discussed here, depicted in . The
      tweened value for blending weight  can be computed
      as follows:Certain types of animation should not use
      linear tweening; a more natural look can often be achieved with one of
      Robert Penner's . Penner's
      quadratic ease-in is fairly straightforward:Penner's "quadratic ease-in-out" equation is
      a bit more complex, but relatively easy to follow when splitting it into
      multiple steps, as in .For position keyframes and color keyframes,
      it's easy to perform interpolation: simply call one the aforementioned
      tweening functions on each of the XYZ or RGB components. At first,
      rotation seems simple too; it's just a matter of tweening the angle. But
      what if you're interpolating between two orientations that don't have
      the same axis of rotation?Picture the robot arm example in . This example was restricted to the plane, but
      consider what you'd need if each joint were a ball joint. Storing an
      angle for each joint would be insufficient — you'd also need the axis of
      rotation. This is known as  notation,
      and requires a total of four floating-point values for each
      joint.It turns out there's an artful way to
      represent an arbitrary rotation using the same number of floats as
      Axis-Angle, but in a way that often better lends itself to
      interpolation. This type of four-dimensional vector is called a
      , and it was conceived in 1843.
      Quaternions were somewhat marginalized when modern vector algebra came
      about, but they experienced a revival in the computer graphics era. Ken
      Shoemake is one of the people who popularized them in the late 1980's
      with his famous equation for interpolating
      between two quaternions.Shoemake's method is actually only one of
        several methods of quaternion interpolation, but it's the most
        popular, and it's the one we use in our vector library. Other methods,
        such as normalized quaternion lerp and log-quaternion lerp, are
        sometimes more desirable in terms of performance.Having said all this, be aware that
      quaternions aren't always the best way to handle an animation problem.
      Sometimes it suffices to simply compute the angle between two vectors,
      find an axis of rotation, and interpolate the angle. However,
      quaternions solve a slightly more complex problem; they don't merely
      interpolate between two vectors, they interpolate between two
      . This may seem pedantic, but it's an
      important distinction. Hold your arm straight out in front of you, palm
      up. Now, bend your arm at the elbow while simultaneously rotating your
      hand. What you've just done is interpolate between two
      orientations.It turns out that quaternions are
      particularly well-suited to the type of "trackball" style rotation that
      we'll be using in much of our sample code. I won't bore you with a bunch
      of equations here, but you can check out  to
      see how to implement quaternions. We'll leverage this in the Hello Cone
      sample, and in the wireframe viewer presented in the next
      chapter.Recall the vertex structure in Hello
    Arrow:If we keep using vanilla C arrays like this
    throughout the book, life would become very tedious! What we really want
    is something like this:This is where the power of C++ operator
    overloading and class templates really shines. It makes it possible (in
    fact, it makes it ) to write a small class
    library that makes much of your application code look like it's written in
    a vector-based language. In fact, that's what we've done for the samples
    in this book. Our entire library consists of only three header files and
    no cpp files:Defines a suite of two-dimensional,
          three-dimensional, and four-dimensional vector types that can be
          either float-based or integer-based. Has no dependencies on any
          other header.Defines classes for 2x2, 3x3, and 4x4
          matrices. Depends only on .Defines a class for Quaternions and
          provides several methods for interpolation and construction. Depends
          on .These files are listed in their entirety in
    , but to give you a taste of how the library is
    structured, portions of  are shown in .Note how we parameterized each vector type
    using C++ templates. This allows the same logic to be used for both
    float-based vectors and integer-based vectors.Even though a 2D vector has much in common with
    a 3D vector, we chose not to share logic between them. This could've been
    achieved by adding a second template argument for dimensionality, as
    in:When designing a vector library, it's important
    to strike the right balance between generality and readability. Since
    there's relatively little logic in each vector class, and since we rarely
    need to iterate over vector components, defining separate classes seems
    like a reasonable way to go. It's also easier for readers to understand
    the meaning of say,  than
    .Since a good bit of application code will be
    making frequent use of these types, the bottom of  defines some abbreviated names using typedefs.
    Lowercase names like  and 
    break the naming convention we've established for types, but they adopt a
    look and feel similar to native types in the language itself.The  style names
    in our C++ vector library are directly pilfered from keywords in GLSL.
    Take care not to confuse this book's C++ listings with shader
    listings.In GLSL shaders, types like  and
       are built into the language itself. Our C++
      vector library merely mimics them.We're finally ready to upgrade the Hello Arrow
    program into Hello Cone. We'll not only go from rendering in 2D to
    rendering in 3D, we'll also support two new orientations for when the
    device is held face up or face down.Even though the visual changes are significant,
    they'll all occur within  and
    . That's the beauty of the
    layered, interface-based approach presented in the previous chapter. First
    we'll deal exclusively with the ES 1.1 renderer,
    .The implementations of Hello Arrow and Hello
      Cone diverge in several ways, as seen in .I decided to use the C++ STL (Standard
        Template Library) in much of this book's sample code. The STL
        simplifies many tasks by providing classes for commonly used data
        structures, such as resizeable arrays ()
        and doubly-linked lists (). Many
        developers would argue against using STL on a mobile platform like the
        iPhone when writing performance-critical code. It's true that sloppy
        usage of STL can cause your application's memory footprint to get out
        of hand, but nowadays, C++ compilers do a great job at optimizing STL
        code. Keep in mind that the iPhone SDK provides a rich set of
        Objective C classes (e.g., ) that are
        analagous to many of the STL classes, and they have similar costs in
        terms of memory footprint and performance.With  in
      mind, take a look at the top of
      , shown in  (note that
      this moves the definition of  is higher
      up in the file than it was before, so you'll need to remove the old
      version of this struct from this file).If you'd like to follow along in code as you read, make a copy
        of the HelloArrow project folder in the Finder, and save it as
        HelloCone. Open the project in Xcode, then select Rename from the
        Project menu. Change the project name to HelloCone and click Rename.
        Next, visit  and add
        , , and
         to the project.
         will be almost completely
        different, so open it up and remove all its content. Now you're ready
        to make the changes shown in this section as you read along. The 
          structure enables smooth three-dimensional transitions. It includes
          quaternions for three orientations: the starting orientation, the
          current interpolated orientation, and the ending orientation. It
          also includes two time spans:  and
          , both of which are in seconds. They'll
          be used to compute a slerp fraction between 0 and 1.The triangle data live in two STL
          containers,  and .
          The  container is ideal because we know how
          big it needs to be ahead of time, and it guarantees contiguous
          storage. Contiguous storage of vertices is an absolute requirement
          for OpenGL.Unlike Hello Arrow, there are
           renderbuffers here. Hello Arrow was
          two-dimensional and therefore only required a color renderbuffer.
          Hello Cone requires an additional renderbuff for depth. We'll learn
          more about the depth buffer in a future chapter; briefly, it's a
          special image plane that stores a single Z value at each
          pixel.The construction methods are very similar to
      what we had in Hello Arrow:The  method, shown in , is
      responsible for generating the vertex data and setting up the
      framebuffer. It starts off by defining some values for the cone's
      radius, height, and geometric level of detail. The level of detail is
      represented by the number of vertical "slices" that constitute the cone.
      After generating all the vertices, it initializes OpenGL's framebuffer
      object and transform state. It also enables depth testing since this a
      true 3D app. We'll learn more about depth testing in Chapter 4.Much of  is standard
        procedure when setting up an OpenGL context, and much of it will
        become more clear in future chapters. For now, here's a brief
        summary:Define some constants to use when
            generating the vertices for the disk and cone.Generate an id for the depth
            renderbuffer, bind it, and allocate storage for it. We'll learn
            more about depth buffers later.Generate an id for the framebuffer
            object, bind it, and attach depth and color to it using
            .Bind the color renderbuffer so that
            future rendering operations will affect it.Set up the left, bottom, width, and
            height properties of the viewport.Turn on depth testing since this is a
            3D scene.Set up the projection and model-view
            transforms.
      replaces the two pieces of vertex generation code with ellipses because
      they deserve an in-depth explanation. The problem of decomposing an
      object into triangles is called ,
      but more commonly you'll see the term
      , which actually refers to the
      broader problem of filling a surface with polygons. Tessellation can be
      a fun puzzle, as any M.C. Escher fan knows; we'll learn more about it in
      later chapters.For now let's form the body of the cone with
      a triangle strip and the bottom cap with a triangle fan, as seen in
      .To form the shape of the cone's body, we
      could use a fan rather than a strip, but this would look strange because
      the color at the fan's center would be indeterminate. Even if we pick an
      arbitrary color for the center, an incorrect vertical gradient would
      result, as seen on the left in .Using a strip for the cone isn't perfect
      either because every other triangle is degenerate (shown in gray in
      ). The only way to fix this would be
      resorting to , which requires twice as
      many elements in the vertex array. It turns out that OpenGL provides an
      indexing mechanism to help with situations like this, which we'll learn
      about in the next chapter. For now we'll use
       and live with the degenerate
      triangles. The code for generating the cone vertices is shown in  and depicted
      visually in  (this code goes after
      the comment  in ). Two
      vertices are required for each slice (one for the apex, one for the
      rim), and an extra slice is required to close the loop (). The total number of vertices is
      therefore  where  is
      the number of slices. Computing the points along the rim is the classic
      graphics algorithm for drawing a circle, and may look familiar if you
      remember your trigonometry.Note that we're creating a grayscale gradient
      as a cheap way to simulate lighting:This is a bit of a hack because the color is
      fixed and does not change as you re-orient the object, but it's good
      enough for our purposes. This technique is sometimes called
      , and we'll learn more about it in
      . We'll also learn how to achieve more
      realistic lighting in . generates vertex data for the disk (this
      code goes after the comment  in ).
      Since it uses a triangle fan, the total number of vertices is
      ; an extra vertex for the center, another for
      closing the loop.To achieve smooth animation,
       calls  on the
      rotation quaternion. When a device orientation change occurs, the
       method starts new animation sequence. These
      methods are shown in .Last but not least, Hello Cone needs a
       method. It's similar to the
       method in Hello Arrow except it makes two draw
      calls, and the  command now has an extra flag
      for the depth buffer.Note the call to
      . In our C++ vector library,
      vectors and matrices have a method called 
      which exposes a pointer to the first innermost element. This is useful
      when passing them to OpenGL.We could've made much of our OpenGL code
        more succinct by changing the vector library such that it provides
        implicit conversion operators in lieu of 
        methods. Personally, I think this would be error prone, and would hide
        too much from the code reader. For similar reasons, STL's
         class requires you to call its
         when you want to get a
        .Because you've only implemented the 1.1 renderer so far, you'll
      also need to enable the  switch at the top of
      . At this point, you can build and run
      your first truly 3D iPhone application! To see the two new orientations,
      try holding the iPhone over your head and at your waist. See  for screenshots of all six device
      orientations.Rather than modify the version of
     from Hello Arrow, it will be
    more instructive if we can start our ES 2.0 backend by copying the
    contents of  over whatever's
    already in , with two exceptions:
    you'll need to save the  and
     methods from the existing
     from Hello Arrow, so copy them
    somewhere safe for the moment. If you're following along, do that now, and
    then you'll be ready to make some changes to the file. The top part of
    RenderingEngine2.cpp is shown in .
    New and changed lines are shown in bold. Some sections of unchanged code
    are shown as a , so don't copy this over the
    existing code in its entirety (just make the changes and additions shown
    in bold).The  method almost
    stays as-is, but this bit is no longer valid:For ES 2.0, this changes to:The  and
     methods are the same as they were for the
    ES 2.0 version of Hello Arrow, no need to list them here. The shaders
    themselves are also the same as Hello Arrow's shaders; remember, the
    lighting is "baked", so simply passing through the colors is
    sufficient.We set up the model-view within the
     method, as seen in . Remember,
     plays a role similar to
     function in ES 1.1.The sequence of events in  is actually quite similar to the sequence in
     on page ; only the details have changed.Next, go through the file and change any
    remaining occurrences of  to
    , including the factory method (and be
    sure to change the name of that method to
    ). You also need to remove any
    occurrences of  and . Now,
    turn off the  switch in
    ; this completes the changes required for
    the shader-based version of Hello Cone. It may seem silly to have added an
    ES 2.0 renderer without having added any cool shader effects, but it
    illustrates the differences between the two APIs.This chapter was perhaps the most academic part
    of the book, but we disseminated some fundamental graphics concepts, and
    cleared up some of the sample code that was glossed over in the first
    chapter.Understanding transforms is perhaps the most
    difficult, but also the most crucial hurdle to overcome for OpenGL
    newbies. I encourage you to experiment with Hello Cone to get a better
    feel for how transformations work. For example, try adding some hard-coded
    rotations and translations to the  method, and
    observe how their ordering affects the final rendering.In the next chapter you'll learn more about
    submitting geometry to OpenGL, and you'll get a primer on the iPhone's
    touch screen.Second star to the right...and straight on 'til
    morning.The iPhone has several input devices, including
  the accelerometer, microphone, and touchscreen, but your application will
  probably make the most use of the touchscreen. Since the screen has
  multi-touch capabilities, your application can obtain a list of several
  touch points at any time. In a way, your 3D application is a "point
  processor": it consumes points from the touchscreen, and produces points
  (e.g., triangle vertices) for OpenGL. So, I thought I'd use the same chapter
  to both introduce the touchscreen and cover some new ways of submitting
  vertices to OpenGL.This chapter also covers some important best practices for vertex
  submission, such as the usage of vertex buffer objects. I would never argue
  with the great man who decreed that premature optimization is the root of
  all evil, but I want to hammer in good habits early on.Towards the end of the chapter, you'll learn how to generate some
  interesting geometry using parametric surfaces. This will form the basis for
  a fun demo app that you'll gradually enhance over the course of the next
  several chapters.In this section, I'll introduce the touch
    screen API by walking through a modification of Hello Cone that makes the
    cone point towards the user's finger. You'll need to change the name of
    the app from "Hello Cone" to "Touch Cone", since the user now touches the
    cone instead of merely greeting it. To do this, make a copy of the project
    folder in the Finder, and name the new folder
    . Next, open the Xcode project (it will
    still have the old name) and select Project→Rename. Change the name to
    TouchCone and click Rename.Apple's multitouch API is actually much richer
    than what we need to expose through our
     interface. For example, Apple's API
    supports the concept of , which is useful
    to robustly handle situations like an interruption from a phone call. For
    our purposes, a simplified interface to the rendering engine is
    sufficient. In fact, we don't even need to accept multiple touches
    simultaneously; the touch handler methods can simply take a single
    coordinate.For starters, let's add three methods to
     for "finger up" (the end of a touch),
    "finger down" (the beginning of a touch), and "finger move". Coordinates
    are passed to these methods using the  type from
    the vector library we added in .  shows the
    modifications to  (new lines are in
    bold).The iPhone notifies your view of touch events
    by calling methods on your  class, which you can
    then override. The three methods we're interested in overriding are
    , , and
    . Open  and
    implement these methods by simply passing on the coordinates to the
    rendering engine:The 
    implementation () is similar to Hello Cone, but the
     and  methods
    become empty.  also notifies the user that the cone is active
    by using  to enlarge the geometry while the
    user is touching the screen. New and changed lines in the class
    declaration are shown in bold. Note that we're removing the
     structure.The only bit of code in  that
    might need some extra explanation is the 
    method; it uses some trigonometric trickery to compute the angle of
    rotation. The best way to explain this is with a diagram, as seen in . Recall from high
    school trig that cosine is "adjacent over hypotenuse". We normalized the
    direction vector, so we know the hypotenuse length is exactly one. Since
    , then . If the
    direction vector points towards the right of the screen, then the rotation
    angle should be reversed, as illustrated on the right. This is because
    rotation angles are counter-clockwise in our coordinate system.Note that  flips
    the Y-axis. The pixel-space coordinates that come from
     have the origin at the upper-left corner of the
    screen, with +Y pointing downwards, while OpenGL (and mathematicians)
    prefer to have the origin at the center, with +Y pointing upwards.That's it, the 1.1 ES version of the Touch Cone
    app is now functionally complete! If you want to compile and run at this
    point, don't forget to turn on the  switch at
    the top of .Let's move on to the ES 2.0 renderer. Open up RenderingEngine2.cpp
    and make the changes shown in bold in . Most of these changes are carried
    over from our ES 1.1 changes, with some minor differences in the
     method.You can now turn off the  switch in
    , and build and run Touch Cone on any Apple
    device. In the following sections we'll continue making improvements to
    the app, focusing on how to efficiently describe the cone geometry.So far we've been using the  function
    for all our rendering. OpenGL ES offers another way of kicking off a
    sequence of triangles (or lines or points) through the use of the
     function. It has much the same effect as
    , but instead of simply plowing forward
    through the vertex list, it first reads a list of indices from an
    , then uses those indices to choose
    vertices from the vertex buffer.To help explain indexing and how it's useful, let's go back to the
    simple "square from two triangles" example from the previous chapter
    (). Here's one way of rendering the square
    with :Note that two vertices — (0, 0) and (1, 1) — appear twice in the
    vertex list. Vertex indexing can eliminate this redundancy. Here's
    how:So, instead of sending 6 vertices to OpenGL (8
    bytes per vertex), we're now sending 4 vertices plus 6 indices (one byte
    per index). That's a total of 48 bytes with
     and 38 bytes with
    .You might be thinking: "But I can just use a triangle strip with
    , and save just as much memory!" That's
    true in this case. In fact, a triangle strip is the best way to draw our
    lonely little square:That's only 48 bytes, and adding an index buffer would buy us
    nothing.However, more complex geometry (such as our cone model) usually
    involves even more repetition of vertices, so an index buffer offers much
    better savings. Moreover,  is great in
    certain cases, but in general it isn't as versatile as
    . With , a
    single draw call can be used to render multiple disjoint pieces of
    geometry. To achieve best performance with OpenGL, it's best to execute as
    few draw calls per frame as possible.Let's walk through the process of updating
    Touch Cone to use indexing. Take a look at these two lines in the class
    declaration of :Indexing allows you to combine these two
    arrays, but it also requires a new array for holding the indices. OpenGL
    ES support two types of indices:  (16 bit) and
     (8 bit). In this case, there are fewer than 256
    vertices, so you can use  for best efficiency.
    Replace those two lines with the following:Since the index buffer is partitioned into two
    parts (body and disk) we also added some counts that will get passed to
    , as you'll see later.Next you need to update the code that generates the geometry. With
    indexing, the number of required vertices for our cone shape is
    , where  is the number of
    slices. There are  vertices at the apex, another
     vertices at the rim, and one vertex for the center
    of the base.  shows how to
    generate the vertices. This code goes inside the
     method of the rendering engine class; before
    you insert it, delete everything between  and .In addition to the vertices, you need to store
    indices for  triangles, which requires a total of
     indices.
    uses exploded views to show the tessellation of a cone with
    . The image on the left depicts the ordering of
    the vertex buffer; the image on the right depicts the ordering of the
    index buffer. Note that each vertex at the rim is shared between four
    different triangles; that's the power of indexing! Remember, the vertices
    at the apex cannot be shared because each of those vertices requires a
    unique color attribute, as discussed in the previous chapter (see ).The code for generating indices is shown in
     (again, this code lives in
    our  method). Note the usage of the modulo
    operator to wrap the indices back to the start of the array.Now it's time to enter in the new
     method, shown in . Take a close look at the core of the
    rendering calls (in boldface). Recall that the body of the cone has a
    grayscale gradient, but the cap is solid white. The
     call that renders the body should heed the color
    values specified in the vertex array, but the draw call for the disk
    should not. So between the two calls to ,
    the  attribute is turned off with
    , and the color is explicitly set
    with . Replace the definition of
     in its entirety with the following:You should be able to build and run at this
    point. Next, modify the ES 2.0 backend by making the same changes we just
    went over. The only tricky part is the  method,
    shown in . From a thirty
    thousand foot view, it basically does the same thing as its ES 1.1
    counterpart, but with some extra footwork at the beginning for setting up
    the transformation state.That covers the basics of index buffers; we
    managed to reduce the memory footprint by about 28% over the non-indexed
    approach. Optimizations like this don't matter much for silly demo apps
    like this one, but applying them to real-world apps can make a big
    difference.OpenGL provides a mechanism called
     (often known as VBOs) whereby
    you give it ownership of a set a vertices (and/or indices), allowing you
    to free up CPU memory, and avoid frequent CPU-to-GPU transfers. Using VBOs
    is such a highly recommended practice that I considered using them even in
    the Hello Arrow sample. Going forward, all sample code in this book will
    use VBOs.Let's walk through the steps required to add
    VBOs to Touch Cone. First, remove these two lines from the RenderingEngine
    class declaration:They're no longer needed because the vertex
    data will be stored in OpenGL memory. You do however need to store the
    handles to the vertex buffer objects. Object handles in OpenGL are of type
    . So, add these two lines to the class
    declaration:The vertex generation code in the
     method stays the same except that you should
    use a temporary variable rather than a class member for storing the vertex
    list. Specifically, replace this snippet:with this:Next you need to create the vertex buffer
    objects and populate them. This is done with some OpenGL function calls
    that follow the same Gen/Bind pattern that you're already using for
    framebuffer objects. The Gen/Bind calls for VBOs are shown here (don't add
    these snippets to the class just yet): generates a
    list of non-zero handles.  specifies the
    desired number of handles;  points to a
    pre-allocated list. In this book we often generate only one handle at a
    time, so be aware that the  functions can also be
    used to efficiently generate several handles at once.The  function
    attaches a VBO to one of two binding points specified with the
     parameter. The legal values for
     are
     (used for indices) and
     (used for vertices).Populating a VBO that's already attached to one
    of the two binding points is accomplished with this function call: is the same as it
    is in ,  is the
    number of bytes in the VBO ( is a typedef of
    ),  points to the source
    memory, and  gives a hint to OpenGL about how
    you intend to use the VBO. The possible values for
     are:This is what we'll commonly use in this
          book; it tells OpenGL that the buffer never changes.This tells OpenGL that the buffer will be
          periodically updated using
          .This tells OpenGL that the buffer will be
          frequently updated (e.g., once per frame) with
          .To modify the contents of an existing VBO, you
    can use :The only difference between this and
     is the 
    parameter, which specifies a number of bytes from the start of the VBO.
    Note that  should only be used to update
    a VBO that has previously been initialized with
    .We won't be using
     in any of the samples in this book.
    Frequent updates with  should be avoided
    for best performance, but in many scenerios it can be very useful.Getting back to Touch Cone, let's add code to
    create and populate the VBOs near the end of the
     method:Before showing you how to use VBOs for
    rendering, let me refresh your memory on the 
    functions that you've been using in the 
    method:The formal declarations for these functions
    look like this:The  parameter in
    all these functions controls the number of vector components per
    attribute. (The legal combinations of  and
     were covered in the previous chapter in .) The  parameter is
    the number of bytes between vertices. The 
    parameter is the one to watch out for — when no VBOs are bound (i.e., the
    current VBO binding is zero), it's a pointer to CPU memory; when a VBO is
    bound to GL_ARRAY_BUFFER, it changes meaning and becomes a byte offset
    rather than a pointer.The  functions are
    used to set up vertex attributes, but recall that indices are submitted
    through the last argument of . Here's the
    formal declaration of : is another
    "chameleon" parameter. When a non-zero VBO is bound to
    GL_ELEMENT_ARRAY_BUFFER, it's a byte offset, otherwise it's a pointer to
    CPU memory.The shape-shifting aspect of
       and  is
      an indicator of how OpenGL has grown organically through the years; if
      the API were designed from scratch, perhaps these functions wouldn't be
      so overloaded.To see  and
     being used with VBOs in Touch Cone, check
    out the  method in . shows
    the ES 2.0 variant. From thirty thousand feet it basically does the same
    thing, even though many of the actual OpenGL calls are different.That wraps up the tutorial on VBOs; we've taken
    the Touch Cone sample as far as we can take it!Let's use vertex buffer objects and the touch
    screen to create a fun new app. Instead of relying on triangles like we've
    been doing so far, we'll use  topology to
    create a simple wireframe viewer, as shown in . The rotation in Touch Cone was
    restricted to the plane, but this app will let you spin the geometry
    around to any orientation; behind the scenes, we'll use quaternions to
    achieve a trackball-like effect. Additionally, we'll include a row of
    buttons along the bottom of the screen to allow the user to switch between
    different shapes. They won't be true buttons in the UIKit sense; remember,
    for best performance, you should let OpenGL do all the rendering. This
    application will provide a good foundation upon which to learn many OpenGL
    concepts, and we'll continue to evolve it in the coming chapters.If you're planning on following along with the code, you'll first
    need to start with WireframeSkeleton project from the book's example code
    (see ). In the Finder, make a copy of
    the directory that contains this project and name the new directory
    . Next, open up the project (it will
    still be named WireframeSkeleton), then choose Project→Rename. Rename it
    to SimpleWireframe.This skeleton project includes all the building blocks you saw (the
    vector library from , the
     class, and the Application Delegate). There are
    a few differences between this and the previous examples, so be sure to
    look over the classes in the project before you proceed:The application delegate has been renamed to have a very generic
        name, "AppDelegate".The GLView class uses an 
        rather than a . This is because
        we'll be taking a new approach to how we factor the ES 1.1- and ES
        2.0-specific code from the rest of the project; more on this
        shortly.You might have been put off by all the work
      required for tessellating the cone shape in the previous samples. It
      would be painful if you had to figure out a clever tessellation for
      every shape that pops into your head! Thankfully, most 3D modelling
      software can export to a format that has post-tessellated content; the
      popular  file format is one example of this.
      Moreover, the cone shape happens to be a mathematically-defined shape
      called a ; all parametric
      surfaces are relatively easy to tessellate in a generic manner. A
      parametric surface is defined with a function that takes a 2D vector for
      input and produces a 3D vector as output. This turns out to be
      especially convenient because the input vectors can also be used as
      texture coordinates, as we'll learn in a future chapter.The input to a parametric function is said to
      be in its , while the output is said to be
      in its . Since all parametric surfaces can
      be used to generate OpenGL vertices in a consistent manner, it makes
      sense to create a simple class hierarchy for them.  shows two sub-classes: a cone and a sphere.
      This has been included in the WireframeSkeleton project for your
      convenience, so there is no need for you to add it here.The classes in 
      request their desired tesselation granularity and domain bound by
      calling  from their constructors. More
      importantly, these classes implement the pure virtual
       method, which simply applies  or .Each of the above equations is only one of
      several possible parameterizations for their respective shapes. For
      example, the z equation for the sphere could be negated, and it would
      still describe a sphere.In addition to the cone and sphere, the
      Wireframe Viewer allows the user to see four other interesting
      parametric surfaces: a torus, a knot, a Möbius strip, and a Klein bottle (see ).
      I've already shown you the classes for the sphere and cone; code for the
      other shapes can be found at the book's web site. They basically do
      nothing more than evaluate various well-known parametric equations.
      Perhaps more interesting is their common base class, shown in . To add this file to Xcode,
      right-click the Classes folder and choose , select , and choose
      . Call it
       and replace everything in it
      with the code shown here.True Möbius strips are one-sided surfaces, and can cause
          complications with the lighting algorithms presented in the next
          chapter. The Wireframe Viewer actually renders a somewhat flattened
          Möbius "tube".We'll explain the 
      interface later; first let's take a look at various elements that are
      controlled by subclasses:The number of divisions that the surface
          is sliced into. The higher the number, the more lines, and the
          greater the level of detail. Note that it's a
          ; in some cases (like the knot shape), it's
          desirable to have more slices along one axis than the other.The domain's upper bound. The lower bound
          is always (0, 0).Called from the subclass to describe the
          domain interval.Abstract method for evaluating the
          parametric equation.
      shows the implementation of the 
      class. Add a new C++ File to your Xcode project called
       (but deselect the option to
      create the associated header file). Replace everything in it with the
      code shown.The 
      method deserves a bit of an explanation. Picture a globe of the Earth,
      and how it has lines for latitude and longitude. The first two indices
      in the loop correspond to a latitudinal line segment; the latter two
      correspond to a longitudinal line segment (see ). Also note some sneaky usage of the modulo
      operator for wrapping back to zero when closing a loop.In the Hello Cone and Hello Arrow samples,
      you might have noticed some duplication of logic between the ES 1.1 and
      ES 2.0 backends. With the Wireframe Viewer sample, we're raising the bar
      on complexity, so we'll avoid duplicated code by introducing a new C++
      component called  (this was
      mentioned in chapter 1; see ). The application engine will contain all
      the logic that isn't coupled to a particular graphics API. shows the
      contents of , which defines three
      component interfaces and some related types. Add a new C and C++ header
      file to your Xcode project called .
      Replace everything in it with the code shown.Consumed by ;
          contains logic common to both rendering backends.Consumed by the rendering engines when
          they generate VBOs for the parametric surfaces.Describes the dynamic visual properties
          of a surface; gets passed from the application engine to the
          rendering engine at every frame.Common abstraction of the two OpenGL ES
          backends.Factory method for the application
          engine; the caller determines OpenGL capabilities and passes in the
          appropriate rendering engine.The factory methods for the two rendering
          engines (and their implementations) now live in separate
          namespaces.In an effort to move as much logic into the
      application engine as possible,  has
      only two methods:  and
      . We'll describe them in detail later.To ensure high portability of the application
      logic, we avoid making any OpenGL calls whatsoever from within the
       class.  is the complete listing of its
      initial implementation. Add a new C++ File to your Xcode project called
       (deselect the option to
      create the associated  file). Replace everything
      in it with the code shown.The bulk of  is dedicated to handling the
      trackball-like behavior with quaternions. I find the
       method to be the most natural way
      of constructing a quaternion. Recall that it takes two unit vectors at
      the origin, and computes the quaternion that moves the first vector onto
      the second. To achieve a trackball effect, these two vectors are
      generated by projecting touch points onto the surface of the virtual
      trackball (see the  method). Note that if
      a touch point is outside the circumference of the trackball (or directly
      on it), then  snaps the touch point to
      just inside the circumference. This allows the user to perform a
      constrained rotation about the Z-axis by sliding his finger horizontally
      or vertically near the edge of the screen.So far we've managed to exhibit most of the
      Wireframe Viewer code without any OpenGL whatsoever! It's time to remedy
      that by showing the ES 1.1 backend class in . Add a new C++ File to your Xcode
      project called  (deselect
      the option to create the associated  file).
      Replace everything in it with the code shown. The ES 2.0 version can be
      downloaded from the book's website (and is included with the skeleton
      project mentioned early in this section).There are no new OpenGL concepts here; you
      should be able to follow the code in . We now have all the big pieces in place
      for the Wireframe Viewer. At this point, it only shows a single
      wireframe; this is improved in the coming sections.Apple provides the
       widget as a part of the UIKit framework.
      This is the familiar list of gray icons that many applications have
      along the bottom of the screen, as seen in .Since UIKit widgets are outside the scope of
      this book, you'll be using OpenGL to create a poor man's tab bar for
      switching between the various parametric surfaces, as in .In many situations like this, a standard
       is preferable since it creates a more
      consistent look with other iPhone applications. But in our case, we'll
      create a fun transition effect: pushing a button will cause it to "slide
      out" of the tab bar and into the main viewport. For this level of
      control over rendering, UIKit doesn't suffice.The Wireframe Viewer has a total of six
      parametric surfaces, but the button bar has only five. When the user
      touches a button, we'll swap its contents with the surface being
      displayed in the main viewport. This allows the application to support
      six surfaces with only five buttons.The state for the five buttons and the
      button-detection code lives in the application engine. New lines in the
      class declaration from  are
      shown in bold in . No
      modifications to the two rendering engines are required.The implementation is shown in . Methods left unchanged (such as
      ) are omitted for brevity. You'll be
      replacing the following methods:
      ,
      , ,
      , , and
      . There are two new methods you'll be
      adding:  and
      .Go ahead and try it out — at this point,
        the Wireframe Viewer is starting to feel like a real
        application!The button-swapping strategy is clever but
      possibly jarring to users; after playing with the app for a while, the
      user might start to notice that his tab bar is slowly being re-arranged.
      To make the swap effect more obvious, and to give the app more of a fun
      Apple feel, let's create a transition animation that actually shows the
      button being swapped with the main viewport. This animation is depicted
      in .Again, no changes to the two rendering
      engines are required, as all the logic can be constrained to
      . In addition to animating the
      viewport, we'll also animate the color (the tab bar wireframes are drab
      gray) and the orientation (the tab bar wireframes are all in the "home"
      position). We can re-use the existing  class
      for this; we need two sets of  objects for the
      start and end of the animation. While the animation is active, we'll
      tween the values between the starting and ending visuals. Let's also
      create an  structure to bundle the visuals
      with a few other animation parameters, as seen in bold in .
      shows the new implementation of .
      Unchanged methods are omitted for brevity. Remember, animation is all
      about interpolation! The  method leverages the
       and  methods from our
      vector class library to achieve the animation in a surprisingly
      straightforward manner.That completes the wireframe viewer! As you
      can see, animation isn't difficult, and it can give your application
      that special Apple touch.This chapter has been a quick exposition of the
    touch screen and OpenGL vertex submission. The toy wireframe app is great
    fun, but it does have a bit of a 1980's feel to it. The next chapter takes
    iPhone graphics to the next level by explaining the depth buffer,
    exploring the use of real-time lighting, and showing how to load 3D
    content from the popular  file format. While this
    chapter has been rather heavy on code listings, the next chapter will be
    more in the style of , mixing in some math review
    with a lesson in OpenGL.Lumos!When my wife and I go out to see a film packed
  with special effects, I always insist on sitting through the entire end
  credits, much to her annoyance. It never ceases to amaze me how many artists
  work together to produce a Hollywood blockbuster. I'm often impressed with
  the number of artists whose full-time job concerns lighting. In Pixar's
  , at least five people have the title "lighting
  technical director", four people have the title "key lighting artist", and
  another four people have the honor of "master lighting artist".Lighting is obviously a key aspect to
  understanding realism in computer graphics, and that's much of what this
  chapter is all about. We'll refurbish the Wireframe Viewer sample to use
  lighting and triangles, rechristening it to "Model Viewer". We'll also throw
  some light on the subject of shaders, which we've been glossing over till
  now (in ES 2.0, shaders are critical to lighting). Finally, we'll further
  enhance the viewer app by giving it the ability to load model files, so that
  we're not stuck with parametric surfaces forever. Mathematical shapes are
  great for geeking out, but they're pretty lame for impressing your ten
  year-old!Before diving into lighting, let's take a
    closer look at depth buffers, since we'll need to add one to Wireframe
    Viewer. You might recall the funky frame buffer object (FBO) set-up code
    in the Hello Cone sample presented in , repeated here in .Create a handle to the renderbuffer object
        that stores depth.Bind the newly created handle, making it
        affected by subsequent renderbuffer commands.Allocate storage for the depth buffer using
        16-bit precision.Attach the depth buffer to the framebuffer
        object.Enable depth testing — we'll explain this
        shortly.Why does Hello Cone need a depth buffer when
    Wireframe Viewer does not? When the scene is composed of nothing but
    monochrome lines, we don't care about ; this means we don't care which lines are obscured by
    other lines. Hello Cone uses triangles rather than lines, so the
    visibility problem needs to be addressed. OpenGL uses the depth buffer to
    handle this problem efficiently. is a
    depiction of Model Viewer's depth buffer in grayscale: white pixels are
    far away, black pixels are nearby. Even though users can't see the depth
    buffer, OpenGL needs it for its rendering algorithm. If it didn't have a
    depth buffer, you'd be forced to carefully order your draw calls from
    furthest to nearest. (Incidentally, such an ordering is called the
    , and there are special cases
    where you'll need to use it anyway, as you'll see in a future chapter that
    covers blending.)OpenGL uses a technique called  to solve the visibility problem. Suppose you were to
    render a red triangle directly in front of the camera, then draw a green
    triangle directly behind the red triangle. Even though the green triangle
    is drawn last, you'd want to the red triangle to be visible; the green
    triangle is said to be . Here's how it
    works: every rasterized pixel not only has its RGB values written to the
    color buffer, but also has its Z value written to the depth buffer. OpenGL
    "rejects" occluded pixels by checking if their Z value is greater than the
    Z value that's already in the depth buffer. In pseudocode, the algorithm
    looks like this:For perspective projections, having the
          near plane close to zero can be detrimental to precision.Similarly, the far plane should still be
          pulled in as far as possible without clipping away portions of your
          scene.Try to avoid defining an
          astronomical-scale scene with huge extents.All iPhones and iPod touches (at the time
          of this writing) support 16-bit and 24-bit depth formats. The bit
          width is determined according to the argument you pass to
           when allocating the
          depth buffer.The fault might not lie with OpenGL but
          with your application code. Perhaps your generated vertices are
          lying on the same Z plane due to a rounding error.In some cases you should probably disable
          depth testing anyway. For example, you don't need it if you're
          rendering a two-dimensional heads-up-display. Disabling the depth
          test can also boost performance.Something to watch out for with depth buffers
      is , which is a visual artifact that
      occurs when overlapping triangles have depths that are too close to each
      other (see ).Recall that the projection matrix defines a viewing frustum
      bounded by six planes (). The two
      planes that are perpendicular to the viewing direction are called the
      . In ES 1.1, these planes
      are arguments to the  or
       functions; in ES 2.0, they're passed to
      a custom function like the  method in
      the C++ vector library from .It turns out that if the near plane is too close to the camera, or
      if the far plane is too distant, this can cause precision issues that
      result in Z-fighting. However this is only one possible cause for
      Z-fighting; there are many more. Take a look at the following list of
      suggestions if you ever see artifacts like the ones in .Let's enhance the Wireframe Viewer app by
    adding in a depth buffer; this paves the way for converting the wireframes
    into solid triangles. Before making any changes, use Finder to make a copy
    of the folder that contains the SimpleWireframe project. Rename the folder
    to ModelViewer, and then open the copy of the SimpleWireframe project
    inside that folder. Select Project→Rename and rename the project to
    ModelViewer.Open , and add
     to the
     section of the class declaration. Next, find
    the  method, and delete everything from the
    comment  to the
     call. Replace the code you
    deleted with the code in .The ES 2.0 variant of  is almost exactly the same. Repeat the process in
    that file, but remove all  and
     suffixes.Next, find the call to  (in both rendering
    engines) and add a flag for depth:At this point, you should be able to compile and run, although
    depth-testing doesn't buy you anything yet since the app is still
    rendering in wireframe.By default, the depth buffer gets cleared to a
    value of 1.0; this makes sense since you want all your pixels to initially
    pass the depth test, and OpenGL clamps the maximum window-space Z
    coordinate to 1.0. Incidentally, if you want to clear the depth buffer to
    some other value, you can call , similar
    to . You can even configure the depth test
    itself using  By default, pixels "win" if
    their Z is  than the value in the depth buffer,
    but you can change the test to any of these conditions:Pixels never pass the depth test.Pixels always pass the depth test.Pixels pass only if their Z value is less
          than the Z value in the depth buffer. This is the default.Pixels pass only if their Z value is less
          than or equal to the Z value in the depth buffer.Pixels pass only if their Z value is
          equal to the Z value in the depth buffer. This could be used to
          create an infinitely thin slice of the scene.Pixels pass only if their Z value is
          greater than the Z value in the depth buffer.Pixels pass only if their Z value is
          greater than or equal to the Z value in the depth buffer.Pixels pass only if their Z value is not
          equal to the Z value in the depth buffer.The flexibility of
     is a shining example of how OpenGL is often
    configurable to an extent more than you really need. I personally admire
    this type of design philosophy in an API; anything that is reasonably easy
    to implement in hardware is exposed to the developer at a low level. This
    makes the API forward-looking because it enables developers to dream up
    unusual effects that the API designers did not necessarily
    anticipate.In this section we'll walk through the steps
    required to render parametric surfaces with triangles rather than lines.
    First we need to enhance the  interface to
    support generation of indices for triangles rather than lines. Open
     and make the changes shown in boldface
    in .You'll also need to open
     and make the complementary
    changes to the class declaration of 
    shown in .Next open  and add the
    implementation of  and
     per . is
    computing indices for two triangles, as shown in .Now we need to modify the rendering engine so
    that it calls these new methods when generating VBOs, as in . The modified lines are shown in bold.
    Make these changes to both 
    and .Getting back to the sample app, at this point
    Wireframe Viewer has officially become Model Viewer; feel free to build it
    and try it out. You may be disappointed — the result is horribly boring,
    as seen in . Lighting to the
    rescue!Before we can enable lighting, there's yet
    another prerequisite we need to get out of the way. To perform the math
    for lighting, OpenGL must be provided with a  at every vertex. A surface normal (often simply called a
    ) is simply a vector perpendicular to the
    surface; it effectively defines the orientation of a small piece of the
    surface.You might recall that normals are one of the
      pre-defined vertex attributes in OpenGL ES 1.1. They can be enabled like
      this:I snuck in something new in the above
      snippet: the  state in ES 1.1, and the
       argument in ES 2.0. Both are used to
      control whether or not OpenGL processes your normal vectors to make them
      unit length. If you already know that your normals are unit length, do
      not turn this feature on; it incurs a performance hit.Don't confuse
        , which refers to making any vector into
        a unit vector, and , which refers to
        any vector that is perpendicular to a surface. It is not redundant to
        say "normalized normal".Even though OpenGL ES 1.1 can perform much of
      the lighting math on your behalf, it does not compute surface normals
      for you. At first this may seem rather ungracious on OpenGL's part, but
      as you'll see later, stipulating the normals yourself give you the power
      to render interesting effects. While the mathematical notion of a normal
      is well-defined, the OpenGL notion of a normal is simply another input
      with discretionary values, much like color and position. Mathematicians
      live in an ideal world of smooth surfaces, but graphics programmers live
      in a world of triangles. If you were to make the normals in every
      triangle point in the exact direction that the triangle is facing, your
      model would looked faceted and artificial; every triangle would have a
      uniform color. By supplying normals yourself, you can make your model
      seem smooth, faceted, or even bumpy, as we'll see later.We scoff at mathematicians for living in an
      artificially ideal world, but we can't dismiss the math behind normals;
      we need it to come up with sensible values in the first place. Central
      to the mathematical notion of a normal is the concept of a
      , depicted in .The diagram in  is, in itself, perhaps the best definition
      of the tangent plane that I can give you without going into calculus.
      It's the plane that "just touches" your surface at a given point
      . Think like a mathematician: for
      them, a plane is minimally defined with three points. So, imagine three
      points at random positions on your surface, then create a plane that
      contains them all. Slowly move the three points towards each other; just
      before the three points converge, the plane they define is the tangent
      plane.The tangent plane can also be defined with
      tangent and binormal vectors ( and
       in ), which are easiest to define within the
      context of a parametric surface. Each of these correspond to a dimension
      of the domain; we'll make use of this when we add normals to our
       class.Finding two vectors in the tangent plane is
      usually fairly easy. For example, you can take any two sides of a
      triangle; the two vectors need not be at right angles to each other.
      Simply take their cross product and unitize the result. For parametric
      surfaces, the procedure can be summarized with the following
      pseudocode:Don't be frightened by the cross product,
      I'll give you a brief refresher. The cross product always generates a
      vector perpendicular to its two input vectors. You can visualize the
      cross product of  with  using your right hand. Point your index finger
      in the direction of , then point your
      middle finger towards ; your thumb now
      points in the direction of × (pronounced "A cross B", not "A times B"). See
      .Here's the relevant snippet from our C++
      library (see  for a full listing):Let's not lose focus on why we're generating
      normals in the first place: they're required for the lighting algorithms
      that we cover later in this chapter. Recall from  that vertex position can live in different spaces:
      object space, world space, and so on. Normal vectors can live in these
      different spaces too; it turns out that lighting in the vertex shader is
      often performed in eye space. (There are certain conditions in which it
      can be done in object space, but that's a discussion for another
      day.)So, we need to transform our normals to eye
      space. Since vertex positions get transformed by the model-view matrix
      to bring them into eye space, it follows that normal vectors get
      transformed the same way, right? Wrong! Actually, wrong
      . This is one of the tricker concepts in
      graphics to understand, so bear with me.Look at the heart shape in  and consider the surface normal at a
      point in the upper-left quadrant (depicted with an arrow). The figure on
      the far left is the original shape and the middle figure shows what
      happens after we translate, rotate, and uniformly shrink the heart. The
      transformation for the normal vector is almost the same as the model's
      transformation; the only difference is that it's a vector and therefore
      doesn't require translation. Removing translation from a 4x4
      transformation matrix is easy. Simply extract the upper-left 3x3 matrix
      and you're done.Now take a look at the figure on the far
      right, which shows what happens when stretching the model only along its
      x-axis. In this case, if we were to apply the upper 3x3 of the
      model-view matrix to the normal vector, we'd get an incorrect result;
      the normal would no longer be perpendicular to the surface. This shows
      that simply extracting the upper-left 3x3 matrix from the model-view
      matrix doesn't always suffice. I won't bore you with the math, but it
      can be shown that the correct transform for normal vectors is actually
      the  of the model-view matrix,
      which is the result of two operations: first an inverse, then a
      transpose.The matrix of
      M is denoted M; it's the matrix that
      results in the identity matrix when multiplied with the original matrix.
      Inverse matrices are somewhat non-trivial to compute, so again I'll
      refrain from boring you with the math. The
       matrix, on the other hand, is easy to
      derive; simply swap the rows and columns of the matrix such that
       becomes .Transposes are denoted M, so the
      proper transform for normal vectors looks like this:Don't forget the middle shape in ; it shows that, at least in some
      cases, the upper 3x3 of the original model-view matrix
       be used to transform the normal vector. In this
      case, the matrix just happens to be equal to its own inverse-transpose;
      such matrices are called . Rigid body
      transformations like rotation and uniform scale always result in
      orthogonal matrices.Why did I bore you with all this mumbo jumbo
      about inverses and normal transforms? Two reasons. In ES 1.1, keeping
      non-uniform scale out of your matrix helps performance because OpenGL
      can avoid computing the inverse-transpose of the model-view. Secondly,
      for ES 2.0, you need to understand nitty-gritty details like this anyway
      to write sensible lighting shaders!Enough academic babble; let's get back to
      coding. Since our goal here is to add lighting to Model Viewer, we need
      to implement the generation of normal vectors. Let's tweak
       in  by
      adding a flags parameter to , as seen
      in . New or modified lines are shown in
      bold.The argument we added to
       could have been a boolean instead of
      a bit mask, but we'll eventually want to feed additional vertex
      attributes to OpenGL, such as texture coordinates. For now, just ignore
      the  flag; it'll come in handy in
      the next chapter.Next we need to open
       and make the complementary
      change to the class declaration of ,
      as seen in . We'll also add
      a new protected method called , which
      derived classes can optionally override.Next let's open
       and replace the
      implementation of , as seen in .Compute the position of the vertex by
          calling , which is has a unique
          implementation for each subclass.Copy the  position
          into the flat floating-point buffer. The 
          method returns an updated pointer.Surfaces might be non-smooth in some
          places where the normal is impossible to determine (for example, at
          the apex of the cone). So, we have a bit of a hack here, which is to
          nudge the point of interest in the problem areas.As covered in , compute the two tangent vectors
          and take their cross product.Subclasses are allowed to invert the
          normal if they wish. (If the normal points away from the light
          source, then it's considered to be the back of the surface and
          therefore looks dark.) The only shape that overrides this method is
          the Klein Bottle.Copy the normal vector into the data
          buffer using its  method.This completes the changes to
      . You should be able to build Model
      Viewer at this point, but it will look the same since we have yet to put
      the normal vectors to good use. That comes next.Drawing is deception.The foundations of real-time graphics are
    almost never based on principles from physics and optics. In a way, the
    lighting equations we'll cover in this section are cheap hacks, simple
    models based on rather shallow empirical observations. We'll be
    demonstrating three different lighting models:
     lighting (subtle, monotone light),
     lighting (the dull matte component of
    reflection), and  lighting (the shiny spot
    on a fresh red apple).  shows how
    these three lighting models can be combined to produce a high-quality
    image.Of course, in the real world, there are no such
    things as "diffuse photons" and "specular photons". Don't be disheartened
    by this pack of lies! Computer graphics is always just a great big hack at
    some level, and knowing this will make you stronger. Even the fact that
    colors are ultimately represented by a red-green-blue triplet has more to
    do with human perception than with optics. The reason we use RGB? It
    happens to match the three types of color sensing cells in the human
    retina! A good graphics programmer can think like a politician and use
    lies to his advantage.Realistic ambient lighting, with the soft,
      muted shadows that it conjures up, can be very complex to render (you
      can see an example of  in ), but ambient lighting in the context of
      OpenGL usually refers to something far more trivial: a solid, uniform
      color. Calling this "lighting" is questionable since its intensity is
      not impacted by the position of the light source or the orientation of
      the surface, but it is often combined with the other lighting models to
      produce a brighter surface.The most common form of real-time lighting is
      , which varies its brightness
      according to the angle between the surface and the light source. Also
      known as , this form of
      lighting is predominant because it's simple to compute, and it
      adequately conveys depth to the human eye.  shows how diffuse lighting works. In
      the diagram,  is the unit length
      vector pointing to the light source, and  is the ,
      which is a unit-length vector that's perpendicular to the surface. We'll
      learn how to compute  later in the
      chapter.The (known as  in ) lies between 0 and 1 and gets
      multiplied with the light intensity and material color to produce the
      final diffuse color, as shown in . is computed by taking
      the dot product of the surface normal with the light direction vector,
      then clamping the result to a non-negative number, as shown in .The dot product is another operation that you
      might need a refresher on. When applied to two unit-length vectors
      (which is what we're doing for diffuse lighting), you can think of the
      dot product as a way of measuring the angle between the vectors. If the
      two vectors are perpendicular to each other, their dot product is zero;
      if they point away from each other, their dot product is negative.
      Specifically, the dot product of two unit vectors is the cosine of the
      angle between them. To see how to compute the dot product, here's a
      snippet from our C++ vector library (see  for
      a complete listing):Don't confuse the dot product with the
        cross product! For one thing, cross products produce vectors, while
        dot products produce scalars.With OpenGL ES 1.1, the math required for
      diffuse lighting is done for you behind the scenes; with 2.0, you have
      to do the math yourself in a shader. You'll learn both methods later in
      the chapter.The  vector
      in  can be computed like
      this:In practice, you can often pretend that the
      light is so far away that all vertices are at the origin. The above
      equation then simplifies to:When you apply this optimization, you're said
      to be using an . Taking each
      vertex position into account is slower but more accurate; this is a
      .I guess you could say the Overlook Hotel
        here has somethin' almost like "shining."Diffuse lighting is not affected by the
      position of the camera; the diffuse brightness of a fixed point stays
      the same, no matter which direction you observe it from. This is in
      contrast to , which moves the
      area of brightness according to your eye position, as seen in . Specular lighting mimics the shiny
      highlight seen on polished surfaces. Hold a shiny apple in front of you
      and shift your head to the left and right; you'll see that the apple's
      shiny spot moves with you. Specular is more costly to compute than
      diffuse because it uses exponentiation to compute falloff. You choose
      the exponent according to how you want the material to look; the higher
      the exponent, the shinier the model.The H vector in  is called the half-angle because it
      divides the angle between the light and the camera in half. Much like
      diffuse lighting, the goal is to compute an intensity coefficient (in
      this case, ) between 0 and 1.  shows how to compute
      .In practice, you can often pretend that the
      viewer is infinitely far from the vertex, in which case the  vector is substituted with (0, 0, 1). This
      technique is called . When  is used, this is called .We'll first add lighting to the OpenGL ES 1.1
      backend since it's much less involved than the 2.0 variant.  shows the new
       method (unchanged portions are replaced
      with ellipses for brevity).Tell the
           object that we need normals by
          passing in the new 
          flag.Enable two vertex attributes: one for
          position, the other for surface normal.Enable lighting and turn on the first
          light source (known as ). The iPhone
          supports up to eight light sources, but we're using only one.The default specular color is black, so
          here we set it to gray, and set the specular exponent to 50. We'll
          set diffuse later.
      uses some new OpenGL functions:  and
      . These are only useful when lighting is
      turned on, and they are unique to ES 1.1 — with 2.0 you'd use
       instead. The declarations for these
      functions are:The  parameter is a
      bit of a carry-over from desktop OpenGL, which allows the back and front
      sides of a surface to have different material properties. For OpenGL ES,
      this parameter must always be set to
      .The  parameter can be
      one of:Specifies the specular exponent as a
            float between 0 and 128. This is the only parameter that you set
            with ; all other parameters require
             because they have four floats
            each.This specifies the ambient color of the
            surface and requires four floats (red, green, blue, alpha). The
            alpha value is ignored, but I always set it to one just to be
            safe.This specifies the specular color of
            the surface and also requires four floats, although alpha is
            ignored.This specifies the emission color of
            the surface. We haven't covered emission because it's so rarely
            used. It's similar to ambient except that it's unaffected by light
            sources. This can be useful for debugging; if you want to verify
            that a surface of interest is visible, set its emission color to
            white. Like ambient and specular, it requires 4 floats and alpha
            ignored.This specifies the diffuse color of the
            surface and requires four floats. The final alpha value of the
            pixel originates from the diffuse color.Using only one function call, this
            allows you to specify the same color for both ambient and
            diffuse.When lighting is enabled, the final color of
      the surface is determined at run time, so OpenGL ignores the color
      attribute that you set with  or
       (see ). Since you'd only specify the color
      attribute when lighting is turned off, it's often referred to as
      .As an alternative to calling
        , you can embed diffuse and ambient
        colors into the vertex buffer itself, through a mechanism called
         When enabled, this redirects the
        non-lit color attribute into the  and
         material parameters. You can enable it
        by calling .Next we'll flesh out the
       method so that it uses normals, as shown in
      . New/changed lines are in
      boldface. Note that we moved up the call to
      ; this is explained further in the
      callouts that follow the listing.Set the position of
          . Be careful about when you make this
          call, because OpenGL applies the current model-view matrix to the
          position, much like it does to vertices. Since we don't want the
          light to rotate along with the model, here we've reset the
          model-view before setting the light position.The non-lit version of the app used
           in the section that started with the
          comment . We're replacing that
          with . More on this later.Point OpenGL to the right place in the
          VBO for obtaining normals. Position comes first and it's a
          , so the normal offset is
          .That's it!  depicts the app now that lighting has been
      added. Since we haven't implemented the ES 2.0 renderer yet, you'll need
      to enable the  constant at the top of
      . introduced a new OpenGL
      function for modifying light parameters,
      :The  parameter
      identifies the light source. Although we're using only one light source
      in Model Viewer, up to eight are allowed ( -
      ).The  argument
      specifies the light property to modify. OpenGL ES 1.1 supports ten light
      properties:As you'd expect, each of these takes
            four floats to specify a color. Note that light colors alone do
            not determine the hue of the surface; they get multiplied with the
            surface colors specified by
            .The position of the light is specified
            with four floats. If you don't set the light's position, it
            defaults to (0, 0, 1, 0). The 
            component should be 0 or 1, where 0 indicates an infinitely
            distant light. Such light sources are just as bright as normal
            light sources, but their "rays" are parallel. This is
            computationally cheaper because OpenGL does not bother
            re-computing the  vector (see
            ) at every vertex.You can restrict a light's area of
            influence to a cone using these parameters. Don't set these
            parameters if you don't need them; doing so can degrade
            performance. I won't go into detail about spotlights since they
            are somewhat esoteric to ES 1.1, and you can easily write a shader
            in ES 2.0 to achieve a similar effect. Consult an OpenGL reference
            to see how to use spotlights. (see )These parameters allow you to dim the
            light intensity according to its distance from the object. Much
            like spotlights, attenuation is surely covered in your favorite
            OpenGL reference book. Again, be aware that setting these
            parameters could impact your frame rate.You may've noticed that the inside of the
      cone appears especially dark. This is because the normal vector is
      facing away from the light source. On third-generation iPhones and iPod
      Touches, you can enable a feature called , which inverts the normals on back-facing
      triangles, allowing them to be lit. It's enabled like this:Use this function with caution, as it is not
      supported on older iPhones. One way to avoid two-sided lighting is to
      re-draw the geometry at a slight offset using flipped normals. This
      effectively makes your one-sided surface into a two-sided surface. For
      example, in the case of our cone shape, we could draw another
      equally-size cone that's just barely "inside" the original cone.Just like every other lighting function,
        doesn't exist under ES 2.0. With ES
        2.0, you can achieve two-sided lighting by using a special shader
        variable called . More on this
        later.Before we add lighting to the ES 2.0 rendering
    engine of Model Viewer, let's go over some shader fundamentals. What
    exactly is a shader? In chapter 1, we mentioned that shaders are
    relatively small snippets of code that run on the graphics processor, and
    that thousands of shader instances can execute simultaneously.Let's dissect the simple vertex shader that
    we've been using in our sample code so far, repeated here in .Declare two 4D floating-point vectors with
        the  storage qualifier. Vertex shaders
        have read-only access to attributes.Declare a 4D floating-point vector as a
        varying. The vertex shader must write to all varyings that it declares
        — if it doesn't, OpenGL's shader compiler will report an error. The
        initial value is undefined.Declare two 4x4 matrices as
        . Much like attributes, the vertex
        shader has read-only access to uniforms. But unlike vertex attributes,
        uniforms cannot change from one vertex to the next.The entry point to the shader. Some shading
        languages let you define your own entry point, but with GLSL, it's
        always .No lighting or fancy math here, just pass
        the  attribute into the
         varying.Here we transform the position by the
        projection and model-view matrices, much like OpenGL ES 1.1
        automatically does on our behalf. Note the usage of
        , which is a special output variable
        built into the vertex shading language.The keywords ,
    , and  are storage
    qualifiers in GLSL.  summarizes the five
    storage qualifiers available in GLSL.Implementation-specific data; pertains only to the
                iPhone 3GS.One way to visualize the flow of shader data is
    shown in . Be aware that this diagram
    is very simplified; for example, it does not include blocks for texture
    memory or program storage.The fragment shader we've been using so far is
    incredibly boring:Declares a 4D floating-point varying
        (read-only) with  precision. Precision
        qualifiers are required for floating-point types in fragment
        shaders.The entry point to every fragment shader is
        its  function. is a
        special built-in vec4 that indicates the output of the fragment
        shader.Perhaps the most interesting new concept here
    is the precision qualifier. Fragment shaders require a precision qualifier
    for all floating-point declarations. The valid qualifiers are
    , , and
    . The GLSL specification gives implementations
    some leeway in the underlying binary format that corresponds to each of
    these qualifiers; specific details for the graphics processor in the
    iPhone 3GS are shown in .An alternative to specifying precision in
      front of every type is to supply a default using the
       keyword. Vertex shaders implicitly have a
      default floating-point precision of . To create
      a similar default in your fragment shader, add  to the top of your shader.Also of interest in  is the 
    variable, which is a bit of a special case. It's a variable that is built
    into the language itself, and always refers to the color that gets applied
    to the framebuffer. The fragment shading language also defines the
    following built-in variables: is an array of output colors
          that has only one element. This exists in OpenGL ES only for
          compatibility reasons; use 
          instead.Input variable that contains window coordinates for the
          current fragment, which is useful for image processing.Boolean input variable; true for front-facing primitives,
          false for back-faces. Use this to implement two-sided
          lighting.This is an input texture coordinate that's used only for point
          sprite rendering; we'll cover it in .OpenGL ES 2.0 does not automatically perform
    lighting math behind the scenes; instead it relies on developers to
    provide it with shaders that perform whatever type of lighting they
    desire. Let's come up with a vertex shader that mimics the math done by ES
    1.1 when lighting is enabled.To keep things simple, we'll use the infinite
    light source model for diffuse ()
    combined with the infinite viewer model for specular (). We'll also assume that the light is white.
    Pseudocode is shown in .Note the 
    variable in the pseudocode; it would be silly to re-compute the
    inverse-transpose of the model-view at every vertex, so we'll compute up
    front in the application code, then pass it in as the
     uniform. In many cases, it happens to be
    equivalent to the model-view, but we'll leave it to the application to
    decide how to compute it.Let's add a new file to the Model Viewer
    project called  for the lighting
    algorithm. In Xcode, right-click the Shaders folder and choose
    . Select the  template in the  category. Name
    it  and add
     after the project folder name in the
    location field. Deselect the checkbox in the Targets list and click
    Finish. translates the pseudocode
    into GLSL. To make the shader usable in a variety of situations, we use
    uniforms to store light position, specular and ambient properties. A
    vertex attribute is used to store the diffuse color; for many models, the
    diffuse color may vary on a per-vertex basis (although in our case, it
    does not). This would allow us to use a single draw call to draw a
    multi-colored model.Remember, we're leaving out the 
      macros in all shader listings from here on out, so take a look at  to see how to add that macro to this file.Take a look back at the pseudocode in ; the vertex shader is an implementation
    of that. The main difference is that GLSL requires you to qualify many of
    the variables as being attributes, uniforms, or varyings. Also note that
    in its final code line,  performs
    the standard transformation of the vertex position, just as it did for the
    non-lit case.GLSL is a bit different from many other
      languages in that it does not autopromote literals from integers to
      floats. For example,  generates a
      compile error, but  does not. On the
      other hand, constructors for vector-based types 
      perform conversion implicitly; it's perfectly legal to write either
       or .To create the ES 2.0 backend to Model Viewer,
      let's start with the ES 1.1 variant and make the following changes, some
      of which should be familiar by now:Copy the contents of
           into
          .Remove the _OES and OES suffixes from the
          FBO code.Change the namespace from
           to .Change the two #includes to point to the
          ES2 folder rather than the ES1 folder.Add in the 
          and  methods (see ). You must
          change all instances of  to
           because we are using namespaces
          to distinguish between the 1.1 and 2.0 renderers.Add declarations for  and
           to the class declaration as shown in
          .Add the  for
           as shown in .Now that the busy work is out of the way,
      let's add declarations for the uniform handles and attribute handles
      that are used to communicate with the vertex shader. Since the vertex
      shader is now much more complex than the simple pass-through program
      we've been using, let's group the handles into simple sub-structures, as
      shown in . Add this code
      to , within the namespace
      declaration, not above it. (The bold part of the listing shows the two
      lines you must add to the class declaration's
       section.)Next we need to change the Initialize method
      so that it compiles the shaders, extracts the handles to all the
      uniforms and attributes, and sets up some default material colors.
      Replace everything from the comment  to the end of the method with the contents of .Next let's replace the
       method, shown in .That's it for the ES 2.0 backend! Turn off
      the  switch in 
      and you should see something very similar to the ES 1.1 screenshot shown
      in .When a model has coarse tessellation,
      performing the lighting calculations at the vertex level can result in
      the loss of specular highlights and other detail, as seen in .One technique to counteract this unattractive
      effect is ; this is when most
      (or all) of the lighting algorithm takes place in the fragment
      shader.Shifting work from the vertex shader to the
        pixel shader can often be detrimental to performance. I encourage you
        to experiment with performance before you commit to a specific
        technique.The vertex shader becomes vastly simplified,
      as shown in . It simply passes the
      diffuse color and eye-space normal to the fragment shader.The fragment shader now performs the burden
      of the lighting math, as shown in .
      The main distinction it has from its per-vertex counterpart () is the presence of precision
      specifiers throughout. We're using  for colors,
       for the varying normal, and
       for the internal math.To try these out, you can replace the contents of your existing
         and  files. Just
        be sure not to delete the first line with STRINGIFY or the last line
        with the closing parenthesis and semicolon.Shifting work from the vertex shader to the
      fragment shader was simple enough, but watch out: we're dealing with the
      normal vector in a sloppy way. OpenGL performs linear interpolation on
      each component of each varying. This causes inaccurate results, as you
      might recall from the coverage of quaternions in . Pragmatically speaking, simply re-normalizing the
      incoming vector is often good enough. We'll cover a more rigorous way of
      dealing with normals when we present bump mapping in .Mimicking the built-in lighting functionality
      in ES 1.1 gave us a fairly painless segue to the world of GLSL. We could
      continue mimicking more and more ES 1.1 features, but that would get
      tiresome. After all, we're upgrading to ES 2.0 to enable
       effects, right? Let's leverage shaders to
      create a simple effect that would otherwise be difficult (if not
      impossible) to achieve with ES 1.1.
      (sometimes ) achieves a cartoony
      effect by limiting gradients to two or three distinct colors, as shown
      in .Assuming you're already using per-pixel
      lighting, achieving this is actually incredibly simple; just add the
      boldface lines in .The toon shading example belongs to a class of
    effects called  effects, often
    known as  effects. Having dangled the carrot of
    shaders in front of you, I'd now like to show that ES 1.1 can also render
    some cool effects.For example, you might want to produce a
    intentionally faceted look to better illustrate the geometry; these is
    useful in applications like CAD visualization or technical illustration.
     shows off a two-pass technique
    whereby the model is first rendered with triangles, then with lines. The
    result is less messy than the Wireframe Viewer app because hidden lines
    have been eliminated.An issue with this two-pass technique is
    Z-fighting (see ). An obvious workaround
    is translating the first pass backwards ever so slightly, or translating
    the second pass forwards. Unfortunately, that approach causes issues due
    to the non-linearity of depth precision; some portions of your model would
    look fine, but other parts may have lines on the opposite side that poke
    through.Turns out that both versions of OpenGL ES offer
    a solution to this specific issue, and it's called . Polygon offset tweaks the Z value of each pixel
    according to the depth slope of the triangle that it's in. You can enable
    and set it up like so: scales the depth
    slope, and  gets added to the result. When polygon
    offset is enabled, the Z values in each triangle get tweaked as
    follows:The code to implement this effect in Model
    Viewer can be found in the downloadable examples (see ). Note that your
     class will need to store two VBO
    handles for index buffers: one for the line indices, the other for the
    triangle indices. In practice, finding the right values for
     and  almost
    always requires experimentation.Due to a hardware limitation, first and
      second generation iPhones ignore the  argument
      in .One rather obscure feature found only in
      OpenGL ES 1.1 is , whereby OpenGL
      forces the color of each triangle to be uniform. OpenGL performs flat
      shading by choosing a specific vertex in each triangle, then spreading
      its associated color throughout the triangle. You can turn it on and off
      as follows:I suspect this feature is a bit of a
      left-over from olden times, when flat shading was a desirable option to
      increase performance. Nowadays it's probably only useful for
      non-photorealistic rendering. Another way to achieve something similar
      is simply duplicating the same normal vector at each of the three
      corners in each triangle; this is one approach you could take for ES
      2.0.So far we've dealing exclusively with a gallery
    of parametric surfaces. They make a great teaching tool, but parametric
    surfaces probably aren't what you'll be rendering in your app. More
    likely, you'll have 3D assets coming from artists who use modelling
    software such as Maya or Blender.The first thing to decide on is the file format we'll use for
    storing geometry. The COLLADA format was devised to solve the problem of
    interchange between various 3D packages, but COLLADA is quite complex;
    it's capable of conveying much more than just geometry, including effects,
    physics and animation.A more suitable format for our modest purposes
    is the simple OBJ format, first developed by Wavefront Technologies in the
    1980's and still in use today. We won't go into its full specification
    here (there are plenty of relevant sources on the web), but we'll cover
    how to load a conformant file that uses a subset of OBJ features.Even though the OBJ format is simple and
      portable, I don't recommend using it in a production game or
      application. The parsing overhead can be avoided by inventing your own
      raw binary format, slurping up the entire file in a single I/O call,
      then directly uploading its contents into a vertex buffer. This type of
      blitz loading can greatly improve the start-up time of your iPhone
      app.Another popular geometry file format for the
      iPhone is PowerVR's POD format. The PowerVR Insider SDK (discussed in
      ) includes tools and code samples for
      generating and reading POD files.Without further ado,  shows an example OBJ file.Lines that start with a 
    specify a vertex position using three floats separated by spaces. Lines
    that start with  specify a "face" with a list of
    indices into the vertex list. If the OBJ consists of triangles only, then
    every face has exactly three indices, which makes it a breeze to render
    with OpenGL. Watch out though: in OBJ files, indices are one-based, not
    zero-based as they are in OpenGL.OBJ also supports vertex normals with lines
    that start with . For a face to refer to a vertex
    normal, it references it using an index that's separate from the vertex
    index, as shown in . The slashes are doubled
    because the format is actually ; this example
    doesn't use texture coordinates () so it's
    blank.One thing that's a bit awkward about this (from
    an OpenGL standpoint) is that each face specifies separate position
    indices and normal indices. In OpenGL ES, you only specify a single list
    of indices; each index simultaneously refers to both a normal and a
    position.Because of this complication, the normals found
    in OBJ files are often ignored in many tools. It's fairly easy to compute
    the normals yourself analytically, which we'll demonstrate soon.3D artist Christopher Desse has graciously
    donated some models to the public domain, two of which we'll be using in
    Model Viewer: a character named "MicroNapalm" (the selected model in ) and a ninja character (far left in the Tab
    Bar). This greatly enhances the cool factor when you want to show off to
    your four year-old; why have cones and spheres when you can have
    ninjas?I should also mention that I processed
      Christopher's OBJ files so that they only contain 
      lines and  lines with three indices each, and that I
      scaled the models to fit inside a unit cube.Note that we'll be loading resources from
      external files for the first time. Adding file resources to a project is
      easy in Xcode. Download the two files
      ( and
      ) from the examples site and put them on
      your desktop or Downloads folder.Create a new folder called "Models" by right-clicking the
      ModelViewer root in the Overview pane and choose . Right-click the new folder and choose
      . Select the two OBJ files
      (available from the book's web site) by holding the command key, then
      click . In the next dialog box, check the box
      labeled "Copy items...", and accept the defaults, then click Add.
      Done!The iPhone differs from other platforms in
      how it handles bundled resources, so it makes sense to create a new
      interface to shield this from the application engine. Let's call it
      , shown in . For now it has a single method that
      simply returns the absolute path to the folder that has resource files.
      This may seem too simple to merit its own interface at the moment, but
      we'll extend it in future chapters to handle more complex tasks, such as
      loading in image files. Add these lines and make the change shown in
      bold to .We added a new argument to
       to allow the
      platform-specific layer to pass in its implementation class. In our case
      the implementation class needs to be a mixture of C++ and Objective C.
      Add a new C++ file to your Xcode project called
       (don't create the corresponding
       file), shown in .Retrieve the global
           object and call its
           method, which returns something like
          this when running on a simulator:When running on a physical device, it
          returns something like this:Convert the Objective C string object
          into a C++ STL string object using the 
          method.The resource manager should be instanced
      within the  class and passed to the application
      engine.  has a field called
       which gets instanced somewhere in
      , and gets passed to
      . (This is similar to how
      we're already handling the rendering engine.) So you'll need to do the
      following:In , add the line
           to the
           section.In , add the line
           to
           (you can add it just above the line
          . Next, add
           as the second argument to
          .Next we need to make a few small changes to
      the application engine per .
      The lines in bold show how we're re-using the
       interface to avoid changing any code in the
      rendering engine. Modified/new lines in
       are shown in bold (make sure
      you replace the existing assignments to 
      and  in
      ):The next step is creating the
       class, which implements all the
       methods, and is responsible for parsing the
      OBJ file. This class will be more than just a dumb loader; recall that
      we want to compute surface normals analytically. Doing so allows us to
      reduce the size of the app, but at the cost of a slightly longer startup
      time.We'll compute the vertex normals by first
      finding the facet normal of every face, then averaging together the
      normals from adjoining faces. The C++ implementation of this algorithm
      is fairly rote, and you can get it from the book's companion website
      (see ); for brevity's sake,
      pseudocode is shown in .The mechanics of loading face indices and
      vertex positions from the OBJ file is somewhat tedious, so you should
      download  and
       from the book's website (see ) and add them to your Xcode project. The
       constructor is shown in , which loads in the vertex indices using the
      fstream facility in C++. Note that I subtracted one from all vertex
      indices; watch out for the one-based pitfall!We covered quite a bit of territory in this
    chapter: we took a deep dive into GLSL, examined some algorithms for
    lighting and simple effects, and finally managed to replace some tiresome
    mathematical shapes with artistic 3D content. Another type of content that
    artists can offer is actually 2D, and yet it's a vital ingredient to
    almost all real-time 3D rendering.  are
    images that get "wrapped" around the objects in your scene. They sound
    simple enough, but trust me, they're thorny enough to deserve their own
    chapter or two. And so we come to the next stop on our graphics
    trek!Not everybody trusts paintings, but people
    believe photographs.Shading algorithms are required for effects that
  need to respond to a changing condition in real time, such as the movement
  of a light source. But procedural methods can go only so far on their own;
  they can never replace the creativity of a professional artist. That's where
   come to the rescue. Texturing allows any
  predefined image, such as a photograph, to be projected onto a 3D
  surface.Simply put, textures are images; yet somehow, an
  entire vocabulary is built around them. Pixels that make up a texture are
  known as . When the hardware reads a texel
  color, it's said to be . When OpenGL scales a
  texture, it's also  it. Don't let the
  vocabulary intimidate you; the concepts are simple. In this chapter, we'll
  focus on presenting the basics, saving some advanced techniques for later in
  the book.We'll begin by modifying Model Viewer to support
  image loading and simple texturing. Afterwards we'll take a closer look at
  some of the OpenGL features involved, such as mipmapping and filtering.
  Towards the end of the chapter we'll use texturing to perform a fun trick
  with the iPhone camera.Our final enhancement to Model Viewer wraps a
    simple grid texture around each of the surfaces in the parametric gallery,
    as seen in . We only need to
    store one cell of the grid in an image file; OpenGL can repeat the source
    pattern as many times as desired.The image file can be in a number of different
    file formats, but we'll go with PNG for now. It's popular because it
    supports an optional alpha channel, lossless compression, and variable
    color precision. (Another common format on the iPhone platform is PVR,
    which we'll cover later in the chapter.)You can either download the image file for the
    grid cell from the book's companion site, or create one using your
    favorite paint or graphics program. I used a tiny 16x16 white square with
    a 1-pixel black border. Save the file as
    .Since we'll be loading a resource file from an
    external file, let's use the OBJ-loading sample from the last chapter as
    the starting point. To keep things simple, the first step is rewinding a
    bit and using parametric surfaces exclusively. Simply revert the
     method so that it uses
    the sphere and cone shapes. To do this, find the following code:And replace it with:Keep everything else the same. We'll enhance the
     interface later to support image
    loading.Next you need to add the actual image file to
    your Xcode project. As with the OBJ files in the previous chapter, Xcode
    automatically deploys these resources to your iPhone. Even though this
    example has only a single image file, I recommend creating a dedicated
    group anyway. Right-click the ModelViewer root in the Overview pane,
    choose , and call it "Textures".
    Right-click the new group, and choose Get Info. To the right of the "Path"
    label on the General tab, click Choose and create a New Folder called
    Textures. Click Choose and close the group info window.Right-click the new group and choose . Select the PNG file, click , and
    in the next dialog box, make sure the "Copy items" checkbox is checked and
    click Add.Unlike OBJ files, it's a bit non-trivial to
      decode the PNG file format by hand since it uses a lossless compression
      algorithm. Rather than manually decoding the PNG file in the application
      code, it makes more sense to leverage the infrastructure that Apple
      provides for reading image files. Recall that the
       implementation is a mix of Objective
      C and C++; so, it's an ideal place for calling Apple-specific APIs.
      Let's make the resource manager responsible for decoding the PNG file.
      The first step is to open  and make the
      changes shown in . New lines
      are in bold (the changes to the last two lines, which you'll find at the
      end of the file, is needed to support a change we'll be making to the
      rendering engine).Load the given PNG file into the resource
          manager.Return a pointer to the decoded color
          buffer. For now, this is always 8-bit-per-component RGBA
          data.Return the width and height of the loaded
          image using the  class from our vector
          library.Free memory used to store the decoded
          data.Now let's open
       and update the actual
      implementation class (don't delete the s,
       statement, or the
       definition). It needs two
      additional items to maintain state: the decoded memory buffer
      (), and the size of the image
      (). Apple provides several ways of loading
      PNG files;  is the simplest way of
      doing this.Most of 
      is straightforward, but  deserves some
      extra explanation:Convert the C++ string into an Objective
          C string object.Obtain the fully qualified path the PNG
          file.Create an instance of a
           object using the
           method, which slurps up
          the contents of the entire file.Extract the inner
           object from .
           is essentially a convenience wrapper for
          . Note that  is
          a Core Graphics class, which is shared across the Mac OS X and
          iPhone OS platforms;  comes from
           and is therefore iPhone-specific.Extract the image size from the inner
           object.Generate a 
          object from the  object.
           is a core foundation object that
          represents a swath of memory.The image loader in  is simple, but not robust. For
        production code, I recommend using one of the enhanced methods
        presented later in the chapter.Next we need to make sure that the resource
      manager can be accessed from the right places. It's already instanced in
      the  class and passed to the application
      engine; now we need to pass it to the rendering engine as well. The
      OpenGL code needs it to retrieve the raw image data.Go ahead and change
       so that the resource manager gets passed to
      the rendering engine during construction. The relevant section of code
      is shown in  (additions are shown in
      bold).To control how the texture gets applied to
      the models, we need to add a 2D texture coordinate attribute to the
      vertices. The natural place to generate texture coordinates is in the
       class. Each subclass should specify
      how many repetitions of the texture get tiled across each axis of
      domain. Consider the torus: its outer circumference is much longer than
      the circumference of its cross-section. Since the x-axis in the domain
      follows the outer circumference, and the y-axis circumscribes the
      cross-section, it follows that more copies of the texture need to be
      tiled across the x-axis than the y-axis. This prevents the grid pattern
      from being stretched along one axis.Recall that each parametric surface describes
      its domain using the  structure, so
      that's a natural place to store the number of repetitions; see . Note that the repetition counts
      for each axis are stored in a .The texture counts that I chose for the cone
      and sphere are shown in bold in . For
      brevity's sake, I've omitted the other parametric surfaces; as always,
      the code can be downloaded from the book's website (see ).Next we need to flesh out a couple methods in
       (). Recall that we're passing in a set
      of flags to to request a set of
      vertex attributes; till now, we've been ignoring the
       flag.In OpenGL, texture coordinates are normalized
      such that (0,0) maps to one corner of the image and (1, 1) maps to the
      other corner, regardless of its size. The inner loop in  computes the texture coordinates like
      this:Since the  coordinate
      ranges from zero up to  (inclusive),
      OpenGL horizontally tiles 
      repetitions of the texture across the surface. We'll take a deeper look
      at how texture coordinates work later in the chapter.Note that if you were loading the model data
      from an OBJ file or other 3D format, you'd probably obtain the texture
      coordinates directly from the model file rather than computing them like
      we're doing here.As always, let's start with the ES 1.1
      rendering engine since the 2.0 variant is more complex. The first step
      is adding a pointer to the resource manager as seen in . Note we're also adding a
       for the grid texture. Much like framebuffer
      objects and vertex buffer objects, OpenGL textures have integer
      names.The code for loading the texture is shown in
      , followed by a detailed
      explanation.Generate the integer identifier for the
          object, then bind it to the pipeline. This follows the Gen/Bind
          pattern used by FBOs and VBOs.Set the  and  of
          the texture object. The texture filter is the algorithm that OpenGL
          uses to shrink or enlarge a texture; we'll cover filtering in detail
          later.Tell the resource manager to load and
          decode the  file.Upload the raw texture data to OpenGL
          using ; more on this later.Tell OpenGL to enable the texture
          coordinate vertex attribute.Tell OpenGL to enable texturing.
      introduces the  function, which
      unfortunately has more parameters than it needs due to historical
      reasons. Don't be intimidated by the eight parameters; it's much easier
      to use than it appears. Here's the formal declaration:Specifies which binding point to upload
            the texture to. For ES 1.1, this must be
            .Specifies the mipmap level. We'll learn
            more about mipmaps soon. For now, use zero for this.Specifies the format of the texture.
            We're using  for now, and other formats
            will be covered shortly. It's declared as a
             rather than a 
            for historical reasons.The size of the image being
            uploaded.Set this zero; texture borders are not
            supported in OpenGL ES. Be happy, that's one less thing you have
            to remember!In OpenGL ES, this has to match
            . The argument may seem
            redundant, but it's yet another carry-over from desktop OpenGL,
            which supports format conversion. Again, be happy; this is a
            simpler API.Describes the type of each color
            component. This is commonly ,
            but we'll learn about some other types later.Pointer to the raw data that gets
            uploaded.Next let's go over the
       method. The only difference is that the vertex
      stride is larger, and we need to call
       to give OpenGL the correct offset
      into the VBO. See .That's it for the ES 1.1 backend!
      Incidentally, in more complex applications you should take care to
      delete your textures after you're done with them; textures can be one of
      the biggest resource hogs in OpenGL. Deleting a texture is done like
      so:This function is similar to
       in that it takes a count and a list of
      names. Incidentally, vertex buffer objects are deleted in a similar
      manner using .The ES 2.0 backend requires some changes to
      both the vertex shader (to pass along the texture coordinate) and the
      fragment shader (to apply the texel color). You do not call
       with ES 2.0; it simply
      depends on what your fragment shader does.Let's start with the vertex shader, shown in
      . This is a modification of the
      simple lighting shader presented in the previous chapter (). Only three new lines are required
      (shown in bold).If you tried out some of the other vertex shaders in that
        chapter, such as the pixel shader or toon shader, the current shader
        in your project may look different than .To try these out, you can replace the contents of your existing
         and  files. Just
        be sure not to delete the first line with STRINGIFY or the last line
        with the closing parenthesis and semicolon.
      simply passes the texture coordinates through, but many interesting
      effects can be achieved by manipulating the texture coordinates, or even
      generating them from scratch. For example, to achieve a "movie
      projector" effect, simply replace the last line in  with this:For now, let's stick with the boring
      pass-through shader as it better emulates the behavior of ES 1.1. The
      new fragment shader is a bit more interesting; see .As before, declare a low-precision
          varying to receive the color produced by lighting.Declare a medium-precision varying to
          receive the texture coordinates.Declare a uniform
          , which represents the texture stage
          from which we'll retrieve the texel color.Use the 
          function to look up a texel color from the sampler, then multiply it
          by the lighting color to produce the final color. This is
          component-wise multiplication, not a dot product or a cross
          product.When setting a uniform sampler from within
      your application, a common mistake is to set it to the handle of the
      texture object you'd like to sample:The correct value of the sampler is the stage
      index that you'd like to sample from, not the handle. Since all uniforms
      default to zero, it's fine to not bother setting sampler values if
      you're not using multi-texturing (we'll cover multi-texturing later in
      the book).Uniform samplers should be set to the stage
        index, not the texture name.Newly introduced in  is the 
      function call. For input, it takes a uniform sampler and a
       texture coordinate. Its return value is always a
      , regardless of the texture format.The OpenGL ES specification stipulates that
         can be called from vertex shaders as
        well, but on many platforms, including the iPhone, it's actually
        limited to fragment shaders only.Note that  uses multiplication to combine the
      lighting color and texture color; this often produces good results.
      Multiplying two colors in this way is called
      , and it's the default method used in
      ES 1.1.Now let's make the necessary changes to the
      C++ code. First we need to add new class members to store the texture id
      and resource manager pointer, but that's the same as ES 1.1, so I won't
      repeat it here. I also won't repeat the texture-loading code as it's the
      same with both APIs.One new thing we need for the ES 2.0 backend
      is an attribute id for texture coordinates. See . Note the lack of a
       for texturing; remember, there's no need for
      it in ES 2.0.You may have noticed that the fragment shader
      declared a sampler uniform, but we're not setting it to anything in our
      C++ code. There's actually no need to set it; all uniforms default to
      zero, which is what we want for the sampler's value anyway. You don't
      need a non-zero sampler unless you're using multitexturing, which is a
      feature that we'll cover in chapter 8.Next up is the 
      method, which is pretty straightforward (). The only way it differs from its ES
      1.1 counterpart is that it makes three calls to
       rather than
      , ,
      and . (Replace everything from
       to the end of the method with the
      corresponding code below.)You must also make the same changes to the ES 2.0 renderer that
        were shown earlier in .That's it, you now have a textured model
      viewer! Before you build and run it, select Build→Clean All Targets
      (we've made a lot of changes to various parts of this app, and this will
      help avoid any surprises by building the app from a clean slate). We'll
      explain some of the details in the sections to come.Recall that texture coordinates are defined
    such that (0,0) is the lower-left corner and (1,1) is the upper-right
    corner. So what happens when a vertex has texture coordinates that lie
    outside this range? The sample code is actually already sending
    coordinates outside this range. For example, the
     parameter for the sphere is
    (20,35).By default, OpenGL simply repeats the texture
    at every integer boundary; it lops off the integer portion of the texture
    coordinate before sampling the texel color. If you want to make this
    behavior explicit, you can add something like this to the rendering engine
    code:The wrap mode passed in to the third argument
    of  can be one of the following:The default wrap mode; discard the
          integer portion of the texture coordinate.Select the texel that lies at the nearest
          boundary.Mirrored wrapping is not included in core OpenGL ES 1.1, but
            the iPhone (all generations) supports it via the
             extension;
            simply append the _OES suffix to the constant.If the integer portion is an even number,
          this acts exactly like . If it's an odd
          number, the fractional portion is inverted before it's
          applied.The three wrap modes are depicted in . From left to right: repeat, clamp-to-edge, and
    mirrored. The figure on the far right uses 
    for the S coordinate and  for the T
    coordinate.What would you do if you wanted to animate your
    texture coordinates? For example, say you want to gradually move the
    grasshoppers in  so that they scroll around
    the Möbius strip. You certainly wouldn't want to upload a new VBO at every
    frame with updated texture coordinates; that would detrimental to
    performance. Instead you would set up a . Texture matrices were briefly mentioned in , and they're configured
    much the same way as the model-view and projection matrices. The only
    difference is that they're applied to texture coordinates rather than
    vertex positions. The following snippet shows how you'd set up a texture
    matrix with ES 1.1 (with ES 2.0, there's no built-in texture matrix, but
    it's easy to create one with a uniform variable).In this book, we use the convention that (0,0)
    maps to the upper-left of the source image (), and that the Y coordinate increases
    downwards.Our texture coordinate convention is not what
      you'd find in the OpenGL specification from Khronos, which proclaims
      (0,0) to be the lower-left corner. However the spec also says that, when
      uploading an image with , it should be
      arranged in memory such that the first row of data corresponds to the
      bottom row in the image. In all of the methods we're using to decode the
      image, the top row of data comes first.By the way, saying that (1,1) maps to a texel
    at the far corner of the texture image isn't a very accurate statement;
    the corner texel's center is actually a fraction, as seen in . This may seem pedantic, but you'll see how
    it's relevant when we cover filtering, which comes next.Is a texture a collection of discrete texels,
    or is it a continuous function across [0, 1]? This is a dangerous question
    to ask a graphics geek; it's a bit like asking a physicist if a photon is
    a wave or a particle.When you upload a texture to OpenGL using
    , it's a collection of discrete texels.
    When you sample a texture using normalized texture coordinates, it's a bit
    more like a continuous function. You might recall these two lines from the
    rendering engine:What's going on here? The first line sets the
    ; the second line sets the
    . Both of these tell OpenGL how
    to map those discrete texels into a continuous function.More precisely, the minification filter
    specifies the scaling algorithm to use when the texture size in screen
    space is smaller than the original image; the magnification filter tells
    OpenGL what to do when the texture size is screen space is larger than the
    original image.The magnification filter can be one of two
    values:Simple and crude; use the color of the
          texel nearest to the texture coordinate.Indicates bilinear filtering. Samples the
          local 2x2 square of texels, and blends them together using a
          weighted average. The image on the far right in  is an example of bilinear
          magnification applied to a simple 8x8 monochrome texture.The minification filter supports the same
    filters as magnification, and adds four additional filters that rely on
    , which are "pre-shrunk" images that you
    need to upload separately from the main image. More on mipmaps
    soon.The available minification modes are:As with magnification, use the color of
          nearest texel.As with magnification, blend together the
          nearest four texels. The middle image in  is an example of bilinear
          minification.Find the mipmap that best matches the
          screen-space size of the texture, then use
           filtering.Find the mipmap that best matches the
          screen-space size of the texture, then use
           filtering.Perform 
          sampling on each of  "best fit" mipmaps,
          then blend the result. OpenGL takes eight samples for this, so it's
          the highest-quality filter. This is also known as
           filtering.Take the weighted average of two samples,
          where one sample is from mipmap A, the other from mipmap B.A comparison of various filtering schemes is
    shown in .Deciding on a filter is a bit of a black art;
    personally I often start with trilinear filtering
    (), and I try cranking down to a
    lower-quality filter only when I'm optimizing my frame rate. Note that
     is perfectly acceptable in some scenarios;
    for example, when rendering 2D quads that have the same size as the source
    texture.First and second generation devices have some
    restrictions on the filters:If magnification is
        , then minification must be one of:
        ,
        , or
        .If magnification is
        , then minification must be one of:
        ,
        , or
        .This isn't a big deal since you'll almost never
    want a different same-level filter for magnification and minification.
    Nevertheless it's important to note that the iPhone simulator and newer
    devices do not have these restrictions.Mipmaps help with both quality and
      performance. They can help with performance especially when large
      textures are viewed from far away. Since the graphics hardware performs
      sampling on an image potentially much smaller than the original, it's
      more likely to have the texels available in a nearby memory cache.
      Mipmaps can improve quality for several reasons; most importantly, they
      effectively cast a wider net, so the final color is less likely to be
      missing contributions from important nearby texels.In OpenGL, mipmap zero is the original image,
      and every following level is half the size of the preceding level. If a
      level has an odd size, then the floor function is used, as in .Watch out though, sometimes you need to
      ensure that all mipmap levels have an even size. In another words, the
      original texture must have dimensions that are powers of two. We'll
      discuss this further later in the chapter.  depicts a popular way of neatly visualizing
      mipmaps levels into an area that's 1.5 times the original width.To upload the mipmaps levels to OpenGL, you
      need to make a series of separate calls to
      , from the original size all the way down
      to the 1x1 mipmap:Usually code like this occurs in a loop. Many
      OpenGL developers like to use a right-shift as a sneaky way of halving
      the size at each iteration. I doubt it really buys you anything, but
      it's great fun:If you'd like to avoid the tedium of creating
      mipmaps and loading them in individually, OpenGL ES can generate mipmaps
      on your behalf:In ES 1.1, mipmap generation is part of the
      OpenGL state associated with the current texture object, and you should
      enable it  uploading level zero. In ES 2.0,
      mipmap generation is an action that you take 
      you upload level zero.You might be wondering why you'd ever want to
      provide mipmaps explicitly when you can just have OpenGL generate them
      for you. There are actually a couple reasons for this:There's a performance hit for mipmap
          generation at upload time. This could prolong your application's
          startup time, which is something all good iPhone developers obsess
          about.When OpenGL performs mipmap generation
          for you, you're (almost) at the mercy of whatever filtering
          algorithm it chooses. You can often produce higher-quality results
          if you provide mipmaps yourself, especially if you have a very
          high-resolution source image, or a vector-based source.Later we'll learn about a couple free tools
      that make it easy to supply OpenGL with ready-made, pre-shrunk
      mipmaps.By the way, you do have some control over the
      mipmap generation scheme that OpenGL uses. The following lines are valid
      with both ES 1.1 and 2.0:If you're a control freak and you'd like to
        tweak the way OpenGL chooses mipmap levels to sample from, you'll be
        glad to hear the iPhone supports an extension that can shift which
        mipmap level(s) get sampled. This is useful for intentional blurring
        or pseudo-sharpening. For more information, head over to the extension
        registry on the Khronos site:This is an ES 1.1 extension only; it's not necessary for ES 2.0
        because you can bias the mipmap level from within the fragment shader
        using an optional third argument to . The
        full function signature looks like this:Incidentally, don't confuse
        , which is a shader function for sampling,
        and , which a C function for
        uploading.It's easy to enable mipmapping in the Model Viewer sample. For the
      ES 1.1 rendering engine, enable mipmap generation after binding to the
      texture object, then replace the minification filter:For the ES 2.0 rendering engine, replace the minification filter
      in the same way, but call  after
      uploading the texture data:Recall that two of the parameters to
     stipulate format, and one stipulates type,
    highlighted in bold here:The allowed formats are:Three-component color.Four-component color; includes
          alpha.Same as  but
          with the blue and red components swapped. This is a non-standard
          format, but available on the iPhone due to the
           extension.Single component format used as an alpha
          mask. (We'll learn a lot more about alpha in the next
          chapter.)Single component format used as
          grayscale.Two component format: grayscale + alpha.
          Very useful for storing text.Don't dismiss the non-RGB formats; if you don't
    need color, you can save significant memory with the one or two-component
    formats.The  parameter in
     can be one of these:Each color component is 8 bits
          wide.Each pixel is 16 bits wide; red and blue
          have five bits each, green has six. Requires the format to be
          . The fact that green gets the extra bit
          isn't random — the human eye is more sensitive to variation in green
          hues.Each pixel is 16 bits wide, and each
          component is 4 bits. Can only be used with
          .Dedicates only one bit to alpha; a pixel
          can only be fully opaque or fully transparent. Each pixel is 16 bits
          wide. Requires format to be .It's also interesting to note the various
    formats supported by the PNG file format, even though this has nothing to
    do with OpenGL:Five grayscale formats: each pixel can be
        1, 2, 4, 8, or 16 bits wide.Two RGB formats: each color component can
        be 8 or 16 bits.Two "gray with alpha" formats: each
        component can be 8 or 16 bits.Two RGBA formats: each component can be 8
        or 16 bits.Paletted formats — we'll ignore
        these.Just because a PNG file grayscale doesn't mean that it's using a grayscale-only
      format! The iPhone SDK includes a command line tool called
       that can help with this. (Skip ahead to
       to see where it's located.) You
      can also right-click an image file in Mac OS X and use the "Get Info"
      option to learn about the internal format.Recall that the
       method presented at the beginning of the
      chapter (page ) did not return any
      format information, and that the rendering engine assumed the image data
      to be in RGBA format. Let's try to make this a bit more robust.We can start by enhancing the
       interface so that it returns some
      format information in an API-agnostic way. (Remember, we're avoiding all
      platform-specific code in our interfaces.) For simplicity's sake, let's
      support only the subset of formats that are supported by both OpenGL and
      PNG. Open  and make the changes shown
      in . New and modified lines
      are shown in boldface. Note that the 
      method has been removed because size is part of
      .The implementation to the new
       method is shown in . Note the Core Graphics functions
      used to extract format and type information, such as
      ,
      , and
      . I won't go into detail about
      these functions as they are fairly straightforward; for more
      information, look them up on Apple's iPhone Developer site.Next, we need to modify the rendering engines
      so that they pass in the correct arguments to
       after examining the API agnostic texture
      description.  shows a
      private method that can be added to both rendering engines; it works
      under both ES 1.1 and 2.0, so add it to both renderers (you will also
      need to add its signature to the  section of
      the class declaration).Now you can remove the following snippet in the
       method (both rendering engines, but leave
      the call to  in the
      2.0 renderer):Replace it with a call to the new private method:At this point you should be able to build and run and get the same
      results as before.Textures are often the biggest memory hog in
    graphically intense applications. 
    is a technique that's quite popular in real-time graphics, even on desktop
    platforms. Like JPEG compression, it can cause a loss of image quality,
    but unlike JPEG, its compression ratio is constant and deterministic. If
    you know the width and height of your original image, then it's simple to
    compute the number of bytes in the compressed image.Block compression is particularly good at
    photographs, and in some cases it's difficult to notice the quality loss.
    The noise is much more noticeable when applied to images with regions of
    solid color, like vector-based graphics and text.I strongly encourage you to use block
    compression when it doesn't make a noticeable difference in image quality.
    Not only does it reduce your memory footprint, it can boost performance as
    well, due to increased cache coherency. The iPhone supports a specific
    type of block compression called PVRTC, named after the PowerVR chip that
    serves as the iPhone's graphics processor. PVRTC has four variants, as
    seen in .Compared to a format with 8 bits per component.Be aware of some important restrictions with
      PVRTC textures: the image must be square, and its width/height must be a
      power of two.The iPhone SDK comes with a command line
    program called  that you can use to generate
    PVRTC data from an uncompressed image, and it's located here:It's possible Apple has modified the path since
    the time of this writing, so I recommend verifying the location of
     using the spotlight feature in Mac OS X. By
    the way, there actually several command line tools at this location
    (including a rather cool one called ). They're
    worth a closer look!Here's how you could use
     to convert 
    into a compressed image called :Some of the parameters are explained
    below.Generate mipmaps.Use PVRTC compression. This can be
          tweaked with additional parameters, explained below.This may seem redundant, but it chooses
          the file format rather than the encoding. The 
          format includes a simple header before the image data that contains
          size and format information. I'll explain how to parse the header
          later.This is an optional PNG file that gets
          generated to allow you preview the quality loss caused by
          compression.The name of the resulting PVR
          file.The encoding argument can be tweaked with
    optional arguments. Some examples:Specifies a 2 bits-per-pixel
          encoding.Specifies a 4 bits-per-pixel encoding.
          This is the default, so there's not much reason to include it on the
          command line.Use perceptual compression and a 2 bpp
          format. Perceptual compression doesn't change the format of the
          image data; rather, it tweaks the compression algorithm such that
          the green channel preserves more quality than the red and blue
          channels. Humans are more sensitive to variations in green.Apply compression equally to all color
          components. This defaults to "on", so there's no need to specify it
          explicitly.At the time of this writing,
       does not include an argument to control
      whether or not the resulting image has an alpha channel. It
      automatically determines this based on the source format.Rather than executing
     from the command line, you can make it an
    automatic step in Xcode's build process. Go ahead and perform the
    following steps:Right-click the Targets group, then choose
        .Lots of stuff in next dialog:Leave the shell as
            .Enter this directly into the script
            box:Add this to Input Files:Add this to Output Files:These fields are important to set
            because they make Xcode smart about rebuilding; i.e., it should
            run the script only when the input file has been modified.Close the dialog by clicking the X in
            the upper-left corner.Open the Targets group and its child node.
        Drag the "Run Script" item so that it appears before the "Copy Bundle
        Resources" item. You can also rename it if you'd like; simply
        right-click it and choose .Build your project once to run the script.
        Verify that the resulting PVRTC file exists. Don't try running
        yet.Add  to your
        project (right-click the Textures group, select Add→Existing Files and
        choose ). Since it's a build artifact,
        I don't recommend checking it into your source code control system.
        Xcode gracefully handles missing files by highlighting them in
        red.Make sure that Xcode doesn't needlessly
        re-run the script when the source file hasn't been modified. If it
        does, then there could be a typo in script dialog. (Simply
        double-click the "Run Script" phase to re-open the script
        dialog.)Before moving on to the implementation, we
    need to incorporate a couple source files from Imagination Technology's
    PowerVR SDK.Go to
        .Click the link for "Khronos OpenGL ES 2.0
        SDKs for PowerVR SGX family".Select the download link under Mac OS /
        iPhone 3GS.In your Xcode project, create a new group
        called . Right-click the new group, and
        choose Get Info. To the right of the "Path" label on the General tab,
        click Choose and create a New Folder called PowerVR. Click Choose and
        close the group info window.After opening up the tarball, look for
         and
         in the 
        folder. Drag these files to the PowerVR group, and check the "Copy
        items" checkbox in the dialog that appears, then click Add.Enough Xcode shenanigans, let's get back to
    writing real code. Before adding PVR support to the
     class, we need to make some
    enhancements to . These changes are
    highlighted in bold in .The implementation of
     is shown in  (you'll replace everything within
    the  definition except the
     and 
    methods). It parses the header fields by simply casting the data pointer
    to a pointer-to-struct. The size of the struct isn't necessarily the size
    of the header, so the  method looks at the
     field to determine where the raw data
    starts.Note that we changed the type of
     from  to
    . Since we create the 
    object using auto-release semantics, there's no need to call a release
    function in the  method. and
       are said to be "toll-free bridged", meaning
      they are interchangeable in function calls. You can think of
       as being the vanilla C version, and
       as the Objective C version. I prefer using
       (in my Objective C code) because it can work
      like a C++ smart pointer.Because of this change, we'll also need to make one change to
    . Find this line:and replace it with:You should now be able to build and run, although your application
    is still using the PNG file. adds a
    new method to the rendering engine for creating a compressed texture
    object. This code will work under both ES 1.1 and ES 2.0.You can now replace this:With this:Since the PVR file contains multiple mipmap levels, you'll also need
    to remove any code you added for mipmap auto-generation
    ( under ES 2.0,
     with
     under ES 1.1).After re-building your project, your app will now be using the
    compressed texture.Of particular interest in  is the section that loops over each mipmap
    level. Rather than calling , it uses
     to upload the data. Here's its
    formal declaration:Specifies which binding point to upload
          the texture to. For ES 1.1, this must be
          .Specifies the mipmap level.Specifies the compression
          encoding.The dimensions of the image being
          uploaded.Set this zero; again, texture borders are
          not supported in OpenGL ES.The size of data being uploaded. Note
          that  doesn't have a parameter like
          this; for non-compressed data, OpenGL computes the byte count based
          on the image's dimensions and format.Pointer to the compressed data.In addition to PVRTC formats, the iPhone also
      supports compressed paletted textures to be conformant to the OpenGL ES
      1.1 standard. But, paletted images on the iPhone won't buy you much;
      internally they get expanded into normal true-color images.The low-precision uncompressed formats (565,
    5551, and 4444) are often overlooked. Unlike block compression, they do
    not cause speckle artifacts in the image. While they work poorly with
    images that have smooth color gradients, they're quite good at preserving
    detail in photographs and keeping clean lines in simple vector art.At the time of this writing, the iPhone SDK
    does not contain any tools for encoding images to these formats, but the
    free PowerVR SDK from Imagination Technologies includes a tool called
     just for this purpose. Download the SDK as
    directed in . Extract the tarball
    archive if you haven't already.After opening up the tarball, execute the
    application in
    .Open your source image in the GUI, and select
    . After you choose a format (try RGB 565),
    you can save the output image to a PVR file. Save it as
     and add it to Xcode as described
    in . Next, go into both renderers, and
    find:And replace it with:You may have noticed that
     has many of the same capabilities as the
     program presented in the previous section,
    and much more. It can encode images to a plethora of formats, generate
    mipmap levels, and even dump out C header files that contain raw image
    data. This tool also has a command line variant to allow integration into
    a script or an Xcode build.We'll use the command line version of
       in Chapter 7 for generating a C header
      file that contains the raw data to an 8-bit alpha texture.Let's go ahead and flesh out some of the
    image-loading code to support the uncompressed low-precision formats. New
    lines in  are shown in bold in
    .Next we need to add some new code to the
     method in the rendering engine class,
    seen in . This code works for both ES
    1.1 and 2.0.Next, we need to make a change to
    :You can use Quartz to draw 2D paths into an
    OpenGL texture, resize the source image, convert from one format to
    another, and even generate text. We'll cover some of these techniques in
    ; for now let's go over a few simple ways to
    generate textures.One way of loading textures into OpenGL is
    creating a Quartz surface using whatever format you'd like, then drawing
    the source image to it, as seen in .As before, use the
         class method to create and
        allocate a  object which wraps a
         object.Since there are four components per pixel
        (RGBA), the number of bytes per pixel is half the number of bits per
        component.Allocate memory for the image surface and
        clear it to zeros.Create a Quartz context with the memory
        that was just allocated.Use Quartz to blit the source image onto
        the destination surface.Create an  object
        that wraps the memory that was allocated.If you want to try out the Quartz-loading code in the sample app,
    perform the following steps:Add  to
        .Add the following method declaration to
         in
        :In the  method in
        , change the
         call to
        .In your render engine's  method,
        make sure your minification filter is  and
        that you're calling .One advantage of loading images with Quartz is
    that you can have it do some transformations before uploading the image to
    OpenGL. For example, say you want to flip the image vertically. You could
    do so by simply adding the following two lines immediately before the line
    that calls :Another neat thing you can do with Quartz is
    generate new images from scratch in real time. This can shrink your
    application, making it faster to download. This is particularly important
    if you're trying to trim down to less than 10 MB, the maximum size that
    Apple allows for downloading over the 3G network. Of course, you can only
    do this for textures that contain simple vector-based images, as opposed
    to truly artistic content.For example, you could use Quartz to generate a
    256x256 texture that contains a blue filled-in circle, as in . The code for creating the surface should look
    familiar; lines of interest are shown in boldface.If you want to try out the circle-generation code in the sample app,
    perform the following steps:Add  to
        .Add the following method declaration to
         in
        :In the  method in
        , change the
         call to
        .In your render engine's  method,
        make sure your minification filter is  and
        that you're calling .Quartz is a rich 2D graphics API and could have
    a book all to itself, so I can't cover it here; check out Apple's online
    documentation for more information.Some of the biggest gotchas in texturing are
    the various constraints imposed on their size. Strictly speaking, OpenGL
    ES 1.1 stipulates that all textures must have dimensions that are powers
    of two, and OpenGL ES 2.0 has no such restriction. In the graphics
    community, textures that have a power-of-two width and height are commonly
    known as POT textures; non power-of-two textures are NPOT.For better or worse, the iPhone platform
    diverges from the OpenGL core specifications here. The POT constraint in
    ES 1.1 doesn't always apply, nor does the NPOT feature in ES 2.0.Newer iPhone models support an extension to ES
    1.1 that opens up the POT restriction, but only under a certain set of
    conditions. It's called
    , and it basically
    states the following:As hairy as this seems, it covers quite a few
    situations, including the common case of displaying a background texture
    with the same dimensions as the screen (320x480). Since it requires no
    minification, it doesn't need mipmapping, so you can create a texture
    object that fits "just right".Not all iPhones support the aforementioned
    extension to ES 1.1; the only sure-fire way to find out is by
    programmatically checking for the extension string, which can be done like
    this:The extensions string returned by OpenGL is
      long and space-delimited, so it's a bit difficult for humans to read. As
      a useful diagnostic procedure, I often dump a "pretty print" of the
      extensions list to Xcode's Debugger Console at startup time. This can be
      done with the following code snippet:If your 320x480 texture needs to be mipmapped
    (or if you're supporting older iPhones) then you can simply use a 512x512
    texture and adjust your texture coordinates to address a 320x480
    subregion. One quick way of doing this is with a texture matrix:Unfortunately, the portions of the image that
    lie outside the 320x480 subregion are wasted. If this causes you to
    grimace, keep in mind that you can add "mini-textures" to those unused
    regions. Doing so makes the texture into a , which we'll discuss further in .If you don't want to use a 512x512 texture,
    then it's possible to create five POT textures and carefully puzzle them
    together to fit the screen, as seen in . This is a hassle though, and I don't
    recommend it unless you have a strong penchant for masochism.By the way, according to the official OpenGL ES
    2.0 specification, NPOT textures are actually allowed in
     situation! Apple has made a minor transgression
    here by imposing the aforementioned limitations.Keep in mind that even when the POT restriction
    applies, your texture can still be non-square (e.g., 512x256), unless it
    uses a compressed format.Think these are a lot of rules to juggle? Well
    it's not over yet! Textures also have a maximum allowable size. At the
    time of this writing, the first two iPhone generations have a maximum size
    of 1024x1024, and third generation devices have a maximum size of
    2048x2048. Again, the only way to be sure is querying its capabilities at
    run time, like so:Don't groan, but there's yet another gotcha I
    want to mention regarding texture dimensions. By default, OpenGL expects
    each row of uncompressed texture data to be aligned on a four-byte
    boundary. This isn't a concern if your texture is
     with ; in this
    case, the data is always properly aligned. However, if your format has a
    texel size less than four bytes, you should take care to ensure each row
    is padded out to the proper alignment. Alternatively you can turn off
    OpenGL's alignment restriction like this:Also be aware that the PNG decoder in Quartz
    may or may not internally align the image data; this can be a concern if
    you load images using the  method
    presented in . It's more
    robust (but less performant) to load in images by drawing to a Quartz
    surface, which we'll go over in the next section.Before moving on, I'll forewarn you of yet
    another thing to watch out for: the iPhone simulator doesn't necessarily
    impose the same restrictions on texture size that a physical device would.
    Many developers throw up their hands and simply stick to power-of-two
    dimensions only; I'll show you how to make this easier in the next
    section.One way to ensure that your textures are
      power-of-two is to scale them using Quartz. Normally I'd recommend
      storing the images in the desired size rather than scaling them at run
      time, but there are reasons why you might want to scale at run time. For
      example, you might be creating a texture that was generated from the
      iPhone camera (which we'll demonstrate in the next section).For the sake of example, let's walk through
      the process of adding a scale-to-POT feature to your
       class. First add a new field to the
       structure called
      , as seen in bold in .We'll use this to store the image's original
      size; this is useful, for example, to retrieve the original aspect
      ratio. Now let's go ahead and create the new
      ()
      method, as seen in . is fairly
      straightforward; most of it is the same as the
       method presented in the previous section,
      with the exception of the  method. It's
      amazing what can be done with some bit shifting! If the input to the
       method is already a power of two, then it
      returns the same value back to the caller; if not, it returns the
       power of two. I won't bore you with the
      derivation of this algorithm, but it's fun to impress your colleagues
      with this trick.For the grand finale sample of this chapter,
    let's create an app called  which allows
    the user to snap a photo and wrap it around an ellipsoid (a squashed
    sphere). The embarrassingly simple user interface consists of a single
    button for taking a new photo, as seen in . We'll also add some animation by
    periodically spinning the ellipsoid along the X axis.Unlike much of the sample code in this book,
    the interesting parts here will actually be in Objective C rather than
    C++. The application logic is simple enough that we can dispense with the
     interface.Using Model Viewer as the baseline, start by
    removing all the ApplicationEngine related code as follows: The code won't build until we fill it out a bit more.
    Replace the  interface in
     with , and move the
     and 
    type definitions to the top of the file.Remove  and
           from
          .Remove the
           file
          from the Xcode project and send it to trash.Remove the  field from
          .Remove the call to 
          from .Replace the call to
           with
          .Remove ,
          , and 
          from .The  method
        loads the button textures and creates the vertex buffer
        objects.The  method takes
        the following three parameters:Multiplication factor for squashing
              the sphere into an ellipse.Angle (in degrees) for X-axis
              rotation.Controls the image on the button,
              which either says "Take Picture" or "Please Wait".The 
        method tells the rendering engine to load in a new texture to wrap
        around the sphere.We'll go over the implementation of these
    methods later. Let's jump back to the Objective C since that's where the
    interesting stuff is. For starters, we need to modify the
     class declaration by adopting a couple new
    protocols and adding a few data fields; see . New code is shown in bold.Recall that in Objective C, the <>
        notation is used on a class declaration to adopt one or more
        protocols. (So far, the only other protocol we've come across is
        .) In this case, we're
        adopting  in order to
        respond to a couple camera-related events, as you'll see later.We must also adopt the
         protocol to handle
        camera events.Declare a
         pointer for the camera interface.
        So far this book has been avoiding view controllers, instead focusing
        on pure OpenGL ES, but in this case a view controller is required.
        This is the only way you can use the camera API on the iPhone.While the camera interface is visible, we
        need to stop the recurrent rendering of the OpenGL scene; the
         variable is used to indicate this
        state.The  variable
        indicates the amount of scale to apply on the Z axis to flatten the
        sphere into an ellipsoid. By varying this, we'll achieve a pulsing
        effect.The  variable
        indicates amount of rotation (in degrees) of the ellipsoid along its X
        axis.Next, open  and
    rewrite the  method as in . The code that computes the time step
    is the same as previous examples; perhaps more interesting are the
    mathematical shenanigans used to oscillate between two types of useless
    and silly animation: "spinning" and "pulsing".While we're still in
    , let's go ahead and write the touch
    handler. Because of the embarrassingly simple UI, we only need to handle a
    single touch event: , as seen in . Note that the first thing it does
    is check if the touch location lies within the bounds of the button's
    rectangle; if not, it returns early.When developing with UIKit, the usual
      convention is that the view controller owns the view, but in this case,
      the view owns the view controller. This is acceptable in our situation,
      since our application is mostly rendered with OpenGL, and we want to
      achieve the desired functionality in the simplest possible way. I'm
      hoping that Apple will release a lower-level camera API in future
      versions of the SDK, so that we don't need to bother with view
      controllers.Perhaps the most interesting piece in  is the code that checks if the
    camera is supported; if so, it sets the camera as the picker's source
    type:I recommend following this pattern even if you
    know a priori that your application will only run on devices with cameras.
    The fallback path provides a convenient testing platform on the iPhone
    simulator; by default, the image picker simply opens a file picker with
    image thumbnails.Next we'll add a couple new methods to
     for implementing the
     protocol, as seen in
    . Depending on the megapixel
    resolution of your camera, the captured image can be quite large, much
    larger than what we need for an OpenGL texture. So, the first thing we do
    is scale the image down to 256x256. Since this destroys the aspect ratio,
    we'll store the original image's dimensions in the
     structure just in case. A more
    detailed explanation of the code follows the listing.The default camera interface includes a
        cancel button to allow the user to back out. When this occurs, we
        release the image picker and re-enable the OpenGL rendering
        loop.The
        
        method gets called when the user is done picking the image (or, in our
        case, taking a picture). The handler receives two parameters: a
        pointer to the picker control, and a dictionary of key-value pairs
        from which the image can be extracted.The camera API provides the orientation of
        the device when the picture was taken; in a subsequent step we'll use
        this information to rotate the image to an upright position.As mentioned earlier, we're scaling the
        image to 256x256, so here we allocate the destination memory assuming
        four bytes per pixel.Rotate the image before drawing it to the
        destination surface. The 
        function assumes that the axis of rotation is at (0,0), so we first
        shift the image to move its center to (0,0). After the rotation, we
        translate it back to its original position.Tell the rendering engine to upload a new
        texture by passing it a filled-in
         structure and a pointer to the
        raw data.The currently hidden OpenGL surface still
        shows the ellipsoid with the old texture, so before removing the
        picker UI we update the OpenGL surface. This prevents a momentary
        flicker after closing the image picker.Much like the
         method, we now
        dismiss the view controller and release the picker control.Crack your OpenGL ES knuckles; it's time to
      implement the rendering engine using ES 1.1. Go ahead and remove the
      contents of  and add the new
      class declaration and  method, seen in
      .There are no new concepts in ; at a high level, the
       method performs the following
      tasks:Create two vertex buffers using the
          parametric surface helper: a quad for the button, and a sphere for
          the ellipsoid.Creates three textures: the initial
          ellipsoid texture, the "Please Wait" text, and the "Take Picture"
          button text. (We'll learn better ways of rendering text in future
          chapters.)Perform some standard initialization
          work, such as creating the FBO and setting up the transformation
          matrices.Next, let's implement the two public methods,
       and , as
      seen in .That was simple! Next we'll implement the
      four private methods ().Much of  is fairly
      straightforward. The  method is used both
      for camera data (where the raw data is passed in), and for image files
      (where the raw data is obtained from the resource manager).We won't bother with an ES 2.0 backend in this case, so you'll
      want to turn on the  flag in
      , comment out the call to
      , and remove
       from the project.At this point, you're almost ready to run the sample, but you'll
      need a few image files (,
      , and
      ). You can obtain these files from
      the book's example code (see ) in
      the "CameraTexture" sample. You'll also want to copy over the
       and  class definitions
      from ; they've been tweaked to
      generate good texture coordinates.This completes the CameraTexture sample,
      another fun but useless iPhone program!In this chapter we went over the basics of
    texturing, and we presented several methods of loading them into an iPhone
    application:Use
         to access the raw data from
        the standard PNG decoder. ()Use 
        () or
         () to generate a PVR file as part of the
        build process, then parse its header at run time (). This is the best method to use
        in production code.Create a Quartz surface in the desired
        format, and draw the source image to it using
        . ()Create a Quartz surface in a power-of-two
        size, and scale the source image. ()We also presented a couple ways of generating
    new textures at run time:Use Quartz to draw vector art into a
        surface. ()Using the
         API to snap a photo, then shrink
        it down with Quartz. (the CameraTexture sample, starting on )Texturing is a deep subject, and we actually
    left quite a lot out of this chapter. New techniques relating to texturing
    will be covered in .All colors are the friends of their neighbors
    and the lovers of their opposites.If you've ever used Photoshop to place yourself
  in front of the Taj Mahal in a paroxysm of wishful thinking, you're probably
  familiar with layers and opacity. Alpha simply represents opacity on a
  zero-to-one scale: zero is transparent, one is fully opaque. Alpha can be
  used both with and without textures, and in this chapter we'll pay special
  attention to textures that contain alpha.  is
  the process of compositing a source color with an existing pixel in the
  framebuffer.Tangentially related to blending is
  , or the attempt to mask "jaggies".
  Antialiased vector art (such as the circle texture we generated in the
  previous chapter) varies the alpha along the edges of the artwork to allow
  it to blend into the background. Antialiasing is also often used for lines
  and triangle edges, but unfortunately the iPhone's OpenGL implementation
  does not support this at present. Fret not, there are ways to get around
  this limitation, as you'll see in this chapter.Also associated with blending are
  heads-up-displays and . Augmented
  reality is the process of overlaying computer-generated imagery with
  real-world imagery, and the iPhone is particularly well-suited for this.
  We'll wrap up the chapter by walking through a sample app that mixes OpenGL
  content with the iPhone's camera interface, and we'll use the compass and
  accelerometer APIs to compute the view matrix. Overlaying the environment
  with fine Mughal architecture will be left as an exercise to the
  reader.Some of my favorite YouTube videos belong to
    the  series. The episode featuring the
    pulverization of an iPhone is a perennial favorite, seconded only by the
    Chuck Norris episode. Alas, this chapter deals with blending of a
    different sort. OpenGL blending requires five ingredients:Ensure your color contains alpha. If it
        comes from a texture, make sure the texture format contains alpha; if
        it comes from a vertex attribute, make sure it has all four color
        components.Disable depth testing.Pay attention to the ordering of your draw calls.Enable blending.Set your blending function.For step five, I'm giving a rather classic blending equation as an
    example, but that's not always what you'll want! (More on this later.)
    Specifically, the above function call sets up the following
    equation: is the source color,
     is the starting destination color, and
     is the final destination color. By default,
    OpenGL's blending equation is this:Since the default blending function ignores
    alpha, blending is effectively turned off even when you've enabled it with
    . So, always remember to set your blending
    function — this is a common pitfall in OpenGL programming.Here's the formal declaration of
    :The blending equation is always an operation on
    two scaled operands: the source color and the destination color. The
    template to the equation is this:The  and
     arguments can be any of the
    following:Multiplies the operand with zero.Multiplies the operand with one.Multiplies the operand by the alpha
          component of the source color.Multiplies the operand by the inverted
          alpha component of the source color.Multiplies the operand by the alpha
          component of the destination color.Multiplies the operand by the inverted
          alpha component of the destination color.Additionally, the
     parameter supports the following:Component-wise multiplication of the
          operand with the destination color.Component-wise multiplication of the
          operand with the inverted destination color.Returns the minimum of source alpha and
          inverted destination alpha. This exists mostly for historical
          reasons, as it was required for an outmoded anti-aliasing
          technique.And the 
    parameter also supports the following:Component-wise multiplication of the
          operand with the source color.Component-wise multiplication of the
          operand with the inverted source color.OpenGL ES 2.0 relaxes the blending constraints by unifying the set
    of choices for  and
    , with the exception of
    .ES 2.0 also adds the concept of "constant
      color", specified via . For more
      information, look up  and
       at the Khronos website:One of the biggest gotchas with textures on
    Apple devices is the issue of .
    If the RGB components in an image have already been scaled by their
    associated alpha value, the image is considered to be premultiplied.
    Normally, PNG images do not store premultiplied RGB values, but Xcode does
    some tampering with them when it creates the application bundle.You might recall that we passed in a flag to
    the  mask that's related to this;  shows a
    snippet of the  class presented in the
    previous chapter, with the flag of interest highlighted in bold.For non-premultiplied alpha, there's a flag
    called  which you're welcome to try,
    but at the time of this writing, the Quartz implementation on the iPhone
    does not support it, and I doubt it ever will, because of the funky
    pre-processing that Xcode performs on image files.So, you're stuck with pre-multiplied alpha.
    Don't panic! There are two rather elegant ways to deal with it:Use 
        to encode your data into a PVR file. Remember,
         can encode your image into
         OpenGL format; it's not restricted to the
        compressed formats.Or, adjust your blending equation so that
        it takes premultiplied alpha into account, like so:By using  for the
        sfactor argument, you're telling OpenGL there's no need to multiply
        the RGB components by alpha.In the last chapter, we also presented a
      method of loading PNG files using
      , but with that technique, the
      simulator and the device can differ in how they treat alpha. Again, I
      recommend using PVR files for fast and reliable results.In , the left column contains a normal texture,
    and the right column contains a texture with pre-multiplied alpha. In
    every row, the  argument is
    .Summarizing the best results from :For textures with straight alpha, set
         to  and
         to
        .For textures with premultiplied alpha, set
         to  and
         to
        .To check if a texture has premultiplied
        alpha, disable blending and look at the silhouette.It's important to remember to disable depth
    testing when blending is enabled. If depth testing is turned on, triangles
    that lie beneath other triangles get completely rejected, so their color
    can't contribute to the frame buffer.An equally important caveat is that you should
    render your triangles in back-to-front order; the standard blending math
    simply doesn't work if you try to draw the top layer before the layer
    beneath it. Let's demonstrate why this is so. Suppose you'd like to depict
    a half-opaque red triangle on top of a half-opaque green triangle.
    Assuming the clear color is black, the history of a pixel in the
    framebuffer would look like this if you use back-to-front ordering:Clear to Black. Result: (0, 0, 0)Draw the half-opaque green triangle.
        Result: (0, 0.5, 0)Draw the half-opaque red triangle. Result:
        (0.5, 0.25, 0)So the resulting pixel is a yellowish red; this
    is what you'd expect. If you try to draw the red triangle first, the
    result is different:Clear to Black. Result: (0, 0, 0)Draw the half-opaque red triangle. Result:
        (0.5, 0, 0)Draw the half-opaque green triangle.
        Result: (0.25, 0.5, 0)Now you have yellowish green. Order matters
    when you're blending! Incidentally, there's a way to adjust the blending
    equations so that you can draw in front-to-back order instead of
    back-to-front; we'll show how in the next section.When blending is enabled, sort your draw
      calls from furthest to nearest, and disable depth testing.OpenGL ES supports an alternative to blending
      called . Rather than applying an
      equation at every pixel to determine the final color, alpha testing
      performs a simple comparison of the source alpha with a pre-supplied
      value. If the comparison fails, the framebuffer is left unchanged,
      otherwise the source color is written out. Alpha testing is generally
      used less frequently than blending because it cannot be used for smooth
      antialiasing or fade-out effects. In some ways, alpha testing is similar
      to blending with one-bit alpha.With alpha testing, there's no back-to-front
      ordering requirement like there is with blending. It's simple to enable
      and configure:The first parameter of
       is a comparison function, similar to the
      depth comparison function (see ). The
      second parameter is the reference value used for comparison. Use alpha
      testing with caution: Apple's documentation warns that it can adversely
      affect performance.Always remember to check for extension support
    using the method described in . At the time of this writing, the
    iPhone supports the following blending-related extensions in OpenGL ES
    1.1:Allows you to specify a blending
          operation other than addition; namely, subtraction.Allows you to specify two separate
          blending operations: one for RGB, the other for alpha.Allows you to specify two separate pairs
          of blend factors: one pair for RGB, the other for alpha.With OpenGL ES 2.0, these extensions are part
    of the core specification. Together they declare the following
    functions:For ES 1.1, remember to append
     to the end of each function since that's the naming
    convention for extensions.The parameters to
     and
     can be one of the
    following:Add the source operand to the source
          operand; this is the default.Subtracts the destination operand from
          the source operand.Subtracts the source operand from the
          destination operand.Again, remember to append
     for these constants when working with ES
    1.1.When all these extensions are supported, you
    effectively have the ability to specify two unique equations: one for
    alpha, the other for RGB. Each equation conforms to one of the following
    templates:You might wonder why you'd ever need all the
      flexibility given by the aforementioned blending extensions. You'll see
      how various blending configurations come in handy with some samples
      presented later in the chapter, but I'll briefly go over some common
      uses here.One use of
       is inverting a region of color on
      the screen to highlight it. Simply use  for
      your sfactor and  for your dfactor. You
      could also use subtraction to perform a comparison, or visual "diff"
      between two images.The separate blending equations can be useful
      too. For example, perhaps you'd like to leave the destination's alpha
      channel unperturbed because you're storing information there for
      something other than transparency. In such a case, you could say:Another time to use separate blending
      equations is when you need to draw your triangles in front-to-back order
      rather than the usual back-to-front order. As you'll see later in the
      chapter, this can be useful for certain effects. To pull this off, take
      the following steps:Set your clear color to (0, 0, 0,
          1).Make sure your source texture (or
          per-vertex color) has premultiplied alpha.Set your blend equation to the
          following:To see why this works, let's go back to the
      example of a half-opaque red triangle being rendered on top of a
      half-opaque green triangle:Clear to Black. Result: (0, 0, 0,
          1)Draw the half-opaque red triangle. Since
          it's pre-multiplied, its source color is (0.5, 0, 0, 0.5). Using the
          above blending equation, the result is (0.5, 0, 0, 0.5).Draw the half-opaque green triangle; its
          source color is (0, 0.5, 0, 0.5). The result after blending is (0.5,
          0.25, 0, 0.25).The resulting pixel is yellowish red, just as
      you'd expect. Note that the framebuffer's alpha value is always inverted
      when you're using this trick.Sometimes you'll need to uniformly tweak the
    alpha values across an entire texture. For example, you may wish to create
    a fade-in effect, or make a texture semi-transparent for drawing a
     (heads up display).With OpenGL ES 1.1, this can be achieved simply
    by adjusting the current vertex color:By default, OpenGL multiplies each component of
    the current vertex color with the color of the texel that it's rendering.
    This is known as , and it's actually only
    one of many ways that you can combine texture color with per-vertex color
    (this will be discussed in detail later in the book).If you're using a texture with pre-multiplied
    alpha, then the vertex color should also be pre-multiplied. The
    aforementioned function call should be changed to:Sometimes you may wish to throttle back only
    one color channel. For example, say your app needs to render some red and
    blue buttons, and that all the buttons are identical except for their
    color. Rather than wasting memory with multiple texture objects, you can
    create a single grayscale texture and modulate its color, like
    this:With ES 2.0, the modulation needs to be
    performed within the pixel shader itself:The above code snippet should look familiar. We
    used the same technique in  when combining
    lighting color with texture color.One use for blending in a 3D scene is
    overlaying a reflection on top of a surface, as seen on the left of . Remember,
    computer graphics is often about cheating! To create the reflection, you
    can re-draw the object using an upside-down projection matrix. Note that
    you need a way to prevent the reflection from "leaking" outside the bounds
    of the reflective surface, as shown on the right in . How can this
    be done?It turns out that third-generation iPhones and
    iPod Touches have support for an OpenGL ES feature known as the
    , and it's well-suited to this
    problem. The stencil buffer is actually just another type of renderbuffer,
    much like color and depth. But instead of containing RGB or Z values, it
    holds a small integer value at every pixel that you can use in different
    ways. There are many applications for the stencil buffer beyond clipping,
    including a shadow technique that we'll present later in the book.To accommodate older iPhones, we'll cover
      some alternatives to stenciling later in the chapter.To check if stenciling is supported on the
    iPhone, check for the  extension using
    the method in . At the time of this of
    this writing, stenciling is supported on third generation devices and the
    simulator, but not on first and second generation devices.The reflection trick can be achieved in four
    steps: (see )Render the disk to stencil only.Render the reflection of the floating
        object with the stencil test enabled.Clear the depth buffer and render the
        actual floating object.Render the disk using front-to-back
        blending.Note that the reflection is drawn
     the textured podium, hence the front-to-back
    blending. We can't render the reflection after the podium because blending
    and depth-testing cannot not both be enabled when drawing complex
    geometry.The complete code for this sample is available
    from O'Reilly's website, but we'll go over the key snippets in the
    following sub-sections. First let's take a look at the creation of the
    stencil buffer itself. The first few steps are generating a renderbuffer
    identifier, binding it, and allocating storage. This may look familiar if
    you remember how to create the depth buffer:Next, attach the stencil buffer to the
    framebuffer object, shown in bold here:As always, remember to omit the OES endings
    when working with ES 2.0.To save memory, sometimes you can interleave
    the depth buffer and stencil buffer into a single renderbuffer. This is
    possible only when the 
    extension is supported. At the time of this writing, it's available on
    third generation devices, but not on the simulator or older devices. To
    see how to use this extension, see . Relevant portions are highlighted in
    bold.Recall that step one in our reflection demo
      renders the disk to the stencil buffer. Before drawing to the stencil
      buffer, it needs to be cleared, just like any other renderbuffer:Next you need to tell OpenGL to enable writes
      to the stencil buffer, and you need to tell it what stencil value you'd
      like to write. Since you're using an eight-bit buffer in this case, you
      can set any value between 0x00 and 0xff. Let's go with 0xff, and set up
      the OpenGL state like this:The first line enables
      , which is a somewhat misleading name
      in this case; you're  to the stencil buffer,
      not  against it. If you don't enable
      , then OpenGL assumes you're not
      working with the stencil buffer at all.The next line,
      , tells OpenGL which stencil operation
      you'd like to perform at each pixel. Here's the formal
      declaration:Specifies the operation to perform when
            the stencil test fails.Specifies the operation to perform when
            the stencil test passes and the depth test fails.Specifies the operation to perform when
            the stencil test passes and the depth test passes.Since the disk is the first draw call in the
      scene, we don't care whether any of these tests fail or not, so we've
      set them all to the same value.Each of the arguments to
       can be one of the following:Replace the value that's currently in
            the stencil buffer with the value specified in
            .Don't do anything.Increment the value that's currently in
            the stencil buffer.Decrement the value that's currently in
            the stencil buffer.Perform a bitwise NOT operation with
            the value that's currently in the stencil buffer.Clobber the current stencil buffer
            value with zero.Again, this may seem like way too much
      flexibility, more than you'd ever need. Later in the book, you'll see
      how all this freedom can be used to perform interesting tricks. For now,
      all we're doing is writing the shape of the disk out to the stencil
      buffer, so we're using the 
      operation.The next function we called to set up our
      stencil state is . Here's its function
      declaration:Specifies the comparison function to
            use for the stencil test, much like the depth test .This "reference value" actually serves
            two purposes:Comparison value to test against
                  if  is something other than
                   or
                  .The value to write if the
                  operation is .Before performing a comparison, this
            bit mask gets ANDed with both the reference value and the value
            that's already in the buffer.Again, this gives the developer quite a bit
      of power, but in this case we only need something simple.Getting back to the task at hand, check out
       to see how
      to render the disk to the stencil buffer only. I adjusted the
      indentation of the code to show how certain pieces of OpenGL state get
      modified before the draw call, then restored after the draw call.Two new function calls appear in :
       and .
      Recall that we're only interested in affecting values in the stencil
      buffer. It's actually perfectly fine to write to all three renderbuffers
      (color, depth, stencil), but to maximize performance, it's good practice
      to disable any writes that you don't need.The four arguments to
       allow you to toggle each of the
      individual color channels; in this case we don't need any of them. Note
      that  has only one argument, since it's a
      single-component buffer. Incidentally, OpenGL ES also provides a
       function, which we're not using
      here.Step two renders the reflection of the object
      and uses the stencil buffer to clip it to the boundary of the disk.
       shows
      how to do this.This time we don't need to change the values
      in the stencil buffer, so we use  for the
      argument to . We changed the stencil
      comparison function to  so that only the
      pixels within the correct region will pass.There are several ways you could go about
      drawing an object upside-down, but I chose to do it with a
      quick-and-dirty projection matrix. The result isn't a very accurate
      reflection, but it's good enough to fool the viewer!  shows how I
      did this using a  method from the C++ vector
      library in Appendix A. (For ES 1.1, you could simply use the provided
       function.)The next step is rather mundane; we simply
      need to render the actual floating object, without doing anything with
      the stencil buffer. Before calling  for
      the object, we turn off the stencil test and disable the depth
      buffer:For the first time, we've found a reason to
      call  somewhere in the of the  method! Importantly, we're
      clearing only the depth buffer, leaving the color buffer intact.Remember, the reflection is drawn just like
      any other 3D object, complete with depth-testing. Allowing the actual
      object to be occluded by the reflection would destroy the illusion, so
      it's a good idea to clear the depth buffer before drawing it. Given the
      fixed position of the camera in our demo, we could actually get away
      without performing the clear, but this allows us to tweak the demo
      without breaking anything.The final step is rendering the marble disk
      underneath the reflection.  sets this up. That's it for the stencil sample! As always,
      head over to the O'Reilly site to download (see ) the complete code.If your app needs to accommodate first and
    second generation iPhones, in many cases you can use a trick that acts
    like stenciling without actually requiring a stencil buffer. These various
    tricks include:Using the framebuffer's alpha component to
        store the "stencil" values, and setting up a blending equation that
        tests against those values.Turning off color writes and writing to the
        depth buffer to mask out certain regions. (The easiest way to
        uniformly offset generated depth values is with the
         function.)Cropping simple rectangular regions can be
        achieved with OpenGL's  function.Some of the bitwise operations available
        with stencil buffers are actually possible with colors as well. In
        fact, there are additional operations possible with colors, such as
        XOR. To see how to do this, check out the 
        function.Let's demonstrate the first trick in the above
    list: using framebuffer alpha as a fake stencil buffer. With this
    technique, it's possible to achieve the result shown in  on older
    iPhones. The sequence of operations becomes:Clear the depth buffer.Render the background image with
        α=0.Render the textured disk normally with
        α=1.Enable blending and set the blending
        equation to
        .Render the reflection of the floating
        object.Set the blending equation to
        .Turn off depth testing and render the
        textured disk again with α=0.5; this fades out the reflection a
        bit.Clear the depth buffer and re-enable depth
        testing.Render the actual floating object.The rendering code for these nine steps is
    shown in . As
    always, the entire sample code is available from O'Reilly's website (see
    ).The iPhone's first-class support for
    framebuffer objects is perhaps its greatest enabler of unique effects. In
    every sample presented so far in the book, we've been using a single FBO:
    namely, the FBO that represents the visible Core Graphics layer. It's
    important to realize that FBOs can also be created as , meaning they don't show up on the screen unless
    bound to a texture. In fact, on most platforms, FBOs are offscreen. The iPhone is rather unique in that the visible
    layer is itself treated as a FBO (albeit a special one).Binding offscreen FBOs to textures enables a
    whole slew of interesting effects, including page curling animations,
    water ripples, and more. We'll cover some of these techniques later in the
    book, but recall that one of the topics of this chapter is antialiasing.
    Several sneaky tricks with FBOs can be used to achieve full-scene
    antialiasing, even though the iPhone does not directly support
    antialiasing! We'll cover two of these techniques in the following
    sub-sections.One technique not discussed here is
      performing a post-process on the final image to soften it. While this is
      not true anti-aliasing, it may produce good results in some cases. It's
      similar to the bloom effect covered in .The easiest and crudest way to achieve
      full-scene antialiasing on the iPhone is to leverage bilinear texture
      filtering. Simply render to an offscreen FBO that has twice the
      dimensions of the screen, then bind it to a texture and scale it down,
      as seen in . This technique is known as
      .To demonstate how to achieve this effect,
      we'll walk through the process of extending the stencil sample to use
      supersampling. As an added bonus, we'll throw in an Apple-esque flipping
      animation, as seen in . Since we're creating a secondary FBO
      anyway, flipping effects like this come virtually for free.The  class
      declaration and related type definitions are shown in .
      Class members that carry over from previous samples are replaced with an
      ellipses for brevity.The "small" FBO is attached to the
          visible EAGL layer (320x480). The "big" FBO is the 640x960 surface
          that contains the 3D scene.The small FBO does not need depth or
          stencil attachments because the only thing it contains is a
          full-screen quad; the big FBO is where most of the 3D rendering
          takes place, so it needs depth and stencil.The 3D scene requires a marble texture
          for the podium and one background for each side of the animation
          (). The
          fourth texture object, , is
          attached to the big FBO.The application layer passes in
           to control the rotation of the
          podium, and  to control the flipping
          transitions. is a new
          private method for convinently determining the size of the currently
          bound FBO. This method helps avoid the temptation to hard-code some
          magic numbers, or duplicate state that OpenGL already
          maintains.First let's take a look at the
       implementation (), which returns a width-height pair for the
      size. The return type is an instance of , one of
      the types defined in the C++ vector library in Appendix A.Next let's deal with the creation of the two
      FBOs. Recall the steps we've been using for creating the on-screen FBO
      used in almost every sample so far:In the 
          constructor, generate an identifier for the color renderbuffer, then
          bind it to the pipeline.In the  class
          (Objective C), allocate storage for the color renderbuffer like
          so:In the
           method, create a
          framebuffer object and attach the color renderbuffer to it.If desired, create and allocate
          renderbuffers for depth and stencil, then attach them to the
          FBO.For the supersampling sample that we're
      writing, we still need to perform the first three steps in the above
      sequence, but then we follow it with the creation of the offscreen FBO.
      Unlike the on-screen FBO, its color buffer is allocated in much the same
      manner as depth and stencil:See  for the  method
      used in the supersampling sample.You may have noticed two new FBO related
      function calls in :
       and
      . The formal function
      declarations look like this:(As usual, the OES suffix can be removed for ES 2.0)The
       function allows you to cast
      a color buffer into a texture object. FBO texture objects get set up
      just like any other texture object: they have an identifier created with
      , they have filter and wrap modes, and
      they have a format which should match the format of the FBO. The main
      difference with FBO textures is the fact that null gets passed to the
      last argument of , since there's no image
      data to upload.Note that the texture in  has
      non-power-of-two dimensions, so it specifies clamp-to-edge wrapping to
      accommodate third generation devices. For older iPhones, the sample
      won't work; you'd have to change it to POT dimensions. Refer to  for hints on how to do this.
      Keep in mind that the values passed to 
      need not match the size of the render buffer; this comes in handy when
      rendering to a NPOT sub-region of a POT texture.The other new function,
      , is a useful sanity check
      to make sure that an FBO has been set up properly. It's easy to bungle
      the creation of FBOs if the sizes of the attachments don't match up, or
      if their formats are incompatible with each other.
       returns one of the
      following values, which are fairly self-explanatory:GL_FRAMEBUFFER_COMPLETEGL_FRAMEBUFFER_INCOMPLETE_ATTACHMENTGL_FRAMEBUFFER_INCOMPLETE_MISSING_ATTACHMENTGL_FRAMEBUFFER_INCOMPLETE_DIMENSIONSGL_FRAMEBUFFER_INCOMPLETE_FORMATSGL_FRAMEBUFFER_UNSUPPORTEDOpenGL ES also supports a more generally
        useful diagnostic function called . Its
        function declaration is simple:The possible return values are:GL_NO_ERRORGL_INVALID_ENUMGL_INVALID_VALUEGL_INVALID_OPERATIONGL_STACK_OVERFLOWGL_STACK_UNDERFLOWGL_OUT_OF_MEMORYAlthough this book doesn't call
         in any of the sample code, always keep
        it in mind as a useful debugging tool. Some developers like to
        sprinkle it throughout their OpenGL code as a matter of habit, much
        like an assert.Aside from building FBO objects, another
        error-prone activity in OpenGL is building shader objects in ES 2.0.
        Flip back to   to
        see how to detect shader errors.Next let's take a look at the render method
      of the supersampling sample. Recall from the class declaration that the
      application layer passes in  to control
      the rotation of the podium, and  to control
      the flipping transitions. So, the first thing the
       method does is look at
       to determine which background image should
      be displayed, and which shape should be shown on the podium. See .Most of  is fairly straightforward. One piece that
      may have caught your eye is the small optimization made right before
      blitting the offscreen FBO to the screen:This is a sneaky little trick. Since the quad
      is the exact same size as the screen, there's no need to clear the color
      buffer; unnecessarily issuing a  can hurt
      performance. However, if a flipping animation is currently underway, the
      color buffer needs to be cleared to prevent artifacts from appearing in
      the background; flip back to   and
      observe the black areas. If  is a multiple of
      180, then the quad completely fills the screen, so there's no need to
      issue a clear.That's it for the supersampling sample. The
      quality of the anti-aliasing is actually not that great; you can still
      see some "stair stepping" along the bottom outline of the shape in . You might
      think that creating an even bigger offscreen buffer, say quadruple-size,
      would provide higher-quality results. Unfortunately, using a
      quadruple-size buffer would require two passes; directly applying a
      1280x1920 texture to a 320x480 quad isn't sufficient because
       filtering only samples from a 2x2
      neighborhood of pixels. To achieve the desired result, you'd actually
      need  FBOs as follows:1280x1920 offscreen FBO for the 3D
          scene.640x960 offscreen FBO that contains a
          quad with the 1280x1920 texture applied to it.320x480 onscreen FBO that contains a quad
          with the 640x960 texture applied to it.Not only is this laborious, it's a memory
      hog. Older iPhones don't even support textures this large! Turns out
      there's another antialiasing strategy called
      , and it can produce high-quality
      results without the memory overhead of supersampling.Jittering is somewhat more complex to
      implement than supersampling, but it's not rocket science. The idea is
      to re-render the scene multiple times at slightly different viewpoints,
      merging the results along the way. You only need two FBOs for this
      method: the on-screen FBO that accumulates the color, and the offscreen
      FBO that the 3D scene is rendered to. You can create as many jittered
      samples as you'd like, and you still need only two FBOs. Of course, the
      more jittered samples you create, the longer it takes to create the
      final rendering. Pseudocode for the jittering algorithm is shown in
      .The key part of  is the blending
      configuration. By using a blend equation of plain old addition
      (), and dimming the color according to
      the number of samples, you're effectively accumulating an average
      color.An unfortunate side effect of jittering is
      reduced color precision; this can cause banding artifacts, as seen in
      . On some
      platforms the banding effect can be neutralized with a high-precision
      color buffer, but that's not supported on the iPhone. In practice, I
      find that creating too many samples is detrimental to performance
      anyway, so the banding effect isn't usually much of a concern.Determining the jitter offsets
      ( in ) is a bit of black art. Totally random
      values don't work well since they don't guarantee uniform spacing
      between samples. Interestingly, dividing up each pixel into an
      equally-spaced uniform grid does not work well either! Some commonly
      used jitter offsets are shown in .Let's walk through the process of creating a
      simple app with jittering. Much like we did with the supersample
      example, we'll include a fun transition animation. (The full project can
      be downloaded from the book's website a described in ) This time we'll use the jitter offsets
      to create a de-focusing effect, as seen in .To start things off, let's take a look at the
       class declaration and related types.
      It's not unlike the class we used for supersampling; the main
      differences are the labels we give to the FBOs.
       denotes the on-screen buffer and
       denotes the offscreen buffer. See . also adds a new private method called
      ; the implementation is shown in . Note that we're
      keeping the  argument that we used in the
      supersample example, but now we're using it to compute a scale factor
      for the jitter offset rather than a Y-axis rotation. If
       is 0 or 180, then the jitter offset is left
      unscaled, so the scene is in focus.The implementation to the main
       method is shown in . The call to
       is shown in bold. might give you sense of déjà vu; it's
      basically an implementation of the pseudocode algorithm that we already
      presented . One deviation is how we
      compute the dimming effect:The second line in the above snippet is there
      only for the special transition effect. In addition to de-focusing the
      scene, it's also brightened to simulate pupil dilation. If
       is 0 or 180, then  is
      left unscaled, so the scene has its normal brightness.An interesting variation on jittering is
      , which blurs out the near and
      distant portions of the scene. To pull this off, compute the viewing
      frustum such that a given slice (parallel to the viewing plane) stays
      the same with each jitter pass; this is the focus plane.Yet another effect is , which simulates the ghosting effect seen on displays
      with low response times. With each pass, make incremental adjustments to
      your animation and gradually fade-in the alpha value using
      .Sometimes full-screen antialiasing is more than
    you really need and can cause too much of a performance hit. You may find
    that you only need antialiasing on your line primitives rather than the
    entire scene. Normally this would be achieved in OpenGL ES like so:Alas, none of the iPhone models support this at
    the time of this writing. However, the simulator 
    support line smoothing; watch out for inconsistencies like this!A clever trick to work around this limitation
    is filling an alpha texture with a circle, then tessellating the lines
    into short triangle strips (). Texture coordinates are chosen such that the
    circle is stretched in the right places. That has the added benefit of
    allowing round end-cap styles and wide lines.Using a 16x16 circle for the texture works well
    for thick lines (see left circle in  and left panel in ). For thinner lines, I find that a highly
    blurred 16x16 texture produces good results (see right circle in  and right panel in
    ).Let's walk through the process of converting
    a line list into a textured triangle list. Each source vertex needs to be
    extruded into four new vertices. It helps to give each extrusion vector a
    name using cardinal directions, as seen in .Before going over the extrusion algorithm,
    let's set up an example scenario. Say we're rendering an animated stick
    figure similar to .
    Note that some vertices are shared by multiple lines, so it makes sense to
    use an index buffer. Suppose the application can render the stick figure
    using either line primitives or textured triangles. Let's define a
     structure that stores the vertex and index
    data for either the non-AA variant or the AA variant; see . The non-AA
    variant doesn't need texture coordinates, but we're including them for
    simplicity's sake.The function prototype for the extrusion
    method needs three arguments: the source 
    (lines), the destination  (triangles), and
    the desired line width. See  and refer back to  to visualize the
    six extrusion vectors (N, S, NE, NW, SW, SE).At this point, we've computed the
    positions of the extruded triangles, but we still haven't provided texture
    coordinates for the triangles, nor the contents of the index buffer. Note
    that the animated figure can change its vertex positions at every frame,
    but the number of lines stays the same. This means we can generate the
    index list only once; no need to recompute it at every frame. Same goes
    for the texture coordinates. Let's declare a couple functions for these
    start-of-day tasks:Flip back to  and note the number of triangles and vertices.
    Every line primitive extrudes into six triangles composed from eight
    vertices. Since every triangle requires three indices, the number of
    indices in the new index buffer is . This
    is different from the number of , which is
    only . See .Et voilà...you now know how to render
    antialiased lines on a device that doesn't support antialiased lines! To
    see this in action, check out the "AaLines" sample from the book's example
    code.In this chapter's introduction, we promised to
    present a poor man's augmented reality app. As a starting point, we'll
    create a 3D environment that includes the aforementioned geodesic dome
    with antialiased borders. We'll also render a mossy ground plane and some
    moving clouds in the background. Later we'll replace the clouds with a
    live camera image. Another interesting aspect to this sample is that it's
    designed for landscape mode; see .For rendering the AA lines in the dome, let's
    use a different trick than the one presented in the previous section.
    Rather than a filling a texture with a ciricle, let's fill it with a
    triangle, as seen in . By choosing texture coordinates in the right
    places (see the hollow circles in the figure) we'll be creating a thick
    border at every triangle.For controlling the camera, the app should use
    the compass and accelerometer APIs to truly qualify as an augmented
    reality app. However, initially let's just show four buttons in a HUD:
    touching any button will cause the environment to "scroll". Horizontal
    buttons control  (angle from North);
    vertical buttons control  (angle above
    horizon). These terms may be familiar to you if you're an astronomy
    buff.Later we'll replace the azimuth/altitude
    buttons with the compass and accelerometer APIs. The benefit of this
    approach is that we can easily provide a fallback option if the app
    discovers that the compass or accelerometer APIs are not available. This
    allows us to gracefully handle three scenarios: Show buttons for both azimuth and
        altitude. Show buttons for azimuth; use the
        accelerometer for altitude. Hide all buttons; use the accelerometer for
        altitude, and the compass for azimuth.In honor of my favorite TV show, the name of
    this sample is . Without further ado, let's
    begin!The basic skeleton for the Holodeck sample is
      much like every other sample we've presented since . The main difference is that we forgo creation of
      an  interface and instead place the
      application logic directly within the  class.
      There's very little logic required for this app anyway; most of the
      heavy footwork is done in the rendering engine. Skipping the application
      layer makes life easier when we add support for the accelerometer,
      compass, and camera APIs.Another difference lies in how we handle the
      dome geometry. Rather than loading in the vertices from an OBJ file, or
      generating them at run time, a Python script generates a C++ header file
      with the dome data, as seen in  (the full listing, along with the Holodeck
      project, can be downloaded from the book's website a described in ). This is perhaps the simplest possible
      way to load geometry into an OpenGL application, and some modelling
      tools can actually export their data as a C/C++ header file!The overall structure of the Holodeck project
      can be seen in .Note that this app has quite a few textures
      compared to our previous samples: six PNG files and two compressed PVRTC
      files. You can also see from the screenshot that we've added a new
      property to  called
      . Recall that this is a
      landscape-only app; if you don't set this property, you'll have to
      manually rotate the virtual iPhone every time you test it in the
      simulator. is much
      the same as in our other sample apps, except that the rendering engine
      interface is somewhat unique; see .The new  method
      takes three parameters:Azimuth in degrees. This is the
            horizontal angle off East.Altitude in degrees. This is the
            vertical angle off the horizon.Bit mask of flags for the HUD.The idea behind the
       mask is that the Objective C code
      () can determine the capabilities of the
      device, and whether or not a button is being pressed, so it sends this
      information to the rendering engine as a set of flags.For now let's ignore the buttons and focus on
      rendering the basic elements of the 3D scene. See  for
      the rendering engine declaration and related types. Utility methods that
      carry over from previous samples, such as
      , are replaced with ellipses for
      brevity.Note that 
      declares two new private methods:  for
      drawing compass direction labels, and a new
       method for creating the geodesic dome.
      Even though it declares eight different texture objects (which could be
      combined into a ; see ), it declares only three VBOs. The
       VBO is re-used for the buttons, the floor, and
      the floating text.Listing  is fairly
      straightforward. It first creates the VBOs and texture objects, then
      initializes various OpenGL state.Let's finally take a look at the
      all-important  method; see .Use a static variable to keep a frame
          count for animation. I don't recommend this approach in production
          code (normally you'd use a delta-time value), but this is fine for
          an example.Rotate theta degrees (azimuth) about the
          Y axis, and phi degrees (altitude) about the X axis.We're clearing depth only; no need to
          clear color since we're drawing a sky sphere.Render the sky sphere.Render the geodesic dome with blending
          enabled.Create an animated variable called
           for the pulse effect, then pass it in
          to the  method.Draw the mossy ground plane.Render the buttons only if the
           mask is non-zero. We'll cover button
          rendering shortly.The  method is
      fairly straightforward; see . Some  trickery
      is used to stretch out the quad and flip it around.Most applications that need to render a HUD
      take the following approach when rendering a single frame of
      animation:Issue a
          .Set up the model-view and projection
          matrices for the 3D scene.Render the 3D scene.Disable depth testing and enable
          blending.Set up the model-view and projection
          matrices for 2D rendering.Render the HUD.Always remember to completely reset your
        transforms at the beginning of the render routine, otherwise you'll
        apply transformations that are left over from the previous frame. For
        example, calling  alone simply multiplies
        the current matrix, so you might need to issue a
         immediately before calling
        .Let's go ahead and modify the
       method to render buttons; replace the ellipses
      in  with the
      code in .Note that  contains quite a few transform operations;
      while this is fine for teaching purposes, in a production environment
      I'd recommend including all four buttons in a single VBO. You'd still
      need four separate draw calls however, since the currently pressed
      button has a unique alpha value.In fact, making this optimization would be an
      interesting project: create a single VBO that contains all four
      pre-transformed buttons, then render it with four separate draw calls.
      Here's a hint: don't forget that the second argument to
       can be non-zero!The  method
      sets alpha to one if the button is being pressed, otherwise it makes the
      button semi-transparent:Next let's go over the code in
       that detects button presses and maintains
      the azimuth / altitude angles. See  for the  class
      declaration and  for the interesting potions of the class
      implementation.For now, we're hard-coding both button
          visibility flags to true. We'll make this dynamic after adding
          compass and accelerometer support.The theta and phi angles are updated
          according to the current velocity vector and delta-time.Right before passing in the button mask
          to the  method, take a look at the velocity
          vector to decide which buttons are being pressed.Simple utility function to detect if a
          given point () is within the bounds of a
          button centered at (, ).
          Note that we're allowing the intrusion of a vanilla C function into
          an Objective C file.To make things simple, the
           vector is set up in response to a
          "finger down" event, and reset to zero in response to a "finger up"
          event. Since we don't need the ability for several buttons to be
          pressed simultaneously, this is good enough.At this point you now have a complete app
      that lets you look around inside a (rather boring) virtual world, but
      it's still a far cry from augmented reality!The next step is carefully integrating
      support for the compass and accelerometer APIs. I say "carefully"
      because we'd like to provide a graceful run-time fallback if the device
      (or simulator) does not have a magnetometer or accelerometer.We'll be using the accelerometer to obtain the gravity vector,
      which in turn enables us to compute the phi angle (that's "altitude" for
      you astronomers) but not the theta angle (azimuth). Conversely, the
      compass API can be used to compute theta but not phi. You'll see how
      this works in the following sections.Using the low-level accelerometer API
        directly is ill advised; the signal includes quite a bit of noise, and
        unless your app is somehow related to , you probably don't want your camera shaking
        around like a shivering chihuahua.Discussing a robust and adaptive low-pass
        filter implementation is beyond the scope of this book, but thankfully
        Apple includes some example code for this. Search for the
         sample on the iPhone developer
        site ()
        and download it. Look inside for two key files and copy them to your
        project folder:  and
        .You can also refer to  for an example implementation of a
          simple low-pass filter.After adding the filter code to your Xcode
        project, open up  and add the three code
        snippets that are highlighted in bold in .Next, open up
         and add the lines shown in bold in
        . You
        might grimace at the sight of the  block, but
        it's a necessary evil because the iPhone simulator pretends to support
        the accelerometer APIs by sending the application fictitious values
        (without giving the user much control over those values). Since the
        fake accelerometer won't do us much good, we turn it off when building
        for the simulator.An Egyptian software company called "vimov" produces a
          compelling tool called  that can
          simulate the accelerometer and other device sensors. Check it out at
          .Since  sets itself
        as the accelerometer delegate, it needs to implement a response
        handler. See .You might not be familiar with the
         function, which takes the arc-tangent of the
        its first argument divided by the its second argument (see ). Why not use the
        plain old single-argument  function, and do the
        division yourself? Because  is smarter; it
        uses the signs of its arguments to determine which quadrant the angle
        is in. Plus it allows the second argument to be zero without throwing
        a divide-by-zero exception.An even more rarely encountered math
          function is . When used together,
           and  can convert
          any two-dimensional Cartesian coordinate into a polar
          coordinate. shows how we compute phi from the
        accelerometer's input values. To understand it, you first need to
        realize that we're using the accelerometer as a way of measuring the
        direction of gravity. It's a common misconception that the
        accelerometer measures speed, but you know better by now! The
        accelerometer API returns a 3D acceleration vector according to the
        axes depicted in .When you hold the device in landscape mode,
        there's no gravity along the Y axis (assuming you're not slothfully
        laying on the sofa and turned to one side). So, the gravity vector is
        composed of X and Z only — see .The direction of gravity can't tell you
        which direction you're facing; that's where the compass support in
        third-generation devices comes in. To begin, open
         and add the bold lines in .The Core Location API is an umbrella for
        both GPS and compass functionality, but we'll only be using the
        compass functionality in our demo. Next we need to create an instance
        of  somewhere in
        ; see .Similar to how it handles the accelerometer
        feedback,  sets itself as the compass
        delegate, so it needs to implement a response handler. See . Unlike the
        accelerometer, any noise in the compass reading is already eliminated,
        so there's no need for handling the low-pass filter yourself. The
        compass API is embarrassingly simple; it simply returns an angle in
        degrees, where 0 is North, 90 is East, and so on. See  for the
        compass response handler.The only decision you have to make when
        writing a compass handler is whether to use
         or .
        The former returns magnetic north, which isn't quite the same as
        geographic north. To determine the true direction of the geographic
        north pole, the device needs to know where it's located on the planet,
        which requires usage of the GPS. Since our app is looking around a
        virtual world, it doesn't matter which heading to use. I chose to use
         because it allows us to avoid
        enabling GPS updates in the location manager object. This simplifies
        the code, and may even improve power consumption.To make this a true augmented reality app, we
      need to bring the camera into play. If a camera isn't available (as in
      the simulator), then the app can simply fall back to the "scrolling
      clouds" background.The first step is adding another protocol to
      the  class — actually we need
       new protocols! Add the bold lines in , noting the
      new data fields as well ( and
      ).Next we need to enhance the
       and 
      methods. See . Until now, every sample in this book has
      set the  property in the EAGL layer to
      . In this sample, we decide its value at run time;
      if a camera is available, don't make the surface opaque to allow the
      image "underlay" to show through.Next we need to implement the
       method that was called from
      . This is an example of "lazy instantiation";
      we don't create the camera controller until we actually need it. The
      method is shown in , and a detailed explanation follows the
      listing. (The  method needs to
      be defined before the  method to avoid a
      compiler warning.)Set the image picker's delegate to the
           class. Since we aren't using the camera to
          capture still images, this isn't strictly necessary, but still a
          good practice.Hide the navigation bar. Again, we aren't
          using the camera for image capture, so there's no need for this UI
          getting in the way.Ditto with the toolbar.Set the source type of the image picker
          to the camera. You might recall this step from the camera texture
          sample in the previous chapter.Hide the camera control UI. Again, we're
          using the camera only as a backdrop, so any UI would just get in the
          way.Set the camera overlay view to the
           class to allow the OpenGL content to be
          rendered.The UI that we're hiding would normally
          leave an annoying gap on the bottom of the screen. By applying a
          scale transform, we can fill in the gap. Maintaining the correct
          aspect ratio causes a portion of the image to be cropped, but it's
          not noticeable in the final app.Finally, present the view controller to
          make the camera image show up.Since we're using the camera API in a way
      that's quite different from how Apple intended, we had to jump through a
      few hoops: hiding the UI, stretching the image, and implementing a
      protocol that never really gets used. This may seem a bit hacky, but
      hopefully Apple will improve the camera API in the future to simplify
      develpment of augmented reality applications.You may've noticed back in  that the
      view class is now passing in a boolean to the rendering engine's
       method; this tells it whether the
      background should contain clouds as before, or if it should be cleared
      to allow the camera underlay to show through. You must modify the
      declaration of  in
       accordingly. Next, the only
      remaining changes are shown in .Note that the alpha value of the clear color
      is zero; this allows the underlay to show through. Also note that the
      color buffer is cleared only if there's no sky sphere. Experienced
      OpenGL programmers make little optimizations like this as a matter of
      habit.That's it for the Holodeck sample! See  for a depiction
      of the app as it now stands.In this chapter we learned how to put FBOs to
    good use for the first time. We learned how to achieve anti-aliasing in
    sneaky ways, how to layer a scene by mixing 2D content with 3D content,
    and how to use the iPhone's orientation sensors in tandem with
    OpenGL.We explored the concept of a two-dimensional
    HUD in the Holodeck sample, but we largely glossed over the subject of
    text. Supplying ready-made textures of complete words (as we did for
    Holodeck) can be a bit cumbersome; often an application needs to render
    large amounts of dynamic text together with a 3D scene. Since text is
    something that OpenGL can't really handle on its own (and justifiably so),
    it deserves more attention. This brings us to the next chapter.Even though OpenGL ES is designed for 3D
  graphics, you'll often find the need to render visual elements that are
  two-dimensional. OpenGL is actually quite well-suited to rendering a flat
  world; several popular 2D libraries, such as ,
  use OpenGL as their rendering engine.The most common type of 2D rendering is text
  rendering. The OpenGL API lives too close to the metal to treat text as a
  first-class citizen, but it's easy to render a pre-generated
   (a character's shape in a given font) using a
  textured quad, and that's the approach we'll take in this chapter.Computing the points along the outline of a
    glyph can be surprisingly complex. For example, the TrueType file format
    specifies a unique programming language — complete with loops and if
    statements — solely for the purpose of tweaking the curves in a
    glyph.In this chapter, we won't attempt to go over
  kerning algorithms, ligatures, or line wrapping; simple text layout is good
  enough for our purposes. (Check out the popular 
  library if you need a full-featured layout engine.)Another common two-dimensional concept is the
  , which is a rather generic term for any bitmap
  that gets composited into a scene. Sprites often contain transparent
  regions, so their texture format contains alpha. Sprites are often animated
  in some way. There are two ways of animating a sprite: its screen position
  can change (imagine a bouncing ball), or its source image can change
  (imagine a ball that's spinning in place).The iPhone supports two extensions to OpenGL ES
  1.1 that make it easy to render sprites:
   and
  . We'll make good use of both these
  extensions throughout the chapter, and we'll wrap up the chapter with a fun
  sample application that renders a spring system with sprites.Rather than demonstrating text rendering with
    yet another goofy toy application, let's do something useful for a change.
    Overlaying a frames-per-second counter in one corner of the iPhone screen
    provides a quick and easy way of evaluating graphics performance; see
    .For more sophisticated run-time analysis of
      graphics performance, Apple provides an excellent free tool called
      , which we'll cover in a subsequent
      chapter.Before writing any application code, you need
    to generate an image that contains bitmaps for the numbers zero through
    nine, as depicted in .You probably already guessed that you need to
    store off the bounding box of each glyph in order to compute the
    appropriate texture coordinates. Thinking about this a bit more, a mere
    bounding box is not enough. When you're writing a sentence on ruled paper,
    some parts of letters extend below the baseline, like the descender of the
    lowercase "p". And, in the case of the rather artsy font shown in , the type
    designer wants the "9" numeral to be vertically offset from the other
    letters. Further complicating matters is the fact that the bounding boxes
    of glyphs can overlap in the destination image. In , observe how
    the descender of the letter "p" extends right below the letter "i".It turns out that associating a specific set of
     with each character supplies enough
    information to achieve the simple text layout shown in . A popular
    naming convention for these metrics is described in ; in this diagram,
    the origin represents the current pen position.To summarize, the four glyph metrics
    are:Two-dimensional vector describing the
          offset from the pen position.Two-dimensional vector describing how to
          advance the pen to the next position after rendering the current
          glyph. The y component is always zero for Latin-based
          alphabets.The horizontal length of the
          glyph.The vertical length of the glyph.Using these metrics, the pseudocode for a
    simple text layout algorithm is shown in .Before writing any application code, we need
      to choose a way of generating a glyphs texture and a set of metrics for
      each glyph.Leveraging Quartz is perhaps the most obvious
      way of generating a glyphs texture (see ). This can be done at run-time
      when your application first starts up. This might slow down your startup
      time by a tiny amount, but has the benefit of shrinking the size of the
      application bundle.My personal preference is to generate the
      glyphs texture as a build step, mostly because it simplifies my
      application code. Build steps take place in the Mac OS X environment
      rather than the iPhone execution environment, which brings a much richer
      toolset to the table. This is a perfect use case for a scripting
      language, and Python comes to mind first.There are  ways of
        generating a glyphs texture; here I'm giving an overview of my
        personal favorite. Take it only as a high-level example.Given that we're using Python in a build
      step, we need to find some useful Python modules for image manipulation
      and image generation. At the time of this writing,
       (the ython
      maging ibrary) is the most popular imaging manipulation
      module, and it provides excellent support for manipulating PNG images at
      a low level. However, it's not quite enough on its own because it
      doesn't provide direct access to the glyph metrics that we need. Another
      popular library is , which has a
      well-maintained Python binding called .
      Cairo is robust and fast, and it's used as the rendering backend in
      Firefox and Mono. It's also well integrated into the GTK toolkit (which,
      incidentally, is a good source to download it from). So, let's go with
      PIL and pycairo.Another possibility is
        , which is layer on top of PIL. pyglet is
        used as the basis for , which we'll
        discuss briefly in a subsequent chapter. I chose not to use pyglet
        here because it's somewhat better suited for run-time logic than for
        offline image generation.Rather than packaging the glyphs texture as a
      PNG or PVR file, let's serialize the data to a C header file. Since it's
      a single-channel texture, the amount of data is relatively small. The
      header file will also provide a convenient place to store the glyph
      metrics. We'll simply have our Python script spawn
       () for
      generating the header file from the image. We'll still generate a PNG
      file for preview purposes, but we won't include it in the application
      bundle. See 
      for the complete Python script that generates .Cairo is a fairly extensive library and is
      beyond the scope of this book, but here's a brief explanation of :Create a 256x32 image surface with Cairo,
          then create a context associated with the surface.Select a TrueType font file, then choose
          its size. In this case, I'm selecting the  font found in
          .Set Cairo's current draw color to
          white.Initialize a string that we'll later
          append to the header file. For starters, define some structs for the
          glyphs table.Initialize the pen position to (0,
          0).Iterate over glyphs '0' through
          '9'.Obtain the metrics for the glyph.Populate the GlyphPosition structure that
          we're defining in the generated header file.Populate the GlyphMetrics structure that
          we're defining in the generated header file.Tell Cairo to fill in the glyph
          shape.Advance the pen position with some
          padding.Save the Cairo surface to a PNG
          file.Load the PNG image into PIL.Use PIL to extract only the alpha
          channel, then overwrite PNG file.Use PVRTexTool to serialize the image
          data to a C header file. (At this point, the PNG is no longer
          needed.)Append the metrics data to the same
          header file that defines the image data.Now that we're past the grunt work of
      generating the glyphs texture, we can move on to the actual rendering
      code. A frames-per-second counter is much more useful than our other toy
      demos, so this time let's strive to make the rendering code very
      self-contained and easy to integrate into any project. We can do this by
      creating a C++ class wholly implemented within a single header file. The
      basic outline for this class is shown in .Private method that generates the vertex
          and texture coordinates for one of the corners in a glyph
          rectangle.Smoothing factor for the low-pass filter;
          this is explained further in the next section.Exponentially-weighted moving average of
          the frame rate (again, this is explained in the next
          section).Time stamp in nanoseconds of the most
          recent call to .Width and height of the viewport (usually
          320x480).Width and height of the glyphs
          texture.The OpenGL id of the glyphs texture
          object.The OpenGL id of the vertex buffer object
          used for rendering the glyphs.To prevent the FPS counter from fluctuating
        wildly, we'll using a low-pass filter similar to the one we used for
        the accelerometer (see ). The
        application can compute a constant called the , which is always between zero and one. Here's one
        way of doing so:In the above listing,
         and 
        help to define what constitutes "noise" in the signal. However, for
        our purposes, computing a smoothing a factor like this is a bit
        pedantic; pragmatically speaking, it's perfectly fine to come up with
        a reasonable number through experimentation. I find that a value of
        0.1 works well for a frame rate counter. A higher smoothing factor
        would result in a more spastic counter.Let's go ahead and implement the
        constructor of the  class; see . It's
        responsible for loading up the glyphs texture and creating the empty
        VBO for rendering up to three digits.The  class
        has only one public method; see . This method is responsible for updating
        the moving average and rendering the digits. Note that updating the
        VBO is quite a hassle; we'll demonstrate a much simpler way of
        rendering textured rectangles in the next section.Next we need to implement the private
         method, which generates the VBO
        data for a given corner of a glyph rectangle. It takes a
        pointer-to-float for input, advances it after writing out each value,
        then returns it back to the caller.That's it for the frame rate counter! It's
        pretty easy to use the class from within the rendering engine class;
        see .Recall that updating the VBO at every frame and
    computing texture coordinates was a bit of a pain; turns out the iPhone
    supports an easier way to render pixel rectangles when you're using OpenGL
    ES 1.1.This extension is not supported under OpenGL
      ES 2.0.The 
    extension (supported on all iPhone models at the time of this writing)
    adds two new functions to OpenGL's repertoire:These two functions are basically equivalent;
    either can be used to render a rectangle. The second function takes a
    pointer to the same five floats described in the first function.The 
      extension actually introduces eight functions in all, due to the
      variants for , , and
      . I tend to use the 
      variants.This extension also introduces a new texture
    parameter called , which can be
    used liked this:When used together, the new
     functions and the new texture parameter make
    it easy to draw rectangles of pixels; no need to mess with cumbersome VBOs
    and triangles.To summarize, use 
    to set the destination rectangle on the screen; use the new crop rectangle
    parameter to set up the rectangle in the source texture.Let's walk through the process of converting
    the  sample to use the
     extension. First we can remove several
    fields from the class. ,
    , and all constants except
    .We can also replace the cumbersome
     method with a new streamlined method
    called . See . For
    brevity, sections of code that remain unchanged are replaced with
    ellipses.Chris Green of Valve Software wrote a very cool
    graphics paper a couple years ago that I feel deserves more attention. It
    describes a simple way to preserve high quality edges in vector art
    (typically text) when storing the art in a relatively low-resolution
    texture. If you'd like to minimize distraction while coming up to speed
    with OpenGL, go ahead and skip this section; distance fields are a
    somewhat advanced concept, and they are not required in simple
    applications. However I personally find them fascinating!Let's review the standard way of rendering text
    in OpenGL. Normally you'd store the glyphs in a texture whose format is
    , and you'd set up a fairly standard blending
    configuration, which would probably look like this:If you're using ES 1.1, you can then set the
    color of the text using . With ES 2.0, you can
    store the color in a uniform variable, and apply it in your fragment
    shader.That's a perfectly reasonable approach, but if
    you zoom in far enough on your texture, you'll see some fuzzy
    stair-stepping as seen in the leftmost panel in . The fuzziness can be
    alleviated by replacing blending with alpha testing (), but the stair-stepping remains; see the middle
    panel.]You'll almost always see stair stepping when
    zooming in with bilinear filtering. Third-order texture filtering (also
    known as ) would mitigate this, but
    it's not easy to implement with OpenGL ES.Turns out there's a way to use bilinear
    filtering and achieve higher-quality results. The trick is to generate a
     for your glyphs. A distance
    field is a grid of values, where each value represents the shortest
    distance from that grid cell to the glyph boundary. Cells that lie inside
    the glyph have negative values; cells that lie outside have positive
    values. If a grid cell lies exactly on the boundary of the glyph, it has a
    distance value of zero.To represent a distance field in an OpenGL
    texture, we need a way to map from the signed distance values to
    grayscale. One approach is to represent a distance of zero as half-black
    (0.5), then to choose maximum and minimum distances, which get mapped to
    1.0 and 0. (This effectively clamps large distances, which is fine.)  shows a distance
    field for the mystical Aum symbol.  zooms in on a portion of the Aum distance
    field with the original glyph boundary represented as a black line.The concept of a distance field may seem
    obscure, but it's useful in many surprising ways. Not only does it provide
    a way to preserve quality of edges (with both ES 1.1 and ES 2.0), it also
    makes it easy to apply a bevy of text effects, such as shadows and
    outlines (these are ES 2.0 only).Before diving in to the application of
      distance fields, let's take a look at how to generate them. The most
      popular way of doing this is actually quite simple to implement, despite
      having a ridiculously complex name: "the eight-points signed sequential
      Euclidean distance transform algorithm", or 8SSEDT for short. The basic
      idea is to store a pair of integers at each grid cell
      ( and ) which represent the
      number of cells between it and the nearest cell on the opposite side of
      the vector boundary. Each cell is initialized to either (0, 0) or (+∞,
      +∞), depending on whether the cell is inside the vector. The algorithm
      itself consists of "propagating" the distances by having each cell
      compare its dx:dy pair to its neighbor, then adding it to the current
      cell if it's closer. To achieve a signed distance, the algorithm is run
      on two separate grids, then merges the results.Let's momentarily go back to using Python and
      the PIL library since they provide a convenient environment for
      implementing the algorithm; see .Don't let  scare you! You're in good shape if
      you simply grok the concept of a distance field. The formal proof of the
      generation algorithm is beyond the scope of this book, but you can
      always flip back to  to
      review it at a high level.To make use of a distance field with iPhone
      models that support only OpenGL ES 1.1, simply bind the distance field
      texture and enable alpha testing with a threshold value of 0.5:Remember, blending applies an equation at
      every pixel to determine the final color, while alpha testing compares
      the source alpha with a given value and leaves the framebuffer unchanged
      if the comparison fails.One of the reasons I love distance fields is
      that they enable more than quality enhancements. On iPhone models that
      support OpenGL ES 2.0, distance fields can be used in conjunction with a
      fragment shader to achieve a variety of special effects, all using the
      same source bitmap. See .The first distance field effect that I want
      to cover is smoothing, as seen in the left-most panel in .Go back and take another look at the big
      stair steps in the left-most panel in ; they correspond to the texels in the source
      image. Alpha testing with a distance field fixed this up (right-most
      panel), but it still exhibits pixel-level aliasing. This is because the
      rasterized pixels are always either fully lit or discarded; no shades of
      gray. We can fix this up with a fragment shader.Before diving into the shader code, let's
      take a look at GLSL's  function. Here's the
      declaration: returns 0.0 if
       is less than or equal to 
      and returns 1.0 if  is greater than or equal to
      . If  is between these two
      values, then it interpolates between 0 and 1. Calling
       is equivalent to:To see how 
      comes in handy for smoothing, visualize two new boundary lines in the
      distance field: one at  (a deflated version of
      the glyph), the other at  (an inflated version
      of the glyph). See ; the middle line is the region where
      distance = 0.Alpha should be opaque at
       and transparent at . To
      achieve smoothing, the fragment shader needs to create an alpha ramp
      between these two boundaries. An implementation is shown in .The fragment shader in  is fairly
      easy to understand, but unfortunately it suffers from a fundamental
      flaw. The value of  is always the same,
      regardless of how much the glyph is magnified. As a result, antialiasing
      is too blurry when the camera is near the texture (), and it's
      ineffective when the camera is very far away.Fortunately, the iPhone supports a fragment
      shader extension to help out with this. Unfortunately, it's not
      supported in the simulator at the time of this writing.It's easy to deal with this disparity. At
        run time, check for extension support using the method described in
        . Compile fragment shader
        A if it's supported, otherwise compile fragment shader B.The name of this extension is
      . That's right,
      "derivatives". Don't run in fear if this conjures up images of a brutal
      Calculus professor! It's not as bad as it sounds. The extension simply
      adds three new functions to GLSL:These functions are only available to the
      fragment shader. They return a value proportional to the rate of change
      of the argument when compared to neighboring pixels. The
       function returns a rate of change along the
      X-axis; the  function returns a rate of change
      along the Y-axis. The  function provides a
      convenient way of combining the two values:In our case, when the camera is far away, the
      rate of change in the on-screen distance field is greater than when the
      camera is close up. To achieve consistent anti-aliasing, we'll simply
      use a larger filter width when the camera is far away. See  for a new
      version of the fragment shader that uses derivatives.Using shaders with distance fields can also
      achieve a variety of special effects, as seen in . In the interest
      of brevity, I won't go into too much detail here; much like the
      smoothing example from the previous section, all these effects rely on
      using  and various offsets from the
       boundary. They also make use of a GLSL
      function called ; here's its declaration:You probable already guessed that this
      function performs linear interpolation between its first two
      arguments:See  for an "ubershader" that can produce any of
      the aforementioned distance field effects, depending on how the
      application sets up the uniforms. Remember, if you're trying to run this
      shader on the simulator, you may need to remove the top line, and
      replace the  function with a constant.The ,
          , and  booleans are
          set from the application to choose which effect to apply. (An
          alternative strategy would be splitting this into three separate
          shaders, which is a perfectly acceptable approach.)The offset of the shadow from the glyph.
          In this case, the aspect ratio of the texture is 2:1, so the X
          offset is half the size the Y offset. Note that you may need to
          negate the X or Y value, depending on how your distance field is
          oriented.The R G B values of the shadow
          color. is the
          alpha value which represents the 
          boundary. tells
          the shader how far from the glyph edge to render the outline. For an
          outline that is just inside the glyph, this value should be less
          than 0.5. tells the
          shader how far out to extend the glow. To create a pulsing glow,
          change this into a uniform and cycle its value from within the
          application code.The shadow effect in  deserves
      further explanation. It applies anti-aliasing not only to the transition
      between the vector and the background, but also between the shadow and
      the background, and between the vector and the shadow. The shader pulls
      this off by deciding which of the following five regions the pixel falls
      into: (see )Completely within the vector.On a vector edge that's not
          shadowed.Completely within the shadow.On a vector edge that's shadowed.On the shadow's outside edge.Let's set aside glyph rendering and visit
    another topic common to 2D graphics: sprites. The iPhone is an ideal
    platform for casual gaming, and many popular iPhone games rely heavily on
    sprites for frame composition. To recap, a sprite is simply a bitmap that
    gets applied to a rectangular region of the screen. Sprites often use
    alpha to allow the background (and possibly other sprites) to show
    through. I like to think of sprite rendering as using an overhead
    projector, where each sprite is a plastic sheet with a cartoon
    drawing.For efficiency, it's common to pack a slew of
    sprites into a single texture; this is called a . In general, a texture that contains multiple disparate
    images is known as a . The numerals
    texture presented in  was an example of a
    texture atlas.There are tools out there to help you build
      sprite sheets. One such tool is a web-based application called zwopple
      by Robert Payne. It can be found at .Recall that there are two ways of animating a
    sprite: the screen position can change (e.g., bouncing ball), or the
    source image can change (e.g., spinning ball). In the former case, the
    application code updates the vertex positions at every frame; in the
    latter case, the application updates the texture coordinates at every
    frame.For an example of a sprite with multiple
    animation frames, see , a sprite sheet from a game that I created in
    my college days. (The game's protagonist is named "Noop", a blobby fellow
    who moves horizontally by repeatedly squishing his legs together in
    worm-like fashion.)Sometimes it's desirable to split a sprite
    sheet into multiple layers, as seen in . The left sheet has the animation frames for
    Noop's body, the right sheet has his eyes and shiny highlights. This
    allows the application to vary the colors of the layers independently. For
    example, my game can draw Noop using a yellowish hue most of the time, but
    sometimes render him in orange to convey that he's hurt. In both cases,
    the eyes and highlights are white.We discussed how to shift the apparent color of
    a texture in  . You can use a luminance or
    luminance-alpha texture rather than a full-blown RGBA texture, then
    modulate the texture's color using per-vertex color (e.g., by calling
    ).The obvious way of composing Noop's eyes with
    his body is to render the same quad in two passes with blending enabled.
    The first pass uses texture coordinates for the body; the second pass uses
    coordinates for the eyes and highlights. An example of this procedure is
    shown in .Note that  is only valid for ES 1.1; under ES 2.0, we
    need to replace the DrawTex-related lines with calls to
     or , and
    we need to replace  with
    . See .Both OpenGL ES 1.1 and ES 2.0 provide a way to
    combine simple two-pass operations like this into a single draw call. It's
    called . Multitexturing allows you to
    set up more than one . Sample code for
    rendering Noop with multitexturing is shown in ; note there's only one call to
    .The key lines in  are the calls to
    , which sets the current texture stage
    and affects all subsequent texture-related calls, including
    . This allows individual stages
    to be independently turned on or off.I should warn you that  alone is not quite
    enough; you also need to tell OpenGL how to combine the color values from
    the two texture stages. With ES 1.1, this is quite a hassle; see . This sets up the
    second texture stage so that it works in a way similar to typical alpha
    blending. Thankfully you can often perform this type of configuration only
    once, when your application first starts up.OpenGL ES 2.0 simplifies this by allowing you
    to combine colors from within your fragment shader. We'll discuss this
    further in the next chapter, and we'll explain 
    in greater detail — I just wanted to whet your appetite!Sprites are often used for rendering interactive widgets in a HUD
    (see ). Handling mouse interaction can be
    a chore when you don't have a UI framework to stand upon. If you're
    developing a 3D application and find yourself yearning for UIKit, don't
    dismiss it right away. It's true that mixing UIKit and OpenGL ES is
    generally inadvised for performance reasons, but in many situations, it's
    the right way to go. This is especially true with 3D applications that
    aren't as graphically demanding as huge, professionally-produced games.
     depicts an application that overlays a
     widget with a 3D scene.The performance of "mixed" rendering has been improving as Apple
      rolls out new devices and new revisions to the iPhone OS. By the time
      you read this, using non-animated UIKit controls in conjunction with
      OpenGL might be a perfectly acceptable practice.Recall that all OpenGL rendering takes place in a UIView-derived
    class; every sample in this book defines a class called GLView for this
    purpose. Adding a few simple controls to GLView is fairly painless, so
    let's try adding a  that selects a
    texture minification filter.First we need to add a field to the GLView class declaration for the
    new control. See the bold line in .Next, we need to instance the control and create a method for event
    handling; see . includes some UIKit and
    Objective C mechanisms that we haven't seen before (such as
    ), but you should be able to tell what's going
    on. Check out Jonathan Zdziarski's  to learn more about UIKit.The full source code for the application depicted in  is available from the O'Reilly site under the name
    . You might also want to check out the
     sample from Apple's SDK, which
    includes a slick sliding animation for a panel of UIKit controls.Note that you can also use UIKit to render "look-a-like" controls,
    rather than using the actual UIKit controls. For example, you can render
    some buttons into a  at launch time, then create
    an OpenGL texture from that (see ). This would give your buttons
    the look and feel of the iPhone's native UI, plus it wouldn't suffer from
    the potential performance issues inherent in mixing the actual UIKit
    control with OpenGL. The downside is that you'd need to implement the
    interactivity by hand.You may find yourself wanting to render a
    system of particles that need a bit more pizazz than mere single-pixel
    points of light. The first thing that might come to mind is rendering a
    small alpha-blended quad for each particle. This is a perfectly reasonable
    approach, but it requires you to come up with the coordinates for two
    textured triangles at each point.Turns out the iPhone supports an extension to
    make this much easier by enabling .
    Point sprites are small screen-aligned quads that get drawn at each vertex
    in a vertex array or VBO. For simplicity, a point sprite uses an entire
    texture; there's no need to provide texture coordinates. This makes it a
    breeze to render particle systems such as the one depicted in .For OpenGL ES 1.1, the name of the extension is
    , and it allows you to make the
    following function calls:With OpenGL ES 2.0, point sprites are supported
    in the core specification rather than an extension. There's no need to
    call any of these functions because point sprite functionality is
    implicit. You'll see how to use point sprites in both ES 1.1 and ES 2.0 in
    the upcoming "Springy Stars" sample.To show off point sprites in action, let's wrap
    up the chapter with a mass-spring simulation that renders a network of
    little white stars, as seen in . The "star net" droops down in the direction
    of gravity according to how you're holding the iPhone.Before we dive into the code, let's take a
      brief detour from graphics and review some physics. The easiest way to
      create a simple physics simulation to call an
       method on every rigid body in the
      simulation. The update method has an argument for the , which represents elapsed time since the previous call.
      (This isn't much different from the 
      method presented way back in Chapter 1.) For our SpringyStars app, the
      pseudocode for the update method looks like this:The above code snippet should make sense if
      you remember your high school physics. Perhaps a bit more foggy in your
      memory is , which we'll need for
      modelling the spring forces between the star sprites; see . is the
      "restoring" force of the spring, so called because it represents the
      effort to restore the spring's length to its rest position. You can
      think of the  constant as being the "stiffness" of
      the spring.  is the displacement
      between the current end of the spring and its rest position.To learn physics from a much more
        authoritative source, take a look at  by David Bourg.Hooke's Law deals with only a single spring,
      but in our case we have a network of springs. Our simulation needs to
      maintain a list of "nodes" (rigid bodies), each of which is connected to
      a set of neighboring nodes. Here's a code snippet that applies Hooke's
      Law to a node and its neighbor:In the above snippet, the
       vector applies to the
       node, while an equal and opposite force (i.e.,
      ) applies to the
       node.Taken alone, Hooke's Law can produce
      oscillations that last forever. To be more realistic, the simulation
      needs to include a  to subdue the
      spring's effect. The damping force between two nodes is proportional to
      their relative velocity:In this case,
       is a damping constant.
      Much like the stiffness constant in Hooke's Law, I find that the best
      way to come up with a reasonable value is through experimentation. (More
      rigorous mass-spring systems include a global damping force, but this is
      good enough for our purposes.)The C++ code snippet for computing damping
      force looks like this:At this point we're ready to design a C++
      class to represent a star sprite in a simple mass-spring system. See
      .Note the boolean field called
      , which causes a node to be impervious to the
      forces acted upon it. We'll use this to affix the four corners of the
      net to their starting positions. This prevents the net from falling off
      the screen.Speaking of falling off the screen, note that
      there's nothing obvious in  that takes gravity into account. That's
      because the application can use the 
      method to initialize the  field to the gravity
      vector, then call  in a separate
      pass to add in all the relevant spring forces. The simulation will
      perform three separate passes through the node list; see the pseudocode
      in . (Don't combine these into a
      single loop.)To avoid code duplication between the ES 2.0
      and ES 1.1 backends, let's put the physics into the application engine
      and pass it a normalized 2D vector for the direction of gravity. As a
      result, the  interface is very
      simple; see .The  class looks
        much like all the other samples in this book, except that it needs to
        pass in a gravity vector. See . For more information about setting up the
        accelerometer, flip back to .See 
      for the application engine implementation. Recall that the
       class calls 
      according to the refresh rate of the display. This provides enough time
      to perform several simulation iterations, each using a small time step.
      Performing several small iterations produces more accurate results than
      a single iteration with a large time step. In fact, an overly large time
      step can cause the simulation to go ballistic.Updating the physics along with the
        rendering is a bit of a hack, but good enough for our purposes. For a
        production-quality application, you might want to create a timer
        object in your  class just for
        physics.The 
          field stores the normalized direction provided by the
           layer.The 
          vector stores the rigid bodies in the simulation.The  vector
          provides a contiguous list of node positions to the rendering
          engine.The ,
          , and 
          constants determine the initial layout of the star sprites.Center the grid of stars at (0,
          0).Add a connection to the node to the
          left.Add a connection to the above
          node.Pin the four corners of the net so that
          they don't move.Call 
          once at start-up to initialize the position list.The gravity direction vector is
          normalized, so it needs to be scaled by the
           constant before passing it in as
          a force vector.As mentioned earlier, we make several
          passes through the simulation loop for increased precision.The contents of this loop corresponds to
          pseudocode in .Copy the node position into the vertex
          array that gets passed to the rendering engine.One difficulty you might come across with
      point sprites is the order-dependency problem imposed by some blending
      equations (flip back to  to review
      this issue). One way to avoid this is to use additive blending. Here's
      how to set it up:This sets up the following equation:You can see how this differs form traditional
      blending because it can only make the framebuffer color brighter and
      brighter as more sprites are rendered. This produces an effect that may
      be desirable anyway; for example, if you're rendering fireworks with a
      dense cloud of point sprites, additive blending helps to vary the
      brightness and make the scene more interesting.Recall that the
       interface has only two methods; the
      ES 1.1 implementations of these are shown in . The remainder of the file is much the same
      as other samples in this book. For the full source, download the code
      from O'Reilly's web site.The only new OpenGL function in  is . This
      sets the width (and height) of the point sprites. OpenGL uses the
      current model-view matrix to determine the distance of each point sprite
      from the camera, and shrinks the size of distant point sprites. This
      effect can be abrogated like this:This seems rather obscure, but it has to do
      with how OpenGL computes the point size:In the above formula,
       is what you pass to
      ,  is the distance
      from the camera, and  is the array of values passed
      to . (I've simplified this a bit by
      leaving out some clamping that can occur.) In my opinion, the API
      designers made this a bit too complex!You can even vary the size of the point
        sprites on a per-vertex basis through the use of the
         extension, which is supported
        on all iPhone models.OpenGL ES 2.0 handles point size quite
      differently from ES 1.1, which brings us to the next section.Before going over the C++ code for the ES 2.0
      rendering engine, let's take a look at the shaders. See  and .By now you've probably noticed that all
      built-in variables can be recognized by their 
      prefix. There are two new built-in variables introduced in the above
      listings:  (written to by the vertex
      shader), and  (fed into the fragment
      shader).OpenGL ES 2.0 requires you to set up the
      point size from the vertex shader rather than the application code,
      which gives you the option to compute it dynamically; if you want, you
      can evaluate the same distance formula that ES 1.1 does. Or, you can do
      something much simpler, like what we're doing here.The  variable
      gives you the auto-computed texture coordinate that varies across the
      point sprite. That's why ES 2.0 doesn't require you to call
       with ;
      it's implicit in your fragment shader.This chapter has shown that OpenGL is quite
    adept at two-dimensional rendering, even though most developers primarily
    think of it as a 3D graphics API.If you've made it this far in the book, you're
    more than ready to make an entry into the world of 3D iPhone programming;
    subsequent chapters will cover more advanced material. Recall that we gave
    a brief taste of multitexturing and the 
    function in this chapter — we'll go over these concepts in further detail
    in the next chapter, along with a bevy of other advanced effects.At this point in the book you may have written a
  couple simple OpenGL demos to impress your co-workers and family members.
  But, your app may need that extra little something to stand out from the
  crowd. This chapter goes over a small selection of more advanced techniques
  that can give your app an extra oomph.The selection of effects dealt with in this
  chapter is by no means comprehensive. I encourage you to check out other
  graphics books, blogs, and academic papers to learn additional ways of
  dazzling your users. For example, this book does not cover rendering
  shadows; there are too many techniques for rendering shadows (or crude
  approximations thereof), so I can't cover them while keeping this book
  concise. But, there's plenty of information out there, and now that you know
  the fundamentals, it won't be difficult to digest it.This chapter starts off by detailing some of the
  more obscure texturing functionality in OpenGL ES 1.1. In a way, some of
  these features, specifically , which
  allow textures to be combined in a variety of ways, are powerful enough to
  serve as a substitute for very simple fragment shaders.The chapter goes on to cover normal maps and DOT3
  lighting, useful for increasing the amount of perceived detail in your 3D
  models. Next we discuss a technique for adding reflective surfaces to your
  objects; it employs a special texture called a , supported only in ES 2.0. We'll then briefly cover
  anisotropic texturing, which can be used to increase the clarity of textured
  primitives in some cases. Finally, we'll go over an image processing
  technique that adds a soft glow to the scene called
  . The bloom effect may remind you of a camera
  technique used in cheesy 80s soap operas, and I claim no responsibility if
  it compels you to marry your nephew in order to secure financial assistance
  for your father's ex-lover.Multitexturing was briefly introduced in the
    previous chapter (), but there's a lot
    more to explain. See  for a
    high-level overview of the iPhone's texturing capabilities.This section doesn't have much in the way of
      example code; if you're not interested in the details of texture
      combination under OpenGL ES 1.1, skip to the next section ().A few disclaimers regarding . First, the diagram assumes that both
    texture stages are enabled; if stage 1 is disabled, the "previous color"
    gets passed on to become the "final color". Second, the diagram shows only
    two texture stages. This is accurate for first and second generation
    devices, but newer devices have eight texture units.In , the
    "primary" color comes from interpolation of per-vertex colos. Per-vertex
    colors are produced by lighting, or set directly from the application
    using  or
    .The two 
    are the post-filtered texel colors, sampled from a particular texture
    image.Each of the two  is configured to combine its various inputs and
    produce an output color. The default configuration is
    , which was briefly mentioned in ; this means that the output color results from a
    per-component multiply of the previous color with the lookup color.There's a whole slew of ways to configure each
    texture environment using the  function. In my
    opinion, this is the worst function in OpenGL, and I'm thankful that it
    doesn't exist in OpenGL ES 2.0. The expressiveness afforded by GLSL makes
     unnecessary. has the following
    prototypes:There are actually a couple more variants for
    fixed-point math, but I've omitted them since there's never any reason to
    used fixed-point on the iPhone. The first parameter,
    , is always set to
    , unless you're enabling point sprites as
    described in . The second parameter,
    , can be any of the following:Sets the mode of the current texture
          environment; see below for the legal values of
          .Sets the constant color. As you'll see
          later, this used only if the mode is  or
          .Sets up a configurable equation for the
          RGB component of color. Legal values of  are
          discussed later.Sets up a configurable equation for the
          alpha component of color. Legal values of 
          are discussed later.Optional scale on the RGB components
          which takes place after all other operations. Scale can be 1, 2, or
          4.Optional scale on the alpha component
          which takes place after all other operations. Scale can be 1, 2, or
          4.If  is
    , then 
    can be any of the following:Set the output color equal to the lookup
          color:This is the default mode; it simply does
          a per-component multiply of the lookup color with the previous
          color:Use the alpha value of the lookup color
          to overlay it with the previous color. Specifically:Invert the lookup color, then modulate it
          with the previous color, then add the result to a scaled lookup
          color:Per-component addition of the previous
          color with the lookup color:Generate the RGB outputs in the manner
          configured by , and generate the
          alpha output in the manner configured by
          .The two texture stages need not have the same
    mode. For example, the following snippet sets the first texture
    environment to  and the second environment to
    :If the mode is set to
      , you can set up two types of combiners:
      the  and the . The former sets up the output color's RGB
      components; the latter configures its alpha value.Each of the two combiners needs to be set up
      using at least five additional (!) calls to .
      One call chooses the arithmetic operation (addition, subtraction, etc.),
      while the other four calls set up the arguments to the operation. For
      example, here's how you can set up the RGB combiner of texture stage
      0:Setting the alpha combiner is done in the
      same way; just swap the RGB suffix with ALPHA, like this:The following is the list of arithmetic
      operations you can use for RGB combiners. (i.e., the legal values of
       when 
      is set to ):Note that  and
       produce a scalar rather than a vector.
      With , that scalar is duplicated into each
      of the three RGB channels of the output color, leaving alpha untouched.
      With , the resulting scalar is written
      out to all four color components. The dot product combiners may seem
      rather strange, but you'll see how they come in handy in the next
      section. actually
      has  arguments; as you'd expect, setting up
      the third argument works the same way as setting up the others; you use
       and
      .For alpha combiners
      (), the list of legal arithmetic
      operations is the same as RGB combiners, except that the two dot-product
      operations are not supported.The 
      arguments in the preceding code snippet can be any of the
      following:Use the lookup color.Use the constant color that's set with
            .Use the primary color (this is the
            color that comes from lighting or
            ).Use the output of the previous texture
            stage. For stage 0, this is equivalent to
            .For RGB combiners, the
       arguments can be any of the
      following:Pull the RGB components from the source
            color.Use the inverted RGB components from
            the source color.Pull the alpha component from the
            source color.Use the inverted alpha component from
            the source color.For alpha combiners, only the last two of the
      preceding list can be used.By now, you can see that combiners
      effectively allow you to set up a set of equations, which is something
      that's much easier to express with a shading language!All the different ways of calling
       are a bit confusing, so I think it's best to
      go over a specific example in detail; we'll do just that in the next
      section, with an in-depth explanation of the DOT3 texture combiner and
      its equivalent in GLSL.If you'd like to render an object with fine
    surface detail, but you don't want to use an incredibly dense mesh of
    triangles, there's a technique called 
    that fits the bill. It's also called , since it works by varying the surface normals to
    affect the lighting. You can use this technique to create more than mere
    bumps; grooves or other patterns can be etched into (or raised from) a
    surface. Remember, a good graphics programmer thinks like a politician and
    uses lies to his advantage! Normal mapping doesn't actually affect the
    geometry at all. This is apparent when you look along the silhouette of a
    normal-mapped object; it appears flat. See .You can achieve this effect with either OpenGL
    ES 2.0 or OpenGL ES 1.1, although bump mapping under 1.1 is much more
    limited.Either approach requires the use of a
    , which is a texture that contains normal
    vectors (XYZ components) rather than colors (RGB components). Since color
    components are, by definition, non-negative, a conversion needs to occur
    to represent a vector as a color:The above transformation simply changes the
    range of each component from [-1, +1] to [0, +1].Representing vectors as colors can sometimes
    cause problems due to relatively poor precision in the texture format. On
    some platforms, you can work around this with a high-precision texture
    format. At the time of this writing, the iPhone does not support
    high-precision formats, but I find that standard 8-bit precision is good
    enough in most scenarios.Another way to achieve bump mapping with
      shaders is to cast aside the normal map and opt for a procedural
      approach. This means doing some fancy math in your shader. While
      procedural bump mapping is fine for simple patterns, it precludes
      artist-generated content.The are a number of ways to generate a normal
    map. Often an artist will create a ,
    which is a grayscale image where intensity represents surface
    displacement. The height map is then fed into a tool that builds a terrain
    from which the surface normals can be extracted (conceptually
    speaking). (see
    ) is such a tool. If you invoke it
    from a terminal window, simply add  to the command
    line, and it generates a normal map. Other popular tools include Ryan
    Clark's  application and NVIDIA's
    , but neither of these are supported on
    Mac OS X at the of this writing.  is probably the most sought-after tool
    for normal map creation (and yes, it's Mac-friendly). For an example of a
    height map and its resulting normal map, see the left two panels in .An important factor to consider with normal
    maps is the "space" that they live in. Here's a brief recap from  concerning the early life of a vertex:Vertex positions are stored in a VBO (or
        vertex array) in .Objects are placed into the scene using a
        , which takes them into
        .The vertices are then transformed by a
        , which takes them into .For bog standard lighting (not bump mapped),
    normals are sent to OpenGL in object space. However, the normal maps that
    get generated by tools like crazybump are defined in  (also known as ). Tangent space is the two-dimensional universe that
    textures live in; if you were to somehow "unfold" your object and lay it
    flat on a table, you'd see what tangent space looks like.Another tidbit to remember from an earlier
    chapter is that OpenGL takes object space normals and transforms them into
    eye space using the inverse-transpose of the model-view matrix (). Here's the kicker: transformation of
    the normal vector can actually be skipped in certain circumstances. If
    your light source is infinitely distant, you can simply perform the
    lighting in object space! Sure, the lighting is a bit less realistic, but
    when has that stopped us?So, normal maps are (normally) defined in
    tangent space, but lighting is (normally) performed in eye space or object
    space. How do we handle this discrepancy? With OpenGL ES 2.0, we can
    revise the lighting shader so that it transforms the normals from tangent
    space to object space. With OpenGL ES 1.1, we'll need to transform the
    normal map itself, as depicted in the right-most panel in . More on this later; first I'll go over
    the shader-based approach since it can give you a better understanding of
    what's going on.Before writing any code, we need to figure
      out how the shader should go about transforming the normals from tangent
      space to object space. In general, we've been using matrices to make
      transformations like this. How can we come up with the right magical
      matrix?Any coordinate system can be defined with a
      set of . The set is often simply
      called a . The formal definition of basis
      involves phrases like "linearly independent spanning set", but I don't
      want you to run away in abject terror, so I'll just give an
      example.For three-dimensional space, we need three
      basis vectors: one for each axis. The  is the space that defines the Cartesian coordinate
      system that we all know and love:Basis Vector A: (1, 0, 0)Basis Vector B: (0, 1, 0)Basis Vector C: (0, 0, 1)The standard basis is
       because each vector has unit length,
      and they're all ninety degrees apart from each other. Turns out that
      there's an elegant way to transform a vector from any orthonormal basis
      to the standard basis. All you need to do is create a matrix by filling
      in each row with a basis vector:If you prefer column-vector notation, then
      the basis vectors form columns rather than rows:In any case, we now have the magic matrix for
      transforming normals! Incidentally, basis vectors can also be used to
      derive a matrix for general rotation about an arbitrary axis. Basis
      vectors are so foundational to linear algebra that mathematicians are
      undoubtedly scoffing at me for not covering them much earlier. I wanted
      to wait till a practical application cropped up — which brings us back
      to bump mapping.So, our bump-mapping shader will need three
      basis vectors to transform the normal map's values from tangent space to
      object space. Where can we get these three basis vectors? Recall for a
      moment the  class that was
      introduced early in the book. In , the
      following pseudocode was presented:Lookee there! The three vectors , , and
       are all perpendicular to each other —
      perfect for forming an orthonormal basis. The
       class already computes  for us, so all we need to do is amend it to
      write out one of the tangent vectors. Either  or  will work
      fine; there's no need to send both because the shader can easily compute
      the third basis vector using a cross product. Take a look at ; new codelines are highlighted in
      bold.Let's crack some knuckles and write some
      shaders. A good starting point is the pair of shaders we used for
      pixel-based lighting in . I've repeated them
      here (), with uniform
      declarations omitted for brevity.To extend this to support bump mapping, we'll
      need to add new attributes for the tangent vector and texture
      coordinates. The vertex shader need not transform them; we can leave
      that up to the pixel shader. See .Before diving into the fragment shader, let's
      review what we'll be doing:Extract a perturbed normal from the
          normal map, transforming it from [0, +1] to [-1, +1]. Depending on
          the tool used to generate the normal map, the color components may
          need to be swizzled.Create three basis vectors using the
          normal and tangent vectors that were passed in from the vertex
          shader.Perform a change of basis on the
          perturbed normal to bring it to object space.Execute the same lighting algorithm that
          we've used in the past, but use the perturbed normal.Now we're ready! See .We're not done just yet though — since the
      lighting math operates on a normal vector that lives in object space,
      the  and 
      uniforms that we pass in from the application need to be in object space
      too. To transform them from world space to object space, we can simply
      multiply them by the model matrix using our C++ vector library. Take
      care not to confuse the model matrix with the model-view matrix; see
      .You might be wondering why we used object
      space lighting for shader-based bump mapping, rather than eye space
      lighting. After all, eye space lighting is what was presented way back
      in  as the "standard" approach. It's
      actually fine to perform bump map lighting in eye space, but I wanted to
      segue to the fixed-function approach, which 
      require object space!Another potential benefit to lighting in
        object space is performance. I'll discuss this more in the next
        chapter.I briefly mentioned a while back that OpenGL
      ES 1.1 requires the normal map itself to be transformed to object space
      (depicted in the far-right panel in ). If it were transformed it to eye
      space, then we'd have to create a brand new normal map every time the
      camera moves. Not exactly practical!The secret to bump-mapping with
      fixed-function hardware lies in a special texture combiner operation
      called . This technique is often simply
      known as "DOT3 lighting". The basic idea is to have the texture combiner
      generate a gray color whose intensity is determined by the dot product
      of its two operands. This is sufficient for simple diffuse lighting,
      although it can't produce specular highlights. See  for a screenshot of the Bumpy app with OpenGL
      ES 1.1.Here's the sequence of
       calls that sets up the texturing state used
      to generate :The above code snippet tell OpenGL to set up
      an equation like this:Where: is the
          per-vertex color, set with . This
          represents the light direction. is the
          texel color. This represents the perturbed normal. is the luminance
          value of the final color. This represents the diffuse factor. is (½,
          ½, ½)Curious about the  offset and the final multiply-by-four?
      Remember, we had to transform our normal vectors from unit space to
      color space:The  offset
      and multiply-by-four simply puts the final result back into unit space.
      Since this assumes that  vectors have been
      transformed in the above manner, take care to transform the light
      position. Here's the relevant snippet of application code, once again
      leveraging our C++ vector library:The result from DOT3 lighting is often
      modulated with a second texture stage to produce a final color that's
      non-gray. Note that DOT3 lighting is basically performing per-pixel
      lighting, but without the use of shaders!Perhaps the most awkward aspect of DOT3
      lighting is that it requires you to somehow create a normal map in
      object space. Some generator tools don't know what your actual geometry
      looks like; these tools take only a simple heightfield for input, so
      they can only generate the normals in tangent space.The trick I used for the Klein bottle was to
      use OpenGL ES 2.0 as a part of my "art pipeline", even though the final
      application used only OpenGL ES 1.1. By running a modified version of
      the OpenGL ES 2.0 demo and taking a screenshot, I obtained an object
      space normal map for the Klein bottle. See .The shaders for this are shown in  and . Note
      that the vertex shader ignores the model-view matrix and the incoming
      vertex position. It instead uses the incoming texture coordinate to
      determine the final vertex position. This effectively "unfolds" the
      object. The , , and
       constants are used to center the image on the
      screen. (I also had to do some cropping and scaling on the final image
      to make it have power-of-two dimensions.)The fragment shader is essentially the same
      as what was presented in , except
      that no lighting math is performed.You might recall a technique presented back in
    , where we rendered an upside-down object to
    simulate reflection. This was sufficient for reflecting a limited number
    of objects onto a flat plane, but if you'd like the surface of a 3D object
    to reflect a richly detailed environment, as shown in , a cube map is required. Cube maps are special
    textures composed from six individual images: one for each of the six
    axis-aligned directions in 3D space. Cube maps are supported only in
    OpenGL ES 2.0.Cube maps are often visualized using a cross
    shape that looks like an unfolded box, as seen in .The cross shape is for the benefit of humans
    only; OpenGL does expect it when you give it the image data for a cube
    map. Rather, it requires you to upload each of the six faces individually,
    like this:Note that, for the first time, we're using a
    texture target other than . This can be a
    bit confusing because the function call name still has the 2D suffix. It
    helps to think of each face as being two-dimensional, although the texture
    object itself is not. The enumerants for the six faces have contiguous
    values, so it's more common to upload the faces of a cube map using a
    loop. For an example of this, see , which
    creates and populates a complete mipmapped cube map.In , the
    passed-in  parameter is the width (or height) of
    each cubemap face. Cube map faces must be square. Additionally, on the
    iPhone, they must have a size that's a power-of-two. shows the vertex
    shader that can be used for cubemap reflection.Newly introduced in  is GLSL's built-in 
    function, which is defined like this: is the surface normal;
     is the , which
    is the vector that strikes the surface at the point of interest (see ).Cube maps can also be used for refraction,
      which is useful for creating glass or other transparent media. GLSL
      provides a  function to help with this.The fragment shader for our cube mapping
    example is fairly simple; see .Newly introduced in  is a new uniform type called
    . Full-blown desktop OpenGL has many sampler
    types, but the only two sampler types supported on the iPhone are
     and . Remember,
    when setting a sampler from within your application, set it to the stage
    index, not the texture handle!The sampler function in  is also new:  differs
    from  in that it takes a
     texture coordinate rather than a
    . You can think of it as a direction vector
    emanating from the center of a cube. OpenGL finds which of the three
    components have the largest magnitude and uses that to determine which
    face to sample from.A common gotcha with cubemaps is incorrect face
    orientation. I find that the best way to test for this issue is to render
    a sphere with a simplified version of the vertex shader that does not
    perform true reflection:Using this technique, you'll easily notice
    seams if one of your cubemap faces need to be flipped, as seen on the left
    in . Note that only five faces are
    visible at a time, so I suggest testing with a negated
     vector as well.To download the full source code to the cubemap
    sample, go to O'Reilly web site for this book.Instead of using a pre-supplied cubemap
      texture, it's possible to generate a cubemap texture in real time from
      the 3D scene itself. This can be done by re-rendering the scene six
      different times, each time using a different model-view matrix. Recall
      the function call that attached an FBO to a texture, first presented in
      :The 
      parameter is not limited to ; it can be
      any of the six face enumerants
      ( and so on). See  for a high-level overview of a render
      method that draws a 3D scene into a cube map.Rendering to a cube map texture is only supported in iPhone OS
        3.1 and above.An issue with standard bilinear texture
    filtering is that it samples the texture using the same offsets,
    regardless of how the primitive is oriented on the screen. Bilinear
    filtering samples the texture four times across a 2x2 square of texels;
    mipmapped filtering makes a total of eight samples (2x2 on one mipmap
    level, 2x2 on another). The fact that these methods sample across a
    uniform 2x2 square can be a bit of a liability.For example, consider a textured primitive
    viewed at a sharp angle, such as the grassy ground plane in the Holodeck
    sample from . The grass looks blurry, even
    though the texture is quite clear. A zoomed-in screenshot of this is shown
    in .A special type of filtering scheme called
     can alleviate blurriness with
    near edge-on primitives. Anisotropic filtering dynamically adjusts its
    sampling distribution depending on the orientation of the surface.
     is a rather intimidating word, so it
    helps to break it down. Traditional bilinear filtering is
    , meaning "uniform in all dimensions";
     is Greek for "equal", and
     means "direction" in this context."Anisotropic" can also describe a lighting
      model that simulates a surface with directional grain, such as satin or
      brushed metal. David Banks developed this technique in a 1994 SIGGRAPH
      paper. My favorite example can be found in the uniforms worn by the main
      characters in . I encourage you to
      look it up and write a shader!Anisotropic texturing is made available via the
     extension. Strangely,
    at the time of this writing, this extension is only available on older
    iPhones. I strongly suggest checking for support at run time before making
    use of it. Flip back to  to
    see how to check for extensions at run time.Even if your device does not support the
      anisotropic extension, it's possible to achieve the same effect in a
      fragment shader that leverages derivatives (discussed in ).The anisotropic texturing extension adds a new
    enumerant for passing in to :The
     constant sets the maximum
    degree of anisotropy; the higher the number, the more texture lookups are
    performed. At present, Apple devices that support this extension have a
    maximum value of 2.0, but you should query it at run time, as seen in
    .For highest quality, you'll want to use this
    anisotropic filtering in concert with mipmapping. Take care with this
    extension; the additional texture lookups can incur a loss in
    performance!Whenever I watch a classic  episode from the 60s, I always get a good laugh when a
    beautiful female (human or otherwise) speaks into the camera; the image
    invariably becomes soft and glowy, as though viewers need help in
    understanding just how feminine she really is.  (often called  for short)
    is a way of letting bright portions of the scene bleed into surrounding
    areas, serving to exaggerate the brightness of those areas. See  for an example of light blooming.For any post-processing effect, the usual
    strategy is to render the scene into an FBO, then draw a full-screen quad
    to the screen with the FBO attached to a texture. When drawing the
    full-screen quad, a special fragment shader is employed to achieve the
    effect.I just described a single-pass process, but for
    many image processing techniques (including bloom), a single pass is
    inefficient. To see why this is true, consider the halo around the white
    circle in the upper-left of ; if the
    halo extends two or three pixels away from the boundary of the original
    circle, the pixel shader would need to sample the source texture over an
    area of 5x5 pixels, requiring a total of 25 texture lookups (see ). This would cause a huge performance hit.There are several tricks we can use to avoid a
    huge number of texture lookups. One trick is downsampling the original FBO
    into smaller textures. In fact, a simple (but crude) bloom effect can be
    achieved by filtering out the low-brightness regions, successively
    downsampling into smaller FBOs, then accumulating the results.  illustrates this process using
    pseudocode.This procedure is almost possible without the
    use of shaders; the main difficulty lies in the high-pass filter step.
    There are a couple ways around this; if you have a priori knowledge of the
    bright objects in your scene, simply render those objects directly into
    the FBO. Otherwise you may be able to use texture combiners (covered at
    the beginning of this chapter) to subtract out the low-brightness regions,
    then multiply the result back to its original intensity.The main issue with the procedure outlined in
     is that it's using nothing more than
    OpenGL's native facilities for bilinear filtering. OpenGL's bilinear
    filter is also known as a "box filter", aptly named since it produces
    rather boxy results, as seen in .A much higher quality filter is the
    , which gets its name from a
    function often used in statistics. It's also known as the ; see .Much like the box filter, the Gaussian filter
    samples the texture over the square region surrounding the point of
    interest. The difference lies in how the texel colors are averaged; the
    Gaussian filter uses a weighted average where the weights correspond to
    points along the bell curve.The Gaussian filter has a property called
    , which means it can be split into two
    passes: a horizontal pass, then a vertical one. So, for a 5x5 region of
    texels, we don't need 25 lookups; instead, we can make five lookups in a
    horizontal pass, then another five lookups in a vertical pass. The
    complete process is illustrated in .
    The labels below each image tell you which framebuffer objects are being
    rendered to. Note that the
    B-B set of FBOs are
    "ping-ponged" (yes, this term is used in graphics literature) to save
    memory, meaning that they're rendered to more than once.Yet another trick to reduce texture lookups is
    to sample somewhere other than at the texel centers. This exploits
    OpenGL's bilinear filtering capabilities. See  for an example of how five texture lookups
    can be reduced to three.A bit of math proves that the 5-lookup and
    3-lookup cases are equivalent if you use the correct off-center texture
    coordinates for the 3-lookup case. First, give the row of texel colors
    names  through , where  is the
    center of the filter. The weighted average from the 5-lookup case can then
    be expressed like this:For the 3-lookup case, give the names  and  to the
    colors resulting from the off-center lookups. The weighted average is the
    following:The texture coordinate for  is chosen such that A contributes one-fifth of
    its color, and  contributes four-fifths.
    The  coordinate follows the same scheme.
    This can be expressed like this:Substituting 
    and  in 
    yields:This is equivalent to , which shows that three carefully-chosen texture
    lookups can provide a good sample distribution over a 5-pixel area.Full-blown Gaussian bloom may bog down your
      frame rate, even when using the sampling tricks that we discussed. In
      practice, I find that performing the blurring passes only on the smaller
      images provides big gains in performance with relatively little loss in
      quality.Enough theory, let's code this puppy up.
       shows the fragment shader used for
      high-pass filtering.Of interest in 
      is how we evaluate the perceived brightness of a given color. The human
      eye responds different to different color components, so it's not
      correct to simply take the "length" of the color vector.Next let's take a look at the fragment shader
      that's used for Gaussian blur. Remember, it has only three lookups! See
      .By having the application code supply
       in the form of a vec2 uniform, we can use the
      same shader for both the horizontal and vertical passes. Speaking of
      application code, check out . The
       boolean turns on hybrid Gaussian/Crude
      rendering; set it to false for a higher-quality blur at a reduced frame
      rate.In , the
      implementation to utility methods such as
       are omitted for brevity, since they're
      the same as in previous samples. As always, you can download the entire
      source code for this sample from O'Reilly's web site.Keep in mind that bloom is only one type of
      image processing technique; there are many more techniques that you can
      achieve with shaders. For example, by skipping the high-pass filter, you
      can soften the entire image; this could be used as a poor man's
      anti-aliasing technique.Also note that image processing techniques
      are often applicable outside the world of 3D graphics — for example, you
      could use OpenGL to perform a bloom pass on an image captured with the
      iPhone camera!This chapter picked up the pace a bit, giving a
    quick overview of some more advanced concepts. I hope you feel encouraged
    to do some additional reading; computer graphics is a deep field and
    there's plenty to learn that's outside the scope of this book.Many of the effects presented in this chapter
    are only possible at the cost of a lower frame rate. You'll often come
    across a trade off between visual quality and performance, but there are
    tricks to help with this. In the next chapter we'll discuss some of these
    optimization tricks and give you a leg up in your application's
    performance.One reason you might be reading this book in the
  first place is that you don't just want pretty graphics; you want
  graphics. Since OpenGL ES gives you the lowest
  possible level of access to Apple's graphics hardware, it's the ideal API
  when speed is your primary concern. Many speed-hungry developers use OpenGL
  even for 2D games for this reason.Until this point in the book, we've been focusing
  mostly on the fundamentals of OpenGL, or showing how to use it to achieve
  cool effects. We've mentioned a few performance tips along the way, but we
  haven't dealt with performance head on.Of all performance metrics, your
     (number of
     calls per second) is the metric
    that you'll want to measure most often. We presented one way of
    determining frame rate back in , where we rendered
    a FPS counter on top of the scene itself. This was convenient and simple,
    but Apple's  tool () can accomplish the same thing, and much more
    — without requiring you to modify your application!The best part is that's included the SDK that you already
    have.As with in performance analysis tool, beware
      of the Heisenberg Effect. In this context, I'm referring to the fact
      that measuring performance can, in itself, affect performance. This has
      never been problematic for me, and it's certainly not as bothersome as a
      Heisenbug (a bug that seems to vanish when you're in the
      debugger.)First, be sure that your  (upper-left corner of Xcode) is set to a Device
    configuration rather than a Simulator configuration. Next, click the
     menu and select the  sub-menu. You should see an option for OpenGL
    ES (if not, you probably have your SDK set to Simulator).Alternatively, you can run Instruments
      directly from
      .While your application is running, Instruments
    is constantly updating a EKG-like graph of your frame rate and various
    other metrics you may be interested in. Try clicking the information icon
    in the OpenGL ES panel on the left, then press the
     button. This allows you to pick and choose
    from a slew of performance metrics.Instruments is a great tool for many aspects of
    performance analysis, not just OpenGL. I find that it's particularly
    useful for detecting memory leaks. The documentation for Instruments is
    available on the iPhone Developer site. I encourage you to read up on it
    and give it a whirl, even if you're not experiencing any obvious
    performance issues.Don't forget that the bottleneck of your
    application may be on the CPU side rather than in the GPU (Graphics
    Processing Unit). You can determine this by diving into your rendering
    code and commenting out all the OpenGL function calls. Re-run your app
    with Instruments and observe the frame rate; if it's unchanged, then the
    bottleneck is on the CPU. Dealing with such bottlenecks is beyond the
    scope of this book, but Instruments can help you track them down. Keep in
    mind that some types of CPU work (such as transforming vertices) can be
    moved to the GPU through clever usage of OpenGL.The manner in which you submit vertex data to
    OpenGL ES can have a huge impact on performance. The most obvious tip is
    something I've mentioned early in the book: use vertex buffer objects
    whenever possible. They eliminate costly memory transfers. VBOs don't help
    as much with older devices, but using them is a good habit to get
    into.VBO usage is just the tip of the iceberg.
      Another best practice that you'll hear a lot about is . The idea is simple: try to render as much as
      possible in as few draw calls as possible. Consider how you'd go about
      drawing a human head. Perhaps your initial code does something like
      .Right off the bat, you should notice that the
      head and nose can be "batched" into a single VBO. You can also do a bit
      of rearranging to reduce the number of texture binding operations. The
      result after this tuning is shown in .Try combing through the code again to see if
      anything be eliminated. Sure, you might be saving a little bit of memory
      by using a single VBO to represent the ear, but suppose it's a rather
      small VBO. If you add two instances of the ear geometry to your existing
      "head and nose" VBO, you can eliminate the need for changing the
      model-view matrix, plus you can use fewer draw calls. Similar guidance
      applies to the eyeballs. The result is shown in .You're not done yet. Remember texture
      atlases, first presented in ? By tweaking
      your texture coordinates and combining the skin texture with the eye and
      lip textures, you can reduce the rendering code to only two
      lines:Pixomatic's
         application is a favorite with
        artists for generating texture atlases.Ok, I admit this example was rather
      contrived. Rarely does production code make linear sequences of OpenGL
      calls as I've done in these examples. Real-world code is usually
      organized into subroutines, with plenty of stuff going on between the
      draw calls. But, the same principles apply. From the GPU's perspective,
      your application is merely a linear sequence of OpenGL calls. If you
      think about your code in this distilled manner, potential optimizations
      can be easier to spot.You might hear the term
       being thrown around in regard to
      OpenGL optimizations. It is indeed a good practice, but it's actually
      nothing special. In fact, every sample in this book has been using
      interleaved data (for a diagram, flip back to  in ). Using
      our C++ vector library, much of this book's sample code declares a POD
      (plain old data) structure representing a vertex, like this:When we create the VBO, we populate it with
      an array of  objects. When it comes time to
      render the geometry, we usually do something like .OpenGL does not require you to arrange our
      VBO in the above manner. For example, consider a small VBO with only
      three vertices. Instead of arranging it like this: You could arrange it like this:This is perfectly acceptable (but not
      advised); the way you'd submit it to OpenGL is shown in .When you submit vertex data in this manner,
      you're forcing the driver to re-order the data to make it amenable to
      the GPU.That's all interleaved data is. It's what
      we've been doing anyway.One aspect of vertex layout you might be
      wondering about is the ordering of attributes. With OpenGL ES 2.0 and
      newer Apple devices, the order has little or no impact on performance
      (assuming you're using interleaved data). On first and second generation
      iPhones, Apple recommends the following order:PositionNormalColorTexture Coordinate (First Stage)Texture Coordinate (Second Stage)Point SizeBone WeightBone IndexYou might be wondering about the two "bone"
      attributes — stay tuned, well discuss them later in the chapter.Another way of optimizing your vertex format
      is shrinking the size of the attribute types. In this book, we've been a
      bit sloppy by using 32-bit floats for just about everything. Don't
      forget there are other types you can use. For example, floating point is
      often overkill for color, since colors usually don't need as much
      precision as other attributes.Never use  on
        any Apple devices. There's no reason to do so.Apple recommends aligning your vertex
      attributes in memory according to their native alignment. For example, a
      4-byte float should be aligned on a 4-byte boundary. Sometimes you can
      deal with this by adding padding to your vertex format:Apple's general advice (at the time of this
      writing) is to prefer  over
      . Strips require fewer vertices, but
      usually at the cost of more draw calls. Sometimes you can reduce the
      number of draw calls by introducing degenerate triangles into your
      vertex buffer.Strips versus separate triangles, indexed versus non-indexed;
      these all have trade-offs. You'll find that many developers have strong
      opinions, and you're welcome to review all the endless debates on the
      forums. In the end, experimentation is the only reliable way to
      determine the best tessellation strategy for your unique
      situation.Imagination Technologies provides code for converting lists into
      strips. Look for  in the PowerVR
      SDK (first discussed in ). They
      also provide a sample app to show it off
      ().If your app has highly tessellated geometry
    (lots of tiny triangles), then lighting can negatively impact performance.
    The following sections discuss ways to optimize lighting. Use your time
    wisely; don't bother with these optimizations without first testing if
    you're . Applications that are fill
    rate bound have their bottleneck in the rasterizer, and lighting
    optimizations don't help much is those cases. Unless, of course, the app
    is doing per-pixel lighting!There's a simple test you can use to determine
    if you're fill rate bound. Try modifying the parameters to
     to shrink your viewport to a small size. If
    this causes a significant increase in your frame rate, you're probably
    fill rate bound.If you discover that you're neither fill rate
    bound nor CPU bound, the first thing to consider is simplifying your
    geometry. This means rendering a coarser scene using larger, fewer
    triangles. If you find you can't do this without a huge loss in visual
    quality, consider one of the optimizations in the following
    sections.If you're CPU bound, try turning off Thumb
      mode; it enables a special ARM instruction set that can slow you down.
      Turn it off in XCode by going to , and uncheck the
       option under the  heading.We briefly mentioned object space lighting in
      ; to summarize, in certain circumstances,
      surface normals don't need transformation when the light source is
      infinitely distant. This principle can be applied under OpenGL ES 1.1
      (in which case the driver makes the optimization for you) or in OpenGL
      ES 2.0 (in which case you can make this optimization in your
      shader).To specify an infinite light source, set the
       component of your light position to
      zero. But what are those "certain circumstances" that we mentioned?
      Specifically, your model-view matrix cannot have non-uniform scale
      (scale that's not the same for all three axes). To review the logic
      behind this, flip back to .If per-vertex lighting under OpenGL ES 1.1
      causes performance issues with complex geometry, or if it's producing
      unattractive results with coarse geometry, I recommend that you consider
      DOT3 lighting. This technique leverages the texturing hardware to
      perform a crude version of per-pixel lighting. Refer back to  for more information.The best way to speed up your lighting
      calculations is to not perform them at all! Scenes with light sources
      that don't move can be pre-lit, or "baked in". This can be accomplished
      by performing some offline processing to create a grayscale texture
      (also called a ). This technique is
      especially useful when used in conjunction with multi-texturing.As an added benefit, baked lighting can be
      used to create a much higher-quality effect than standard OpenGL
      lighting. For example, by using a raytracing tool, you can account for
      the ambient light in the scene, producing beautiful soft shadows (see
      ). One popular offline tool for this is
      , developed by Santiago Orgaz.If your frame rate soars when you try disabling
    texturing, take a look at the following list.Don't use textures that are any larger than
        necessary. This is especially true when porting a desktop game; the
        iPhone's small screen usually means that you can use smaller
        textures.Older devices have 24 MB of texture memory;
        don't exceed this. Newer devices have unified memory so it's less of a
        concern.Use a compressed or low-precision format if
        possible.Use texture atlases to reduce the number of
        bind calls.Another tip: it won't help with frame rate, but
    your load time might be improved by converting your image files into a
    "raw" format like PVRTC, or even into a C-array header file. You can do
    either of these using  ().Avoid telling OpenGL to render things that
    aren't visible anyway. Sounds easy right? In practice, this guideline can
    be more difficult to follow than you might think.Consider something simple: an OpenGL scene
      with a spinning, opaque sphere. All the triangles on the "front" of the
      sphere (the ones that face that camera) are visible, but the ones in the
      back are not. OpenGL doesn't need to process the vertices on the back of
      the sphere. In the case of OpenGL ES 2.0, we'd like to skip running a
      vertex shader on back-facing triangles; with OpenGL ES 1.1, we'd like to
      skip transform and lighting operations on those triangles.
      Unfortunately, the graphics processor doesn't know that those triangles
      are occluded until after it performs the rasterization step in the
      graphics pipeline.So, we'd like to tell OpenGL to skip the
      back-facing vertices. Ideally we could do this without any CPU overhead,
      so changing the VBO at each frame is out of the question.How can we know ahead of time that a triangle
      is back-facing? Consider a single layer of triangles in the sphere; see
      . Note that the triangles have
      consistent "winding"; triangles that wind clockwise are back-facing,
      while triangles that wind counter-clockwise are front-facing.OpenGL can quickly determine if a given
      triangle goes clockwise or counter-clockwise. Behind the scenes, the GPU
      can take the cross product of two edges in screen space; if the
      resulting sign is positive, the triangle is front-facing, otherwise it's
      back-facing.Face culling is enabled like so:You can also configure OpenGL to define which
      winding direction is the front:Depending on how your object is tessellated,
      you may need to play with this setting.Use culling with caution; you won't always
      want it to be enabled! It's mostly useful for opaque, enclosed objects.
      For example, a ribbon shape would disappear if you tried to view it from
      the back.As an aside, face culling is useful for much
      more than just performance optimizations. For example, developers have
      come up with tricks that use face culling in conjunction with the
      stencil buffer to perform CSG operations (composite solid geometry).
      This allows you to render shapes that are defined from the intersections
      of others shapes. It's cool stuff, but outside the scope of this
      book.
      provide another way of culling away un-necessary portions of a 3D scene,
      and they're often useful outside the context of performance
      optimization. Here's how you enable a clip plane with OpenGL ES
      1.1:Alas, with OpenGL ES 2.0 this feature doesn't
      exist. Let's hope for an extension!The coefficients passed into
       define the plane equation; see .One way of thinking about  is interpreting A, B, and C as the components to
      the plane's normal vector, and D as the distance from the origin. The
      direction of the normal determines which half of the scene to cull
      away.Older Apple devices support only one clip
      plane, but newer devices support six simultaneous planes. The number of
      supported planes can be determined like so:To use multiple clip planes, simply add a
      zero-based index to the 
      constant:This is consistent with working with multiple
      light sources, which requires you to add a light index to the
       constant.Unlike the goofy toy applications in this
      book, real-world applications often have huge, sprawling worlds that
      users can explore. Ideally, you'd only give OpenGL the portions of the
      world that are within the viewing frustum, but at first glance this
      would be incredibly expensive. The CPU would need to crawl through every
      triangle in the world to determine its visibility!The way to solve this problem is to use a
       (BVH), a tree structure
      for facilitating fast intersection testing. Typically the root node of a
      BVH corresponds to the entire scene, while leaf nodes correspond to
      single triangles, or small batches of triangles.A thorough discussion of BVHs is beyond the scope of this book,
      but volumes have been written on the subject and you should be able to
      find plenty of information.I'll keep this section brief; GLSL is a
    programming language much like any other, and you can apply the same
    algorithmic optimizations that can be found in more general-purpose
    programming books. Having said that, there are certain aspects of GPU
    programming that are important to understand.Shaders are executed on a
      
      (SIMD). The GPU has many, many execution units, and they're usually
      arranged into blocks, where all units with a block share the same
      program counter. This has huge implications. Consider an if-else
      conditional in a block of SIMD units, where the condition can vary
      across units:How on earth could this work and still
      produce correct results? The secret lies in the processor's ability to
      ignore its own output. If the conditional is false, the processor's
      execution unit temporarily disables all writes to its register bank, but
      executes instructions anyway. Since register writes are disabled, the
      instructions have no affect.So, conditionals are usually much more
      expensive on SIMD architectures than on traditional architectures,
      especially if the conditional is different from one pixel (or vertex) to
      the next.Conditionals based solely on uniforms or
      constants aren't nearly as expensive. Having said that, you may want to
      try splitting up shaders that have many conditionals like this, and let
      your application choose which one to execute. Another potential
      optimization is the inverse: combine multiple shaders to reduce the
      overhead of switching between them. As always, there's a trade off, and
      experimentation is often the only way to determine the best course of
      action. Again, I won't try to give any hard and fast rules, since
      devices and drivers may change in the future.One GLSL instruction we haven't discussed in
      this book is the  instruction, available only
      within fragment shaders. This instruction allows you to return early and
      leave the framebuffer unaffected. While useful for certain effects, it's
      generally inadvisable to use this instruction on Apple devices. (The
      same is true for alpha testing, first discussed in .)Keep in mind that alpha blending can often be
      used to achieve the same effect that you're trying to achieve with
      , and at potentially lower cost to
      performance.Every time you access a texture from within a
      fragment shader, the hardware may be applying a filtering operation.
      This can be expensive. If your fragment shader makes tons of texture
      lookups, think of ways to reduce them, just like we did for the bloom
      sample in the previous chapter.Typically, 
    refers to color blending, discussed in .
     (also known as ) is an entirely different kind of blending, although
    it too leverages linear interpolation.First, a disclaimer. This is a chapter on
    optimization, and yes, vertex skinning is an optimization — but only when
    performed on the GPU. Generally speaking, vertex skinning is more of a
    technique than an optimization. Put simply, it makes it easy to animate
    "rounded" joints in your model.For example, let's go back to the stick figure
    demo, originally presented in .
     shows a comparison of the stick
    figure with and without vertex skinning. (Usually skinning is applied to
    3D models, so this is a rather contrived case.) Notice how the elbows and
    knees have curves rather than sharp angles.The main idea behind GPU-based skinning is that
    you need not change your vertex buffer during animation; the only data
    that gets sent to the GPU is a new list of model-view matrices.Yes, you heard that right: a
     of model-view matrices! So far we've been
    dealing with only one model-view at a time; with vertex skinning, you give
    the GPU a list of model-views for only one draw call. Because there are
    several matrices, it follows that each vertex now has several
    post-transformed positions. Those post-transformed positions get blended
    together to form the final position. Don't worry if this isn't clear yet;
    you'll have a deeper understanding after we go over some example
    code.Skinning requires you to include additional
    vertex attributes to your vertex buffer. Each vertex is now bundled with a
    set of  and . Bone indices tell OpenGL which model-view matrices to
    apply; bone weights are the interpolation constants. Just like the rest of
    the vertex buffer, bone weights and indices are set up only once, and
    remain static during the animation.The best part of vertex skinning is that you
    can apply it with both OpenGL ES 2.0 (via the vertex shader) or OpenGL ES
    1.1 (via an iPhone-supported extension). Much like I did with bump
    mapping, I'll cover the OpenGL ES 2.0 method first, since it'll help you
    understand what's going on behind the scenes.Much of the prep work required for vertex
      skinning will be the same for both OpenGL ES 1.1 and OpenGL ES 2.0. To
      achieve the curvy lines in our stick figure, we'll need to tessellate
      each limb shape into multiple slices.  depicts an idealized elbow joint; note
      that the vertices in each vertical slice have the same blend
      weights.In , the
      upper arm will be rigid on the left and curvy as it approaches the
      forearm. Conversely, the forearm will curve on the left, and straighten
      out closer to the hand.Let's define some structures for our program,
      again leveraging the vector library in :The  structure
          is a POD type that defines the layout of the vertex buffer.The  structure
          encapsulates a relatively small set of points that make up an
          animated "stick figure". We won't be sending these points to OpenGL:
          it's for internal purposes only, as you'll see.The 
          structure encapsulates the data that we'll send to OpenGL. It
          contains handles for the static VBOs and a list of matrices that
          we'll update at every frame.Given a  object,
      computing a list of model-view matrices is a bit tricky; see . This computes a sequence of matrices for the
      joints along a single limb.Compute the primary model-view which will
          be multiplied with each bone-specific transform.Fill the columns of a change-of-basis
          matrix; to review the math behind this, flip back to .Translate the bone to the origin, then
          rotate it about the origin.Translate the bone to its current
          position.Combine the primary model-view with the
          rotation and translation matrices to form the final bone
          matrix. shows the
      vertex shader for skinning; this lies at the heart of the
      technique.Note that we're only applying two bones at a
      time for this demo. By modifying the shader, you could potentially blend
      between three or more bones. This can be useful for situations that go
      beyond the classic elbow example, such as soft-body animation. Imagine a
      wibbly-wobbly blob that lurches around the screen; it could be rendered
      using a network of several "bones" that meet up at its center.The fragment shader for the stick figure demo
      is incredibly simple; see . As you can see,
      all the real work for skinning is on the vertex shader side of
      things.The rendering code is fairly straightforward;
      see .This is the largest number of attributes
      we've ever enabled; as you can see, it can be quite a chore to set them
      all up. One thing I find helpful is creating my own variant of the
       macro, useful for passing a byte offset to
      . Here's how I define it:The compiler will complain if you use
      on a type that it doesn't consider to be a
      POD type. This is mostly done just to conform to the ISO C++ standard;
      in practice, it's usually safe to use  on
      simple non-POD types. You can turn off the warning by adding
       to the gcc command line. (To
      add gcc command line arguments in Xcode, right-click the source file and
      choose .)All Apple devices at the time of this writing
      support the  extension under
      OpenGL ES 1.1. As you'll soon see, it works in a manner quite similar to
      the OpenGL ES 2.0 method previously discussed. The tricky part is that
      it imposes limits on the number of so-called "vertex units" and "palette
      matrices".Each 
      performs a single bone transformation. In the simple stick figure
      example, we only need two vertex units for each joint, so this isn't
      much of a problem. are
      simply another term for bone matrices. We need 17 matrices for our stick
      figure example, so a limitation might complicate matters.Here's how you can determine how many vertex
      units and palette matrices are supported: shows
      the limits for current apple devices at the time of this writing.Uh-oh, we need 17 matrices, but at most only
      11 are supported! Fret not; we can simply split the rendering pass into
      two draw calls. That's not too shoddy! Moreover, since
       allows you to pass in an offset, we
      can still store the entire stick figure in only one VBO.Let's get down to the details. Since OpenGL
      ES 1.1 doesn't have uniform variables, it supplies an alternate way of
      handing bone matrices over to the GPU. It works like this:Pretty straightforward! When you enable
      , you're telling OpenGL to
      ignore the standard model-view, and instead use the model-views that get
      specified while the matrix mode is set to
      .We also need a way to give OpenGL the blend
      weights and bone indices. Simple enough:Ok, we're now ready to write some rendering
      code, taking into account the fact that the number of supported matrix
      palettes might be less than the number of bones in our model. Check out
       to see how we "cycle" the available
      matrix slots; further explanation follows the listing.Under our system, if the model has 17 bones
      and the hardware supports 11 bones, vertices affected by the 12th matrix
      should have an index of 1 rather than 11; see  for a depiction of how this works.Unfortunately our system breaks down if at
      least one vertex needs to be affected by two bones that "span" the two
      passes, but this rarely occurs in practice.The limitation on available matrix palettes
      also needs to be taken into account when annotating the vertices with
      their respective matrix indices. 
      shows how our system generates the blend weights and indices for a
      single limb. (This code can be used for both the ES 1.1-based method and
      the shader-based method.)In , the
       and  variables are
      the increments used for each half of limb; refer to  and flip back to  to see how this works.For simplicity, we're using a linear falloff
      of bone weights here, but I encourage you to try other variations. Bone
      weight distribution is a bit of a black art.That's it for the skinning demo! As always,
      the complete sample code can be found on O'Reilly's web site. You might
      also want to check out the 3D skinning demo included in the PowerVR
      SDK.Before you get too excited, I should warn you
      that vertex skinning isn't a magic elixir. An issue called "pinching"
      has caused many a late night for animators and developers. Pinching is a
      side effect of interpolation that causes severely-angled joints to
      become distorted ().If you're using OpenGL ES 2.0 and pinching is
      causing headaches, you may want to research a technique called "dual
      quaternion skinning", developed by Ladislav Kavan and others. It's
      beyond the scope of this book, but might be worth a look.The list of optimizations in this chapter is by
    no means complete. Apple's  document, available from the iPhone Developer Site, has
    an excellent list of up-to-date advice. Imagination Technologies publishes
    a similar document that's worth a look.Since we're at the end of the book, I'd also
    like to point out a couple resources for adding to your general OpenGL
    skills.  by Randi Rost
    provides a tight focus on shader development. Although the book is written
    for desktop GLSL, the shading language for ES is so similar to standard
    GLSL that almost everything in Randi's book can be applied to OpenGL ES as
    well.The official specification documents for OpenGL
    ES are available as PDFs at the Khronos Website:The OpenGL specs are incredibly detailed and unambiguous. I find
    them to be a handy reference, but they're not exactly ideal for casual
    armchair reading. For a more web-friendly reference, check out the HTML
    man pages at Khronos: In the end, one of the best ways to learn is to
    play — write some 3D iPhone apps from the ground up and enjoy
    yourself!